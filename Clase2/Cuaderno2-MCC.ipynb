{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7bf5772",
   "metadata": {},
   "source": [
    "### **Aprendizaje profundo, optimización y atención**\n",
    "\n",
    "Este cuaderno cubre **redes neuronales + optimización** y cierra con un **puente directo hacia Transformers**: *self-attention*, **conexiones residuales (skip connections)**, **capas de normalización** y **redes feed-forward** (FFN).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34fe1e9",
   "metadata": {},
   "source": [
    "#### **Configuración**\n",
    "\n",
    "Este cuaderno usa **NumPy + PyTorch** (CPU) y datasets sintéticos pequeños para que todo corra rápido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94213ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reproducibilidad\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a876583",
   "metadata": {},
   "source": [
    "#### **1. Perceptrón (lineal) y MLP (no lineal)**\n",
    "\n",
    "**Perceptrón**: modelo lineal con activación.\n",
    "**MLP**: compone capas lineales + activaciones para modelar fronteras no lineales.\n",
    "\n",
    "##### **1.1 Perceptrón \"a mano\" (NumPy)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900121e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_linearly_separable(n=200, margin=0.4):\n",
    "    X = np.random.randn(n, 2)\n",
    "    # Separación por una recta: x0 + x1 > 0\n",
    "    y = (X[:,0] + X[:,1] > 0).astype(int)\n",
    "    # Aumentamos el margen alejando puntos\n",
    "    X = X + (2*y[:,None]-1)*margin*np.array([[1.0, 1.0]])\n",
    "    return X, y\n",
    "\n",
    "X, y = make_linearly_separable()\n",
    "\n",
    "def perceptron_train(X, y, lr=0.1, epochs=25):\n",
    "    w = np.zeros(X.shape[1])\n",
    "    b = 0.0\n",
    "    for _ in range(epochs):\n",
    "        for xi, yi in zip(X, y):\n",
    "            score = np.dot(w, xi) + b\n",
    "            yhat = 1 if score >= 0 else 0\n",
    "            err = yi - yhat\n",
    "            w += lr * err * xi\n",
    "            b += lr * err\n",
    "    return w, b\n",
    "\n",
    "w, b = perceptron_train(X, y, lr=0.05, epochs=20)\n",
    "\n",
    "def perceptron_predict(X, w, b):\n",
    "    return (X @ w + b >= 0).astype(int)\n",
    "\n",
    "acc = (perceptron_predict(X, w, b) == y).mean()\n",
    "acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947c924e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualización de frontera\n",
    "xx = np.linspace(X[:,0].min()-1, X[:,0].max()+1, 200)\n",
    "yy = np.linspace(X[:,1].min()-1, X[:,1].max()+1, 200)\n",
    "XX, YY = np.meshgrid(xx, yy)\n",
    "grid = np.c_[XX.ravel(), YY.ravel()]\n",
    "Z = (grid @ w + b).reshape(XX.shape)\n",
    "\n",
    "plt.figure()\n",
    "plt.contourf(XX, YY, Z >= 0, alpha=0.3)\n",
    "plt.scatter(X[:,0], X[:,1], c=y, s=15)\n",
    "plt.title(\"Perceptrón (frontera lineal)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b127e82",
   "metadata": {},
   "source": [
    "##### **1.2 MLP (PyTorch) sobre un dataset no lineal (make_moons)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcd71af",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples=1200, noise=0.15, random_state=SEED)\n",
    "X = X.astype(np.float32)\n",
    "y = y.astype(np.int64)\n",
    "\n",
    "X_train, X_tmp, y_train, y_tmp = train_test_split(X, y, test_size=0.4, random_state=SEED, stratify=y)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_tmp, y_tmp, test_size=0.5, random_state=SEED, stratify=y_tmp)\n",
    "\n",
    "def to_tensor(a, dtype=None):\n",
    "    t = torch.tensor(a)\n",
    "    return t.to(dtype=dtype) if dtype is not None else t\n",
    "\n",
    "X_train_t = to_tensor(X_train, torch.float32).to(device)\n",
    "y_train_t = to_tensor(y_train, torch.long).to(device)\n",
    "X_val_t   = to_tensor(X_val, torch.float32).to(device)\n",
    "y_val_t   = to_tensor(y_val, torch.long).to(device)\n",
    "X_test_t  = to_tensor(X_test, torch.float32).to(device)\n",
    "y_test_t  = to_tensor(y_test, torch.long).to(device)\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, d_in=2, d_hidden=64, d_out=2, dropout_p=0.0, use_bn=False):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d_in, d_hidden)\n",
    "        self.bn1 = nn.BatchNorm1d(d_hidden) if use_bn else None\n",
    "        self.drop = nn.Dropout(dropout_p)\n",
    "        self.fc2 = nn.Linear(d_hidden, d_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        if self.bn1 is not None:\n",
    "            x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.drop(x)\n",
    "        return self.fc2(x)\n",
    "\n",
    "def accuracy(logits, y):\n",
    "    return (logits.argmax(dim=-1) == y).float().mean().item()\n",
    "\n",
    "def train_classifier(model, optimizer, max_epochs=200, patience=20):\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    best_val = float(\"inf\")\n",
    "    best_state = None\n",
    "    bad = 0\n",
    "    history = {\"train_loss\":[], \"val_loss\":[], \"train_acc\":[], \"val_acc\":[]}\n",
    "\n",
    "    for _epoch in range(1, max_epochs+1):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(X_train_t)\n",
    "        loss = loss_fn(logits, y_train_t)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            tr_logits = model(X_train_t)\n",
    "            va_logits = model(X_val_t)\n",
    "            tr_loss = loss_fn(tr_logits, y_train_t).item()\n",
    "            va_loss = loss_fn(va_logits, y_val_t).item()\n",
    "\n",
    "        history[\"train_loss\"].append(tr_loss)\n",
    "        history[\"val_loss\"].append(va_loss)\n",
    "        history[\"train_acc\"].append(accuracy(tr_logits, y_train_t))\n",
    "        history[\"val_acc\"].append(accuracy(va_logits, y_val_t))\n",
    "\n",
    "        # Early stopping\n",
    "        if va_loss < best_val - 1e-4:\n",
    "            best_val = va_loss\n",
    "            best_state = {k:v.detach().cpu().clone() for k,v in model.state_dict().items()}\n",
    "            bad = 0\n",
    "        else:\n",
    "            bad += 1\n",
    "            if bad >= patience:\n",
    "                break\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    return history\n",
    "\n",
    "model = MLP(dropout_p=0.0, use_bn=False).to(device)\n",
    "opt = torch.optim.SGD(model.parameters(), lr=0.2)\n",
    "hist = train_classifier(model, opt)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_logits = model(X_test_t)\n",
    "test_acc = accuracy(test_logits, y_test_t)\n",
    "test_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c1f852",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(hist[\"train_loss\"], label=\"train\")\n",
    "plt.plot(hist[\"val_loss\"], label=\"val\")\n",
    "plt.title(\"Pérdida (baseline MLP)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(hist[\"train_acc\"], label=\"train\")\n",
    "plt.plot(hist[\"val_acc\"], label=\"val\")\n",
    "plt.title(\"Accuracy (baseline MLP)\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3f288f",
   "metadata": {},
   "source": [
    "#### **2. Activaciones y pérdidas: saturación y gradientes**\n",
    "\n",
    "Sigmoid/tanh tienden a saturar -> gradientes pequeños. ReLU/GELU suelen ser más estables en profundidad.\n",
    "\n",
    "##### **2.1 Norma de gradiente vs profundidad (intuición)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8de3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_norm_through_depth(act=\"sigmoid\", depth=30, width=128):\n",
    "    x = torch.randn(1, width, device=device, requires_grad=True)\n",
    "    layers = [nn.Linear(width, width).to(device) for _ in range(depth)]\n",
    "\n",
    "    h = x\n",
    "    for lin in layers:\n",
    "        h = lin(h)\n",
    "        if act == \"sigmoid\":\n",
    "            h = torch.sigmoid(h)\n",
    "        elif act == \"tanh\":\n",
    "            h = torch.tanh(h)\n",
    "        elif act == \"relu\":\n",
    "            h = F.relu(h)\n",
    "        elif act == \"gelu\":\n",
    "            h = F.gelu(h)\n",
    "        else:\n",
    "            raise ValueError(act)\n",
    "\n",
    "    loss = h.sum()\n",
    "    loss.backward()\n",
    "    return x.grad.norm().item()\n",
    "\n",
    "acts = [\"sigmoid\",\"tanh\",\"relu\",\"gelu\"]\n",
    "depths = [10,20,30,40]\n",
    "vals = {a: [grad_norm_through_depth(a, depth=d) for d in depths] for a in acts}\n",
    "\n",
    "plt.figure()\n",
    "for a in acts:\n",
    "    plt.plot(depths, vals[a], marker=\"o\", label=a)\n",
    "plt.yscale(\"log\")\n",
    "plt.title(\"Norma de gradiente vs profundidad (escala log)\")\n",
    "plt.xlabel(\"profundidad (capas)\")\n",
    "plt.ylabel(\"||dL/dx||\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e449b9fe",
   "metadata": {},
   "source": [
    "## **3. Retropropagación y verificación por diferencias finitas**\n",
    "\n",
    "La retropropagación calcula gradientes exactos (analíticos) usando regla de la cadena en reversa.  \n",
    "Para verificar que la implementación es correcta, usamos una aproximación numérica:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\theta} \\approx \\frac{L(\\theta+\\epsilon)-L(\\theta-\\epsilon)}{2\\epsilon}\n",
    "$$\n",
    "\n",
    "Compararemos el gradiente numérico con el gradiente de retropropagación (backprop) usando error relativo:\n",
    "\n",
    "$$\n",
    "\\text{rel_err}=\n",
    "\\frac{\\lVert g_{\\text{num}}-g_{\\text{backprop}} \\rVert}{\\lVert g_{\\text{num}} \\rVert+\\lVert g_{\\text{backprop}} \\rVert + 10^{-12}}\n",
    "$$\n",
    "\n",
    "\n",
    "**Durante la verificación numérica de gradientes**, hacemos que el cálculo sea **determinista**: desactivamos dropout, fijamos batch normalization en modo evaluación (o congelamos sus estadísticas) y desactivamos el barajado de datos. Esto asegura que las evaluaciones de $L(\\theta+\\epsilon)$ y $L(\\theta-\\epsilon)$ sean comparables.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec77aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finite_diff_check(eps=1e-4):\n",
    "    theta = torch.tensor([0.7], device=device, requires_grad=True)\n",
    "\n",
    "    def L(t):\n",
    "        return (torch.sin(t) + t**2).sum()\n",
    "\n",
    "    loss = L(theta)\n",
    "    loss.backward()\n",
    "    grad_autograd = theta.grad.detach().cpu().item()\n",
    "\n",
    "    theta_p = theta.detach().clone() + eps\n",
    "    theta_m = theta.detach().clone() - eps\n",
    "    grad_fd = ((L(theta_p) - L(theta_m)) / (2*eps)).detach().cpu().item()\n",
    "    return grad_autograd, grad_fd\n",
    "\n",
    "finite_diff_check()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a43b98",
   "metadata": {},
   "source": [
    "#### **4. Regularización y generalización (L2, dropout, batch norm, early stopping)**\n",
    "\n",
    "\n",
    "A continuación se comparan en **make_moons** (mismo modelo, cambios mínimos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441ffad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(tag, dropout_p=0.0, use_bn=False, optimizer_name=\"sgd\", lr=0.2, weight_decay=0.0):\n",
    "    model = MLP(dropout_p=dropout_p, use_bn=use_bn).to(device)\n",
    "\n",
    "    if optimizer_name == \"sgd\":\n",
    "        opt = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay)\n",
    "    elif optimizer_name == \"adam\":\n",
    "        opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    elif optimizer_name == \"adamw\":\n",
    "        opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    else:\n",
    "        raise ValueError(optimizer_name)\n",
    "\n",
    "    hist = train_classifier(model, opt, max_epochs=400, patience=30)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_acc = accuracy(model(X_test_t), y_test_t)\n",
    "\n",
    "    return tag, hist, test_acc\n",
    "\n",
    "experiments = [\n",
    "    (\"baseline\", 0.0, False, \"sgd\", 0.2, 0.0),\n",
    "    (\"L2 (wd=1e-3)\", 0.0, False, \"sgd\", 0.2, 1e-3),\n",
    "    (\"dropout(0.2)\", 0.2, False, \"sgd\", 0.2, 0.0),\n",
    "    (\"batchnorm\", 0.0, True, \"sgd\", 0.2, 0.0),\n",
    "]\n",
    "\n",
    "results = []\n",
    "for tag, dp, bn, optn, lr, wd in experiments:\n",
    "    results.append(run_experiment(tag, dp, bn, optn, lr, wd))\n",
    "\n",
    "[(tag, acc) for tag,_,acc in results]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ea8e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "for tag, hist, _ in results:\n",
    "    plt.plot(hist[\"val_loss\"], label=tag)\n",
    "plt.title(\"Validación: pérdida vs época (regularización)\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f31c90",
   "metadata": {},
   "source": [
    "#### **5. Optimizadores modernos y programación de tasa de aprendizaje**\n",
    "\n",
    "Mostremos el **SGD con momentum, Adam/AdamW, y schedules**.\n",
    "\n",
    "##### **5.1 Comparación: SGD+momentum vs Adam vs AdamW**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd7179a",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_exps = [\n",
    "    (\"SGD+mom\", 0.0, False, \"sgd\", 0.2, 1e-4),\n",
    "    (\"Adam\",    0.0, False, \"adam\", 3e-3, 1e-4),\n",
    "    (\"AdamW\",   0.0, False, \"adamw\", 3e-3, 1e-2),\n",
    "]\n",
    "\n",
    "opt_results = []\n",
    "for tag, dp, bn, optn, lr, wd in opt_exps:\n",
    "    opt_results.append(run_experiment(tag, dp, bn, optn, lr, wd))\n",
    "\n",
    "[(tag, acc) for tag,_,acc in opt_results]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0a2d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "for tag, hist, _ in opt_results:\n",
    "    plt.plot(hist[\"train_loss\"], label=tag)\n",
    "plt.title(\"Entrenamiento: pérdida vs época (optimizadores)\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26cd46d",
   "metadata": {},
   "source": [
    "##### **5.2 Scheduler de ejemplo (cosine)**\n",
    "\n",
    "Un *learning-rate scheduler* ajusta la tasa de aprendizaje (LR) **durante el entrenamiento**. La razón es práctica: con una LR fija, a menudo:\n",
    "\n",
    "- al inicio quieres una LR relativamente alta para **avanzar rápido**,\n",
    "- cerca del final quieres una LR más baja para **refinar** (evitar \"rebotar\" alrededor del mínimo).\n",
    "\n",
    "**CosineAnnealingLR** implementa un descenso suave de la LR siguiendo una curva coseno. En su forma estándar, la LR disminuye desde `lr_inicial` hacia `eta_min` (por defecto 0) en `T_max` pasos:\n",
    "\n",
    "$$\n",
    "\\text{lr}(t) = \\eta_{\\min} + \\frac{1}{2}(\\eta_{\\max}-\\eta_{\\min})\\left(1+\\cos\\left(\\pi \\frac{t}{T_{\\max}}\\right)\\right)\n",
    "$$\n",
    "\n",
    "- $\\eta_{\\max}$: LR inicial (la que le pasas al optimizador, por ejemplo `lr=3e-3`).\n",
    "- $\\eta_{\\min}$: LR mínima (si no se especifica, suele ser 0).\n",
    "- $t$: paso actual del scheduler (aquí lo hacemos por *step*).\n",
    "- $T_{\\max}$: número total de pasos para \"completar\" la caída coseno.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f250d139",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(dropout_p=0.1, use_bn=False).to(device)\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=3e-3, weight_decay=1e-2)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=100)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "lrs, losses = [], []\n",
    "\n",
    "model.train()\n",
    "for step in range(100):\n",
    "    opt.zero_grad()\n",
    "    logits = model(X_train_t)\n",
    "    loss = loss_fn(logits, y_train_t)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    scheduler.step()\n",
    "    lrs.append(opt.param_groups[0][\"lr\"])\n",
    "    losses.append(loss.item())\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.title(\"Loss (con cosine schedule)\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(lrs)\n",
    "plt.title(\"Learning rate (cosine schedule)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c641df5",
   "metadata": {},
   "source": [
    "#### **6. Limitaciones de RNN/LSTM en secuencias largas y motivación hacia atención**\n",
    "\n",
    "##### **6.1 Experimento: norma de gradiente vs longitud en una RNN simple**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9191ebec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyRNN(nn.Module):\n",
    "    def __init__(self, d_in=8, d_h=32, d_out=8):\n",
    "        super().__init__()\n",
    "        self.Wx = nn.Linear(d_in, d_h, bias=False)\n",
    "        self.Wh = nn.Linear(d_h, d_h, bias=False)\n",
    "        self.Wo = nn.Linear(d_h, d_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [T, B, d_in]\n",
    "        h = torch.zeros(x.size(1), self.Wh.in_features, device=x.device)\n",
    "        for t in range(x.size(0)):\n",
    "            h = torch.tanh(self.Wx(x[t]) + self.Wh(h))\n",
    "        return self.Wo(h)\n",
    "\n",
    "def grad_norm_rnn(T=50):\n",
    "    rnn = TinyRNN().to(device)\n",
    "    x = torch.randn(T, 16, 8, device=device)\n",
    "    y = torch.randint(0, 8, (16,), device=device)\n",
    "\n",
    "    logits = rnn(x)\n",
    "    loss = nn.CrossEntropyLoss()(logits, y)\n",
    "    loss.backward()\n",
    "    return rnn.Wh.weight.grad.norm().item()\n",
    "\n",
    "Ts = [10, 20, 40, 80, 120, 160]\n",
    "gn = [grad_norm_rnn(T) for T in Ts]\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(Ts, gn, marker=\"o\")\n",
    "plt.yscale(\"log\")\n",
    "plt.title(\"RNN: norma de gradiente vs longitud T (escala log)\")\n",
    "plt.xlabel(\"T\")\n",
    "plt.ylabel(\"||grad||\")\n",
    "plt.show()\n",
    "\n",
    "gn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab87a37",
   "metadata": {},
   "source": [
    "#### **RNN/LSTM/Seq2Seq (traducción): puente hacia atención**\n",
    "\n",
    "En esta sección integrada trabajamos un modelo **secuencia-a-secuencia** recurrente para traducción y observamos, en práctica, por qué las RNN/LSTM:\n",
    "\n",
    "- se vuelven frágiles con **dependencias largas** (propagación de gradientes y \"cuello de botella\" del estado oculto),\n",
    "- requieren trucos como **teacher forcing** y estrategias de decodificación,\n",
    "- motivan un mecanismo explícito de **atención** para acceder a todo el contexto sin comprimirlo en un único vector.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb713ed",
   "metadata": {},
   "source": [
    "### **Modelos RNN de secuencia a secuencia: Tarea de traducción**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca9ad25",
   "metadata": {},
   "source": [
    "#### **Configuración**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a9b508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todas las librería necesarias para este cuaderno  están listadas a continuación.  \n",
    "\n",
    "#!mamba install -qy numpy==1.21.4 seaborn==0.9.0  \n",
    "#**Nota**: Si tu entorno no admite `!mamba install`, utiliza `!pip install`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afeac5bf",
   "metadata": {},
   "source": [
    "Las siguientes librerías necesarias **no** están preinstaladas en el entorno del curso. **Necesitarás ejecutar la siguiente celda** para instalarlas:  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3000b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch==2.0.0\n",
    "#!pip install spacy==3.7.2\n",
    "#!pip install nltk==3.8.1\n",
    "#!pip install -U matplotlib\n",
    "\n",
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download de_core_news_sm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683a0948",
   "metadata": {},
   "source": [
    "#### **Importación de librerías necesarias**  \n",
    "_Se recomienda importar todas las librerías necesarias en un solo lugar (aquí):_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9488f3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable, List\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Usamos esta sección para suprimir warnings generados por el codigo:\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f3116d",
   "metadata": {},
   "source": [
    "#### **Introducción a las RNN**\n",
    "\n",
    "Las RNN son una clase de redes neuronales diseñadas para procesar datos secuenciales.   Mantienen una memoria interna ($h_t$) para capturar información de pasos anteriores y utilizarla en predicciones actuales.  \n",
    "\n",
    "Las redes neuronales recurrentes (RNN) operan sobre secuencias y utilizan estados anteriores para influir en el estado actual. Aquí está la formulación general de una RNN simple:\n",
    "\n",
    "Dado:\n",
    "\n",
    "- $ \\mathbf{x}_t $: vector de entrada en el instante de tiempo $t$\n",
    "\n",
    "- $ \\mathbf{h}_{t-1} $: vector del estado oculto del paso de tiempo anterior\n",
    "\n",
    "- $ \\mathbf{W}_x $ y $ \\mathbf{W}_h $: matrices de pesos para la entrada y el estado oculto, respectivamente\n",
    "\n",
    "- $ \\mathbf{b} $: vector de sesgo\n",
    "\n",
    "- $\\sigma$: función de activación (a menudo una sigmoide o tanh)\n",
    "\n",
    "Las ecuaciones de actualización para el estado oculto $ \\mathbf{h}_t $ y la salida $ \\mathbf{y}_t $ son las siguientes:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{h}_t &= \\sigma(\\mathbf{W}_x \\cdot \\mathbf{x}_t + \\mathbf{W}_h \\cdot \\mathbf{h}_{t-1} + \\mathbf{b})\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Puede verse que la función de estado oculto depende del estado oculto anterior así como de la entrada en el tiempo $t$, por lo que tiene una memoria colectiva de los pasos anteriores.\n",
    "\n",
    "Para la salida (si estamos haciendo una predicción en cada paso de tiempo):\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{y}_t &= \\text{softmax}(\\mathbf{W}_o \\cdot \\mathbf{h}_t + \\mathbf{b}_o)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "\n",
    "$ \\mathbf{W}_o $: matriz de pesos para la salida Y $ \\mathbf{b}_o $: vector de sesgo para la salida\n",
    "\n",
    "Dependiendo de la tarea específica, una celda RNN puede producir una salida desde $h_t$ o simplemente transferirla a la siguiente celda, actuando como memoria interna.  \n",
    "Aunque la capacidad de la arquitectura para retener memoria pueda parecer esquiva a primera vista, aclaremos esto implementando una RNN simple para manejar el siguiente mecanismo de datos:\n",
    "\n",
    "![a title](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0205EN-SkillsNetwork/Screenshot%202023-10-19%20at%2011.29.23%E2%80%AFAM.png)\n",
    "\n",
    "El diagrama muestra una máquina de estados o modelo de transición con tres estados distintos, representados por los círculos púrpuras prominentes. Cada estado está claramente etiquetado con un valor para $ h $: $ h = -1 $, $ h = 0 $, y $ h = 1 $.\n",
    "\n",
    "1. **Estado $ h = -1 $**:\n",
    "   - Se mantiene en sí mismo cuando $ x = 1 $ (ilustrado por el bucle amarillo).\n",
    "   - Pasa al estado $ h = 0$ al recibir $ x = -1$ (destacado por la flecha roja).\n",
    "\n",
    "2. **Estado $ h = 0 $**:\n",
    "   - Se mueve al estado $h = -1 $ cuando $ x = 1$ (ilustrado por la flecha roja).\n",
    "   - Avanza al estado $ h = 1 $ con $ x = -1$ (marcado por la flecha roja).\n",
    "\n",
    "3. **Estado $h = 1 $**:\n",
    "   - Mantiene su posición cuando $ x = -1 $ (indicado por el bucle amarillo).\n",
    "   - Transiciona al estado $ h = 0 $ al recibir $ x = 1 $ (señalado por la flecha roja).\n",
    "\n",
    "Para resumir, el diagrama representa eficazmente las transiciones entre tres estados basadas en la entrada $ x $.  \n",
    "Dependiendo del estado actual y de la entrada $ x $, la máquina de estados transiciona a un estado diferente o permanece estacionaria."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c8c278",
   "metadata": {},
   "source": [
    "Podemos  representar la máquina de estados mencionada anteriormente, para ello usamos $tanh$ ya que el valor de $h$ debe estar entre `[-1, 1]`. \n",
    "\n",
    ">Tenemos en cuenta que hemos excluido la salida para simplificar.\n",
    "\n",
    "$$\\begin{align*}\n",
    "W_{xh} & = -10.0 \\\\\\\\\\\n",
    "W_{hh} & = 10.0 \\\\\n",
    "b_h & = 0.0 \\\\\n",
    "x_t & = 1 \\\\\n",
    "h_{\\text{prev}} & = 0.0 \\\\\n",
    "h_t & = \\tanh(x_t \\cdot W_{xh} + h_{\\text{prev}} \\cdot W_{hh} + b_h)\n",
    "\\end{align*}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c630a952",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_xh=torch.tensor(-10.0)\n",
    "W_hh=torch.tensor(10.0)\n",
    "b_h=torch.tensor(0.0)\n",
    "x_t=1\n",
    "h_prev=torch.tensor(-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6882f0",
   "metadata": {},
   "source": [
    "Consideramos la siguiente secuencia $x_t$ para  $t=0,1,..,7$,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274f3784",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=[1,1,-1,-1,1,1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24bee23",
   "metadata": {},
   "source": [
    "Asumiendo que comenzamos desde el estado inicial $h = 0$ con el vector de entrada $x$ anterior, el vector de estados $h$ debería verse así:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f163de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "H=[-1,-1,0,1,0,-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5d8b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializa una lista vacía para almacenar los valores del estado predicho\n",
    "H_hat = []\n",
    "\n",
    "# Variable auxiliar para llevar el control del tiempo t\n",
    "t = 1\n",
    "\n",
    "# Itera sobre cada punto de datos en la secuencia de entrada X\n",
    "for x in X:\n",
    "    # Imprime el instante de tiempo actual\n",
    "    print(\"t=\", t)\n",
    "    \n",
    "    # Asigna el valor del punto de datos actual a x_t\n",
    "    x_t = x\n",
    "\n",
    "    # Imprime el valor del estado anterior (h en el tiempo t-1)\n",
    "    print(\"h_t-1\", h_prev.item())\n",
    "\n",
    "    # Calcula el estado actual (h en el tiempo t) utilizando la fórmula de la RNN con activación tanh\n",
    "    h_t = torch.tanh(x_t * W_xh + h_prev * W_hh + b_h)\n",
    "\n",
    "    # Actualiza h_prev con el estado actual para usarlo en la próxima iteración\n",
    "    h_prev = h_t\n",
    "\n",
    "    # Imprime el valor de entrada actual (x en el tiempo t)\n",
    "    print(\"x_t\", x_t)\n",
    "\n",
    "    # Imprime el valor del estado actual (h en el tiempo t)\n",
    "    print(\"h_t\", h_t.item())\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Convierte h_t a entero y lo agrega a la lista H_hat\n",
    "    H_hat.append(int(h_t.item()))\n",
    "\n",
    "    # Incrementa el tiempo\n",
    "    t += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f342498",
   "metadata": {},
   "source": [
    "Podemos evaluar la precisión del estado predicho ```H_hat``` comparándolo con el estado real ```H```. En las RNN, el estado $ h_t $ se utiliza para predecir una secuencia de salida $ y_t $ basada en la secuencia de entrada dada $ x_t $.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62320e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "H_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a947a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "H\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4d6d05",
   "metadata": {},
   "source": [
    "Aunque hemos predefinido los valores de $W_{xh}$, $W_{hh}$ y $b_h$, en la práctica estos valores deben ser determinados mediante entrenamiento con datos. \n",
    "\n",
    "En la práctica, se utilizan con frecuencia modificaciones y mejoras, como LSTM (Long Short-Term Memory) y GRU (Gated Recurrent Units) para abordar problemas como el desvanecimiento del gradiente en las RNN básicas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac6d700",
   "metadata": {},
   "source": [
    "Una celda LSTM tiene tres componentes principales: una puerta de entrada, una puerta de olvido y una puerta de salida.  \n",
    "\n",
    "- La **puerta de entrada** controla cuánta información nueva debe almacenarse en la memoria de la celda. Observa la entrada actual y el estado oculto anterior y decide qué partes de la nueva entrada deben recordarse.  \n",
    "\n",
    "- La **puerta de olvido** determina qué información debe descartarse u olvidarse de la memoria de la celda. Considera la entrada actual y el estado oculto anterior y decide qué partes de la memoria anterior ya no son relevantes.  \n",
    "\n",
    "- La **puerta de salida** determina qué información debe salir de la celda. Observa la entrada actual y el estado oculto anterior y decide qué partes de la memoria de la celda deben incluirse en la salida.\n",
    "\n",
    "La idea clave detrás de las celdas LSTM es que tienen un estado de memoria separado que puede retener o olvidar información selectivamente con el tiempo. Esto les ayuda a manejar dependencias de largo alcance y recordar información importante de pasos anteriores en una secuencia."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11609399",
   "metadata": {},
   "source": [
    "#### **Arquitectura secuencia a secuencia**\n",
    "\n",
    "Los modelos Seq2seq tienen una estructura de codificador-decodificador. El codificador codifica la secuencia de entrada en una representación de dimensión fija, a menudo llamada vector de contexto ($h_t$). El decodificador genera la secuencia de salida basada en el vector de contexto codificado.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c2175a",
   "metadata": {},
   "source": [
    "<video width=\"640\" height=\"480\"\n",
    "src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0205EN-SkillsNetwork/Translation_RNN.mp4\"\n",
    "controls>\n",
    "</video>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5530094",
   "metadata": {},
   "source": [
    "#### **Implementación del codificador en PyTorch**\n",
    "\n",
    "\n",
    "**Nota**: Al usar un LSTM, disponemos de un estado de celda adicional. Sin embargo, si utilizamos un GRU, solo tendríamos el estado oculto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354c59af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_len, emb_dim, hid_dim, n_layers, dropout_prob):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        # Capa de embedding: convierte índices de palabras en vectores de embedding\n",
    "        self.embedding = nn.Embedding(vocab_len, emb_dim)\n",
    "\n",
    "        # Capa LSTM: recibe embeddings y genera salidas y estados\n",
    "        self.lstm = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout_prob)\n",
    "\n",
    "        # Capa de dropout para regularización\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, input_batch):\n",
    "        # input_batch = [longitud de la secuencia fuente, tamaño del lote]\n",
    "\n",
    "        # Aplica embedding seguido de dropout\n",
    "        embed = self.dropout(self.embedding(input_batch))\n",
    "        embed = embed.to(device)\n",
    "\n",
    "        # outputs = [longitud de la secuencia fuente, tamaño del lote, dimensión oculta * número de direcciones]\n",
    "        # hidden = [número de capas * número de direcciones, tamaño del lote, dimensión oculta]\n",
    "        # cell = [número de capas * número de direcciones, tamaño del lote, dimensión oculta]\n",
    "        outputs, (hidden, cell) = self.lstm(embed)\n",
    "\n",
    "        return hidden, cell\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a17f142",
   "metadata": {},
   "source": [
    "Ahora estamos listo para crear una instancia del codificador y ver cómo funciona.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138348a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_len = 8\n",
    "emb_dim = 10\n",
    "hid_dim=8\n",
    "n_layers=1\n",
    "dropout_prob=0.5\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "encoder_t = Encoder(vocab_len, emb_dim, hid_dim, n_layers, dropout_prob).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686da332",
   "metadata": {},
   "source": [
    "Veamos un ejemplo simple donde el método `forward` del codificador transforma la oración `src` en los estados `hidden` y `cell`.  \n",
    "\n",
    "El tensor `tensor([[0],[3],[4],[2],[1]])` es equivalente a `src` = 0,3,4,2,1, en el cual cada número representa un token en el vocabulario de `src`.  \n",
    "\n",
    "Por ejemplo: 0:`<bos>`, 3:\"Das\", 4:\"ist\", 2:\"schön\", 1:`<eos>`.  \n",
    "\n",
    "Tenemos un tamaño de lote (*batch size*) de 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb6e5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_batch = torch.tensor([[0,3,4,2,1]])\n",
    "# Necesitas transponer el tensor de entrada ya que el LSTM del codificador está en modo \"secuencia primero\" por defecto\n",
    "src_batch = src_batch.t().to(device)\n",
    "print(\"Forma del tensor de entrada (src):\", src_batch.shape)\n",
    "hidden_t , cell_t = encoder_t(src_batch)\n",
    "print(\"Tensor oculto (hidden) del codificador:\", hidden_t, \"\\nTensor de celda (cell) del codificador:\", cell_t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ee4faf",
   "metadata": {},
   "source": [
    "El codificador toma toda la secuencia de origen como entrada, la cual consiste en una secuencia de palabras o tokens. El LSTM del codificador procesa toda la secuencia de entrada y actualiza sus estados ocultos en cada paso de tiempo. Los estados ocultos de la red LSTM actúan como una forma de memoria y capturan la información contextual de la secuencia de entrada. Después de procesar toda la secuencia de entrada, el estado oculto final del LSTM del codificador captura la representación resumida del contexto de la secuencia de entrada. Este estado oculto final a veces se conoce como  **vector de contexto**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac325c1f",
   "metadata": {},
   "source": [
    "#### **Implementación del decodificador en PyTorch**\n",
    "\n",
    "Para comprender mejor el mecanismo interno de la parte del decodificador, echemos un vistazo más detallado:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fe2850",
   "metadata": {},
   "source": [
    "<video width=\"640\" height=\"480\"\n",
    "       src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0205EN-SkillsNetwork/decoder_RNN.mp4\"\n",
    "       controls>\n",
    "</video>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441f9640",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.output_dim = output_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        # Capa de embedding para convertir índices en vectores densos\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "\n",
    "        # Capa LSTM que procesa la secuencia embebida\n",
    "        self.lstm = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)\n",
    "\n",
    "        # Capa totalmente conectada que proyecta a la dimensión del vocabulario\n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "\n",
    "        # Capa de softmax logarítmica para obtener probabilidades\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "        # Capa de dropout para regularización\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input, hidden, cell):\n",
    "\n",
    "        # input = [tamaño del lote]\n",
    "\n",
    "        # hidden = [n capas * n direcciones, tamaño del lote, dim oculta]\n",
    "        # cell = [n capas * n direcciones, tamaño del lote, dim oculta]\n",
    "\n",
    "        # En el decodificador, el número de direcciones siempre será 1, por lo tanto:\n",
    "        # hidden = [n capas, tamaño del lote, dim oculta]\n",
    "        # cell = [n capas, tamaño del lote, dim oculta]\n",
    "\n",
    "        # Añade una dimensión de secuencia (tamaño 1) al input\n",
    "        input = input.unsqueeze(0)\n",
    "        # input = [1, tamaño del lote]\n",
    "\n",
    "        # Aplica embedding y dropout\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        # embedded = [1, tamaño del lote, dim embedding]\n",
    "\n",
    "        # Pasa por la LSTM\n",
    "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
    "        # output = [long secuencia, tamaño del lote, dim oculta * n direcciones]\n",
    "        # hidden = [n capas * n direcciones, tamaño del lote, dim oculta]\n",
    "        # cell = [n capas * n direcciones, tamaño del lote, dim oculta]\n",
    "\n",
    "        # Como la longitud de la secuencia y el número de direcciones son 1:\n",
    "        # output = [1, tamaño del lote, dim oculta]\n",
    "        # hidden = [n capas, tamaño del lote, dim oculta]\n",
    "        # cell = [n capas, tamaño del lote, dim oculta]\n",
    "\n",
    "        # Aplica capa lineal y softmax logarítmico para obtener la predicción\n",
    "        prediction_logit = self.fc_out(output.squeeze(0))\n",
    "        prediction = self.softmax(prediction_logit)\n",
    "        # prediction = [tamaño del lote, dimensión de salida]\n",
    "\n",
    "        return prediction, hidden, cell\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb10fee",
   "metadata": {},
   "source": [
    "Podemos crear una instancia del decodificador. La dimensión de salida se establece como la longitud del vocabulario objetivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50ae9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dim = 6\n",
    "emb_dim=10\n",
    "hid_dim = 8\n",
    "n_layers=1\n",
    "dropout=0.5\n",
    "decoder_t = Decoder(output_dim, emb_dim, hid_dim, n_layers, dropout).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927d986c",
   "metadata": {},
   "source": [
    "Ahora que tenemos instancias tanto del codificador como del decodificador, estamos listo para conectarlos (la caja roja en el diagrama a continuación). Primero, veamos cómo podemos pasar los estados *Hidden* y *Cell* (la celda rosada dentro de la caja roja) del codificador (el contenedor de cajas verdes) al decodificador (el contenedor de cajas naranjas). \n",
    "\n",
    "\n",
    "![connection](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0205EN-SkillsNetwork/ED_connection.JPG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c451813b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_t = torch.tensor([0]).to(device) #<bos>\n",
    "input_t.shape\n",
    "prediction, hidden, cell = decoder_t(input_t, hidden_t , cell_t)\n",
    "print(\"Prediccion:\", prediction, '\\nOculto:',hidden,'\\nCelda:', cell)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50945309",
   "metadata": {},
   "source": [
    "#### **Conexión codificador-decodificador** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d5529e",
   "metadata": {},
   "source": [
    "Aprendimos cómo crear los módulos del codificador y del decodificador y cómo pasarles entradas. Ahora necesitamos crear la conexión para que el modelo pueda procesar pares (`src`, `trg`) y generar la traducción. Supongamos que `trg` es el tensor `([[0],[2],[3],[5],[1]])`, lo cual es equivalente a la secuencia 0,2,3,5,1, en la que cada número representa un token en el vocabulario objetivo.\n",
    "\n",
    "Por ejemplo: 0:`<bos>`, 2:\"this\", 3:\"is\", 5:\"beautiful\", 1:`<eos>`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d593df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trg = [longitud de trg, tamaño del lote]\n",
    "# teacher_forcing_ratio es la probabilidad de usar teacher forcing\n",
    "# por ejemplo, si teacher_forcing_ratio es 0.75, usas entradas reales (ground-truth) el 75% del tiempo\n",
    "teacher_forcing_ratio = 0.5\n",
    "trg = torch.tensor([[0],[2],[3],[5],[1]]).to(device)\n",
    "\n",
    "# Obtiene el tamaño del lote desde la dimensión 1 de trg\n",
    "batch_size = trg.shape[1]\n",
    "\n",
    "# Obtiene la longitud de la secuencia trg\n",
    "trg_len = trg.shape[0]\n",
    "\n",
    "# Obtiene el tamaño del vocabulario de salida del decodificador\n",
    "trg_vocab_size = decoder_t.output_dim\n",
    "\n",
    "# Tensor para almacenar las salidas del decodificador\n",
    "outputs_t = torch.zeros(trg_len, batch_size, trg_vocab_size).to(device)\n",
    "\n",
    "# Envía los tensores hidden y cell al dispositivo (CPU o GPU)\n",
    "hidden_t = hidden_t.to(device)\n",
    "cell_t = cell_t.to(device)\n",
    "\n",
    "# la primera entrada para el decodificador es el token <bos>\n",
    "input = trg[0, :]\n",
    "\n",
    "# Bucle sobre la longitud de la secuencia de salida\n",
    "for t in range(1, trg_len):\n",
    "\n",
    "    # Se recorre la longitud de trg y se generan tokens\n",
    "    # El decodificador recibe el token anterior generado, el estado de celda y el estado oculto\n",
    "    # El decodificador produce la predicción (distribución de probabilidad del siguiente token) y actualiza hidden y cell\n",
    "    output_t, hidden_t, cell_t = decoder_t(input, hidden_t, cell_t)\n",
    "\n",
    "    # Guarda la predicción en el tensor que almacena las salidas del decodificador\n",
    "    outputs_t[t] = output_t\n",
    "\n",
    "    # Decide si se usará teacher forcing\n",
    "    teacher_force = random.random() < teacher_forcing_ratio\n",
    "\n",
    "    # Obtiene el token con mayor probabilidad entre las predicciones\n",
    "    top1 = output_t.argmax(1)\n",
    "\n",
    "    # Si se usa teacher forcing, se utiliza el token real como siguiente entrada\n",
    "    # Si no, se utiliza el token predicho\n",
    "    input = trg[t] if teacher_force else top1\n",
    "\n",
    "# Muestra las salidas del decodificador y su forma\n",
    "print(outputs_t, outputs_t.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d8738c",
   "metadata": {},
   "source": [
    "El tamaño del tensor de salida es `(trg_len, batch_size, trg_vocab_size)`. Esto se debe a que, para cada token de `trg` (longitud de `trg`), el modelo genera una distribución de probabilidad sobre todos los posibles tokens (longitud del vocabulario de `trg`). Por lo tanto, para generar los tokens predichos o la traducción de la oración `src`, necesitas obtener la probabilidad máxima para cada token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab124779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ten en cuenta que necesitamos obtener el `argmax` de la segunda dimensión, ya que **outputs** es un \n",
    "# arreglo de tensores de **salida**.\n",
    "pred_tokens = outputs_t.argmax(2)\n",
    "print(pred_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8b3f20",
   "metadata": {},
   "source": [
    "No es sorprendente que la traducción no sea correcta (`trg = tensor([[0],[2],[3],[5],[1]])`), ya que el modelo aún no ha pasado por ningún entrenamiento. Vamos a reunir todo el código para conectar el codificador y el decodificador en una clase `seq2seq` para una mejor usabilidad.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc77ff1",
   "metadata": {},
   "source": [
    "#### **Implementación del modelo secuencia a secuencia en PyTorch**\n",
    "\n",
    "Vamos a conectar los componentes del codificador y el decodificador para crear el modelo `seq2seq`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678cb9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device, trg_vocab):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        self.trg_vocab = trg_vocab\n",
    "\n",
    "        # Asegúrate de que las dimensiones ocultas del codificador y del decodificador sean iguales\n",
    "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
    "            \"¡Las dimensiones ocultas del codificador y del decodificador deben ser iguales!\"\n",
    "        # Asegúrate de que ambos tengan el mismo número de capas\n",
    "        assert encoder.n_layers == decoder.n_layers, \\\n",
    "            \"¡El codificador y el decodificador deben tener el mismo número de capas!\"\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
    "        # src = [longitud de src, tamaño del lote]\n",
    "        # trg = [longitud de trg, tamaño del lote]\n",
    "        # teacher_forcing_ratio es la probabilidad de usar teacher forcing\n",
    "        # por ejemplo, si teacher_forcing_ratio es 0.75, usas la entrada real el 75% del tiempo\n",
    "\n",
    "        batch_size = trg.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "\n",
    "        # Tensor para almacenar las salidas del decodificador\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "\n",
    "        # El último estado oculto del codificador se usa como estado inicial del decodificador\n",
    "        hidden, cell = self.encoder(src)\n",
    "        hidden = hidden.to(device)\n",
    "        cell = cell.to(device)\n",
    "\n",
    "        # la primera entrada del decodificador son los tokens <bos>\n",
    "        input = trg[0, :]\n",
    "\n",
    "        for t in range(1, trg_len):\n",
    "\n",
    "            # Inserta el embedding del token de entrada, estado oculto y de celda anteriores\n",
    "            # Recibe el tensor de salida (predicciones) y los nuevos estados oculto y de celda\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "\n",
    "            # Guarda las predicciones en un tensor que almacena las predicciones para cada token\n",
    "            outputs[t] = output\n",
    "\n",
    "            # Decide si se va a usar teacher forcing\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "\n",
    "            # Obtiene el token con mayor probabilidad de las predicciones\n",
    "            top1 = output.argmax(1)\n",
    "\n",
    "            # Si se usa teacher forcing, usa el siguiente token real como entrada\n",
    "            # Si no, usa el token predicho\n",
    "            input = trg[t] if teacher_force else top1\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f32772",
   "metadata": {},
   "source": [
    "#### **Entrenamiento del modelo en PyTorch**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92159b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(modelo, iterator, optimizer, criterion, clip):\n",
    "\n",
    "    # Establece el modelo en modo de entrenamiento\n",
    "    modelo.train()\n",
    "\n",
    "    # Inicializa la pérdida acumulada de la época\n",
    "    epoch_loss = 0\n",
    "\n",
    "    # Envuelve el iterador con tqdm para mostrar el progreso durante el entrenamiento\n",
    "    train_iterator = tqdm(iterator, desc=\"Entrenando\", leave=False)\n",
    "\n",
    "    # Itera sobre los lotes del conjunto de entrenamiento\n",
    "    for i, (src, trg) in enumerate(iterator):\n",
    "\n",
    "        # Envía los datos al dispositivo (CPU o GPU)\n",
    "        src = src.to(device)\n",
    "        trg = trg.to(device)\n",
    "\n",
    "        # Reinicia los gradientes del optimizador\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Ejecuta el modelo sobre las secuencias de entrada y objetivo\n",
    "        output = modelo(src, trg)\n",
    "\n",
    "        # trg = [longitud de trg, tamaño del lote]\n",
    "        # output = [longitud de trg, tamaño del lote, dimensión de salida]\n",
    "\n",
    "        output_dim = output.shape[-1]\n",
    "\n",
    "        # Excluye el primer token (<bos>) de la salida y reestructura el tensor\n",
    "        output = output[1:].view(-1, output_dim)\n",
    "\n",
    "        # Excluye también el primer token (<bos>) de trg y lo convierte en un tensor contiguo plano\n",
    "        trg = trg[1:].contiguous().view(-1)\n",
    "\n",
    "        # trg = [(longitud de trg - 1) * tamaño del lote]\n",
    "        # output = [(longitud de trg - 1) * tamaño del lote, dimensión de salida]\n",
    "\n",
    "        # Calcula la pérdida entre la salida del modelo y el valor real\n",
    "        loss = criterion(output, trg)\n",
    "\n",
    "        # Propagación hacia atrás: calcula los gradientes\n",
    "        loss.backward()\n",
    "\n",
    "        # Aplica recorte de gradientes para evitar explosiones de gradientes\n",
    "        torch.nn.utils.clip_grad_norm_(modelo.parameters(), clip)\n",
    "\n",
    "        # Actualiza los parámetros del modelo\n",
    "        optimizer.step()\n",
    "\n",
    "        # Actualiza la barra de progreso de tqdm con la pérdida actual\n",
    "        train_iterator.set_postfix(loss=loss.item())\n",
    "\n",
    "        # Acumula la pérdida del lote actual\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    # Devuelve la pérdida promedio por lote en toda la época\n",
    "    return epoch_loss / len(list(iterator))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a85b60c",
   "metadata": {},
   "source": [
    "#### **Evaluación del modelo en PyTorch**\n",
    "\n",
    "También necesitamos definir una función para evaluar el modelo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0763f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(modelo, iterator, criterion):\n",
    "\n",
    "    # Establece el modelo en modo evaluación\n",
    "    modelo.eval()\n",
    "\n",
    "    # Inicializa la pérdida acumulada de la época\n",
    "    epoch_loss = 0\n",
    "\n",
    "    # Envuelve el iterador con tqdm para mostrar el progreso durante la evaluación\n",
    "    valid_iterator = tqdm(iterator, desc=\"Evaluando\", leave=False)\n",
    "\n",
    "    # Desactiva el cálculo de gradientes para ahorrar memoria y acelerar la evaluación\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # Itera sobre los lotes del conjunto de evaluación\n",
    "        for i, (src, trg) in enumerate(iterator):\n",
    "\n",
    "            # Envía los datos al dispositivo (CPU o GPU)\n",
    "            src = src.to(device)\n",
    "            trg = trg.to(device)\n",
    "\n",
    "            # Ejecuta el modelo con teacher forcing desactivado\n",
    "            output = modelo(src, trg, 0)  # desactiva el teacher forcing\n",
    "\n",
    "            # trg = [longitud de trg, tamaño del lote]\n",
    "            # output = [longitud de trg, tamaño del lote, dimensión de salida]\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "\n",
    "            # Excluye el primer token (<bos>) y reestructura la salida\n",
    "            output = output[1:].view(-1, output_dim)\n",
    "\n",
    "            # Excluye también el primer token (<bos>) de trg y lo convierte en tensor plano\n",
    "            trg = trg[1:].contiguous().view(-1)\n",
    "\n",
    "            # trg = [(longitud de trg - 1) * tamaño del lote]\n",
    "            # output = [(longitud de trg - 1) * tamaño del lote, dimensión de salida]\n",
    "\n",
    "            # Calcula la pérdida entre la salida del modelo y el valor real\n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            # Actualiza la barra de progreso de tqdm con la pérdida actual\n",
    "            valid_iterator.set_postfix(loss=loss.item())\n",
    "\n",
    "            # Acumula la pérdida del lote actual\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    # Devuelve la pérdida promedio por lote durante la evaluación\n",
    "    return epoch_loss / len(list(iterator))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe5f2fd",
   "metadata": {},
   "source": [
    "### **Tarea de traducción**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f09471-153e-428e-8df5-726c34ec37ce",
   "metadata": {},
   "source": [
    "#### **1. Preprocesamiento con spaCy + dataset mínimo (EN->DE)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e9ebfe-3885-4730-a836-e5c5d88841ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import spacy\n",
    "\n",
    "# Tokenizadores spaCy (EN y DE)\n",
    "spacy_en = spacy.load(\"en_core_web_sm\")\n",
    "spacy_de = spacy.load(\"de_core_news_sm\")\n",
    "\n",
    "def tokenize_en(texto: str):\n",
    "    \"\"\"Tokeniza en inglés (minúsculas).\"\"\"\n",
    "    return [tok.text.lower() for tok in spacy_en.tokenizer(texto)]\n",
    "\n",
    "def tokenize_de(texto: str):\n",
    "    \"\"\"Tokeniza en alemán (minúsculas).\"\"\"\n",
    "    return [tok.text.lower() for tok in spacy_de.tokenizer(texto)]\n",
    "\n",
    "#  Dataset paralelo pequeño\n",
    "pairs = [\n",
    "    (\"i am a student\", \"ich bin ein student\"),\n",
    "    (\"i am a teacher\", \"ich bin ein lehrer\"),\n",
    "    (\"he is a student\", \"er ist ein student\"),\n",
    "    (\"she is a teacher\", \"sie ist eine lehrerin\"),\n",
    "    (\"i like apples\", \"ich mag äpfel\"),\n",
    "    (\"i like books\", \"ich mag bücher\"),\n",
    "    (\"we like apples\", \"wir mögen äpfel\"),\n",
    "    (\"they like books\", \"sie mögen bücher\"),\n",
    "    (\"good morning\", \"guten morgen\"),\n",
    "    (\"good night\", \"gute nacht\"),\n",
    "    (\"thank you\", \"danke\"),\n",
    "    (\"you are welcome\", \"bitte\"),\n",
    "    (\"how are you\", \"wie geht es dir\"),\n",
    "    (\"i am fine\", \"mir geht es gut\"),\n",
    "]\n",
    "\n",
    "# Split determinístico (evita que justo tus ejemplos queden en valid y el modelo colapse) ---\n",
    "train_pairs = pairs[:12]\n",
    "valid_pairs = pairs[12:]\n",
    "\n",
    "print(\"Tamaño train:\", len(train_pairs))\n",
    "print(\"Tamaño valid:\", len(valid_pairs))\n",
    "print(\"\\nEjemplos TRAIN:\", train_pairs[:3])\n",
    "print(\"Ejemplos VALID:\", valid_pairs[:3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def10f27-c7f3-43e9-8845-3b519409d086",
   "metadata": {},
   "source": [
    "#### **2. Vocabulario propio**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72410f3c-90ec-40f8-9e52-40f25445776d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokens especiales\n",
    "PAD = \"<pad>\"\n",
    "BOS = \"<bos>\"\n",
    "EOS = \"<eos>\"\n",
    "UNK = \"<unk>\"\n",
    "\n",
    "class Vocab:\n",
    "    \"\"\"Vocabulario mínimo: stoi/itos + encode/decode.\"\"\"\n",
    "    def __init__(self, listas_tokens, min_freq=1):\n",
    "        freq = {}\n",
    "        for toks in listas_tokens:\n",
    "            for t in toks:\n",
    "                freq[t] = freq.get(t, 0) + 1\n",
    "\n",
    "        # Orden fijo: especiales primero\n",
    "        self.itos = [PAD, BOS, EOS, UNK]\n",
    "\n",
    "        # Resto por frecuencia y orden alfabético\n",
    "        for t, c in sorted(freq.items(), key=lambda x: (-x[1], x[0])):\n",
    "            if c >= min_freq and t not in self.itos:\n",
    "                self.itos.append(t)\n",
    "\n",
    "        self.stoi = {t: i for i, t in enumerate(self.itos)}\n",
    "\n",
    "        self.pad_idx = self.stoi[PAD]\n",
    "        self.bos_idx = self.stoi[BOS]\n",
    "        self.eos_idx = self.stoi[EOS]\n",
    "        self.unk_idx = self.stoi[UNK]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "\n",
    "    def encode(self, tokens):\n",
    "        return [self.stoi.get(t, self.unk_idx) for t in tokens]\n",
    "\n",
    "    def decode(self, ids):\n",
    "        return [self.itos[i] if i < len(self.itos) else UNK for i in ids]\n",
    "\n",
    "#  Construir vocab SOLO con train (práctica correcta)\n",
    "src_token_lists = [tokenize_en(s) for s, _ in train_pairs]\n",
    "trg_token_lists = [tokenize_de(t) for _, t in train_pairs]\n",
    "\n",
    "SRC_VOCAB = Vocab(src_token_lists, min_freq=1)\n",
    "TRG_VOCAB = Vocab(trg_token_lists, min_freq=1)\n",
    "\n",
    "print(\"SRC vocab size:\", len(SRC_VOCAB))\n",
    "print(\"TRG vocab size:\", len(TRG_VOCAB))\n",
    "print(\"TRG PAD idx:\", TRG_VOCAB.pad_idx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed6b3f1-7947-49c9-b432-20a0fde60a59",
   "metadata": {},
   "source": [
    "#### **3.Dataset + DataLoader (tensors en formato [seq_len, batch])**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3dbf3ae-00d7-48cd-ac73-9174ca32a618",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def numericalize_src(texto_en: str):\n",
    "    \"\"\"Convierte oración EN a ids con <bos> ... <eos>.\"\"\"\n",
    "    toks = tokenize_en(texto_en)\n",
    "    ids = [SRC_VOCAB.bos_idx] + SRC_VOCAB.encode(toks) + [SRC_VOCAB.eos_idx]\n",
    "    return torch.tensor(ids, dtype=torch.long)\n",
    "\n",
    "def numericalize_trg(texto_de: str):\n",
    "    \"\"\"Convierte oración DE a ids con <bos> ... <eos>.\"\"\"\n",
    "    toks = tokenize_de(texto_de)\n",
    "    ids = [TRG_VOCAB.bos_idx] + TRG_VOCAB.encode(toks) + [TRG_VOCAB.eos_idx]\n",
    "    return torch.tensor(ids, dtype=torch.long)\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, pares):\n",
    "        self.pares = pares\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pares)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src, trg = self.pares[idx]\n",
    "        return numericalize_src(src), numericalize_trg(trg)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Pad a la longitud máxima del batch. Salida: [seq_len, batch].\"\"\"\n",
    "    src_list, trg_list = zip(*batch)\n",
    "    src_pad = pad_sequence(src_list, padding_value=SRC_VOCAB.pad_idx)\n",
    "    trg_pad = pad_sequence(trg_list, padding_value=TRG_VOCAB.pad_idx)\n",
    "    return src_pad, trg_pad\n",
    "\n",
    "train_ds = TranslationDataset(train_pairs)\n",
    "valid_ds = TranslationDataset(valid_pairs)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "valid_loader = DataLoader(valid_ds, batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Sanity check de formas\n",
    "src_b, trg_b = next(iter(train_loader))\n",
    "print(\"SRC batch shape:\", src_b.shape, \" (src_len, batch)\")\n",
    "print(\"TRG batch shape:\", trg_b.shape, \" (trg_len, batch)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0babe606-3fab-4973-b5cf-e089c66efc8e",
   "metadata": {},
   "source": [
    "#### **4. Entrenamiento Seq2Seq (SIN atención) + \"overfit check\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae49eaa6-c60c-418c-909a-ff1fb07d9c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "def train_with_tf(modelo, iterator, optimizer, criterion, clip, teacher_forcing_ratio=1.0):\n",
    "    \"\"\"Entrena 1 época, pudiendo fijar teacher forcing ratio explícitamente.\"\"\"\n",
    "    modelo.train()\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    for src, trg in iterator:\n",
    "        src = src.to(device)\n",
    "        trg = trg.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Importante: usamos teacher_forcing_ratio controlado\n",
    "        output = modelo(src, trg, teacher_forcing_ratio=teacher_forcing_ratio)\n",
    "\n",
    "        output_dim = output.shape[-1]\n",
    "\n",
    "        # Quitamos <bos> para loss\n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg = trg[1:].contiguous().view(-1)\n",
    "\n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(modelo.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "# Hiperparámetros\n",
    "INPUT_DIM  = len(SRC_VOCAB)\n",
    "OUTPUT_DIM = len(TRG_VOCAB)\n",
    "\n",
    "ENC_EMB_DIM = 64\n",
    "DEC_EMB_DIM = 64\n",
    "HID_DIM = 128\n",
    "N_LAYERS = 1\n",
    "\n",
    "# Dropout en 0 para que memorice fácil (solo para prueba)\n",
    "ENC_DROPOUT = 0.0\n",
    "DEC_DROPOUT = 0.0\n",
    "\n",
    "CLIP = 1.0\n",
    "\n",
    "# Modelo: usa TUS clases Encoder/Decoder/Seq2Seq\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT).to(device)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT).to(device)\n",
    "model = Seq2Seq(enc, dec, device, TRG_VOCAB).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.NLLLoss(ignore_index=TRG_VOCAB.pad_idx)\n",
    "\n",
    "# Overfit check: debería bajar fuerte la loss en train\n",
    "for epoch in range(1, 201):\n",
    "    train_loss = train_with_tf(model, train_loader, optimizer, criterion, CLIP, teacher_forcing_ratio=1.0)\n",
    "    if epoch % 25 == 0:\n",
    "        print(f\"Epoca {epoch:03d} | train loss {train_loss:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bd1b69-d881-4dc7-a340-51a851a8253e",
   "metadata": {},
   "source": [
    "#### **5. Inferencia (greedy decoding) SIN atención**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12876276-5cc2-40a2-bc76-f57836f729c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_greedy(modelo, sentence_en: str, max_len=20):\n",
    "    \"\"\"Traduce EN->DE usando greedy decoding.\"\"\"\n",
    "    modelo.eval()\n",
    "    with torch.no_grad():\n",
    "        # src: [src_len] -> [src_len, 1]\n",
    "        src = numericalize_src(sentence_en).unsqueeze(1).to(device)\n",
    "\n",
    "        hidden, cell = modelo.encoder(src)\n",
    "\n",
    "        # Comenzamos con <bos>\n",
    "        input_tok = torch.tensor([TRG_VOCAB.bos_idx], device=device)\n",
    "\n",
    "        out_ids = [TRG_VOCAB.bos_idx]\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            output, hidden, cell = modelo.decoder(input_tok, hidden, cell)\n",
    "            top1 = output.argmax(1).item()\n",
    "            out_ids.append(top1)\n",
    "            input_tok = torch.tensor([top1], device=device)\n",
    "\n",
    "            if top1 == TRG_VOCAB.eos_idx:\n",
    "                break\n",
    "\n",
    "    tokens = TRG_VOCAB.decode(out_ids)[1:]  # quitamos <bos>\n",
    "    if EOS in tokens:\n",
    "        tokens = tokens[:tokens.index(EOS)]\n",
    "    return tokens\n",
    "\n",
    "# Probar sobre TRAIN (debería funcionar mejor tras overfit)\n",
    "print(\"PRUEBAS (TRAIN)\")\n",
    "for s, _ in train_pairs[:6]:\n",
    "    print(s, \"->\", \" \".join(translate_greedy(model, s)))\n",
    "\n",
    "print(\"\\nPRUEBAS (VALID)\")\n",
    "for s, _ in valid_pairs:\n",
    "    print(s, \"->\", \" \".join(translate_greedy(model, s)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e0df34-1152-41bc-bf6e-eaa0cdf7a648",
   "metadata": {},
   "source": [
    "#### **6. Evaluación BLEU (con smoothing para frases cortas)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40985704-a30d-4566-8b5e-d3f05f21609a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "smooth = SmoothingFunction().method1\n",
    "\n",
    "print(\"BLEU (VALID)\")\n",
    "for s, ref in valid_pairs:\n",
    "    hyp = translate_greedy(model, s)\n",
    "    bleu = sentence_bleu([tokenize_de(ref)], hyp, smoothing_function=smooth)\n",
    "    print(f\"EN: {s} | REF: {ref} | HYP: {' '.join(hyp)} | BLEU: {bleu:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd0e7cd-4080-48da-8f7c-db35d9f66c0e",
   "metadata": {},
   "source": [
    "#### **7. Atención + Encoder/Decoder con atención**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc6b3e4-fcc0-4fbd-b577-105e3292aa59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \"\"\"Atención aditiva (tipo Bahdanau).\"\"\"\n",
    "    def __init__(self, hid_dim):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear(hid_dim * 2, hid_dim)\n",
    "        self.v = nn.Linear(hid_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        # hidden: [n_layers, batch, hid_dim]\n",
    "        # encoder_outputs: [src_len, batch, hid_dim]\n",
    "        src_len = encoder_outputs.shape[0]\n",
    "\n",
    "        # Tomamos la última capa del hidden\n",
    "        hidden_last = hidden[-1].unsqueeze(0).repeat(src_len, 1, 1)  # [src_len, batch, hid_dim]\n",
    "\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden_last, encoder_outputs), dim=2)))  # [src_len,batch,hid]\n",
    "        scores = self.v(energy).squeeze(2).transpose(0, 1)  # [batch, src_len]\n",
    "        return torch.softmax(scores, dim=1)  # [batch, src_len]\n",
    "\n",
    "\n",
    "class EncoderAttn(nn.Module):\n",
    "    \"\"\"Encoder LSTM que devuelve outputs por token (para atención).\"\"\"\n",
    "    def __init__(self, vocab_len, emb_dim, hid_dim, n_layers, dropout_prob):\n",
    "        super().__init__()\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Embedding(vocab_len, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout_prob)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, input_batch):\n",
    "        embedded = self.dropout(self.embedding(input_batch)).to(device)  # [src_len,batch,emb]\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)                   # outputs: [src_len,batch,hid]\n",
    "        return outputs, hidden, cell\n",
    "\n",
    "\n",
    "class DecoderAttn(nn.Module):\n",
    "    \"\"\"Decoder LSTM que usa atención sobre encoder_outputs.\"\"\"\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout, attention: Attention):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.attention = attention\n",
    "\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        # El LSTM recibe [embedding + contexto]\n",
    "        self.lstm = nn.LSTM(emb_dim + hid_dim, hid_dim, n_layers, dropout=dropout)\n",
    "\n",
    "        # Proyección a vocab: usamos output + contexto + embedding\n",
    "        self.fc_out = nn.Linear(hid_dim * 2 + emb_dim, output_dim)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input, hidden, cell, encoder_outputs):\n",
    "        # input: [batch] -> [1,batch]\n",
    "        input = input.unsqueeze(0)\n",
    "        embedded = self.dropout(self.embedding(input))  # [1,batch,emb]\n",
    "\n",
    "        # Pesos de atención: [batch, src_len]\n",
    "        attn_weights = self.attention(hidden, encoder_outputs)          # [batch,src_len]\n",
    "        attn_weights_bmm = attn_weights.unsqueeze(1)                    # [batch,1,src_len]\n",
    "\n",
    "        # encoder_outputs a [batch,src_len,hid]\n",
    "        enc = encoder_outputs.transpose(0, 1)\n",
    "        context = torch.bmm(attn_weights_bmm, enc)                      # [batch,1,hid]\n",
    "        context = context.transpose(0, 1)                               # [1,batch,hid]\n",
    "\n",
    "        rnn_input = torch.cat((embedded, context), dim=2)               # [1,batch,emb+hid]\n",
    "        output, (hidden, cell) = self.lstm(rnn_input, (hidden, cell))   # output: [1,batch,hid]\n",
    "\n",
    "        output = output.squeeze(0)    # [batch,hid]\n",
    "        context = context.squeeze(0)  # [batch,hid]\n",
    "        embedded = embedded.squeeze(0)# [batch,emb]\n",
    "\n",
    "        logits = self.fc_out(torch.cat((output, context, embedded), dim=1))\n",
    "        prediction = self.softmax(logits)\n",
    "\n",
    "        return prediction, hidden, cell, attn_weights  # attn_weights: [batch, src_len]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83360c2-3897-489e-abe3-d9e86bf8a664",
   "metadata": {},
   "source": [
    "##### **7.1 Seq2Seq con atención**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b9f478-4cd3-4877-a167-274abbf4ef2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqAttn(nn.Module):\n",
    "    \"\"\"Seq2Seq con atención: encoder devuelve outputs completos.\"\"\"\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "        assert encoder.hid_dim == decoder.hid_dim\n",
    "        assert encoder.n_layers == decoder.n_layers\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        batch_size = trg.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "\n",
    "        encoder_outputs, hidden, cell = self.encoder(src)\n",
    "\n",
    "        input = trg[0, :]  # <bos>\n",
    "\n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden, cell, _ = self.decoder(input, hidden, cell, encoder_outputs)\n",
    "            outputs[t] = output\n",
    "\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(1)\n",
    "            input = trg[t] if teacher_force else top1\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecf45e3-e576-4527-bbbc-23d0b4eb2f31",
   "metadata": {},
   "source": [
    "##### **7.2 Entrenamiento con atención**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf75456d-a032-4fa8-ab99-4cc52daf2aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = Attention(HID_DIM).to(device)\n",
    "\n",
    "enc_a = EncoderAttn(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, 0.0).to(device)\n",
    "dec_a = DecoderAttn(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, 0.0, attn).to(device)\n",
    "\n",
    "model_attn = Seq2SeqAttn(enc_a, dec_a, device).to(device)\n",
    "\n",
    "optimizer_a = optim.Adam(model_attn.parameters(), lr=1e-3)\n",
    "criterion_a = nn.NLLLoss(ignore_index=TRG_VOCAB.pad_idx)\n",
    "\n",
    "for epoch in range(1, 201):\n",
    "    train_loss = train_with_tf(model_attn, train_loader, optimizer_a, criterion_a, CLIP, teacher_forcing_ratio=1.0)\n",
    "    if epoch % 25 == 0:\n",
    "        print(f\"[Attn] Epoca {epoch:03d} | train loss {train_loss:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75561225-8434-415c-9da3-c1caccb6bf84",
   "metadata": {},
   "source": [
    "#### **8. Inferencia con atención + heatmap de alineamientos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f8bcc9-3c67-4ac1-ab71-3c70afd9010d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_greedy_attn(modelo, sentence_en: str, max_len=20):\n",
    "    \"\"\"Traduce EN->DE y devuelve matriz de atención (para visualizar alineamientos).\"\"\"\n",
    "    modelo.eval()\n",
    "    attn_rows = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        src = numericalize_src(sentence_en).unsqueeze(1).to(device)  # [src_len,1]\n",
    "        encoder_outputs, hidden, cell = modelo.encoder(src)\n",
    "\n",
    "        input_tok = torch.tensor([TRG_VOCAB.bos_idx], device=device)\n",
    "        out_ids = [TRG_VOCAB.bos_idx]\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            output, hidden, cell, attn_w = modelo.decoder(input_tok, hidden, cell, encoder_outputs)\n",
    "            top1 = output.argmax(1).item()\n",
    "            out_ids.append(top1)\n",
    "\n",
    "            # attn_w: [batch=1, src_len] -> guardamos [src_len]\n",
    "            attn_rows.append(attn_w.squeeze(0).detach().cpu().numpy())\n",
    "\n",
    "            input_tok = torch.tensor([top1], device=device)\n",
    "            if top1 == TRG_VOCAB.eos_idx:\n",
    "                break\n",
    "\n",
    "    tokens = TRG_VOCAB.decode(out_ids)[1:]\n",
    "    if EOS in tokens:\n",
    "        tokens = tokens[:tokens.index(EOS)]\n",
    "\n",
    "    attn_mat = np.stack(attn_rows, axis=0) if len(attn_rows) else None\n",
    "    return tokens, attn_mat\n",
    "\n",
    "#  Prueba + visualización\n",
    "frase = \"i like books\"\n",
    "hyp, attn_mat = translate_greedy_attn(model_attn, frase)\n",
    "print(frase, \"->\", \" \".join(hyp))\n",
    "\n",
    "# Ejes del heatmap\n",
    "src_tokens = [BOS] + tokenize_en(frase) + [EOS]\n",
    "trg_tokens = hyp\n",
    "\n",
    "if attn_mat is not None:\n",
    "    plt.figure()\n",
    "    plt.imshow(attn_mat, aspect=\"auto\")\n",
    "    plt.xticks(range(len(src_tokens)), src_tokens, rotation=45, ha=\"right\")\n",
    "    plt.yticks(range(len(trg_tokens)), trg_tokens)\n",
    "    plt.xlabel(\"Tokens fuente (EN)  ->  (keys/values del encoder)\")\n",
    "    plt.ylabel(\"Pasos del decoder (DE)  ->  (queries)\")\n",
    "    plt.title(\"Alineamiento por atención (Bahdanau)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23edd80b",
   "metadata": {},
   "source": [
    "#### **Conexión directa con la Atención y el Transformer**\n",
    "\n",
    "Qué debes quedarte de Seq2Seq recurrente:\n",
    "\n",
    "1. **Bottleneck**: comprimir toda la fuente en un único estado (o una trayectoria de estados) limita la capacidad.\n",
    "2. **Long-range**: la señal de gradiente y la información útil \"se diluyen\" a medida que crece la longitud.\n",
    "3. **Atención** resuelve ambos: en cada paso, el decodificador **consulta** (pondera) todos los estados del codificador.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f777ebbe-0de4-43aa-bdfe-26344ff9cb0d",
   "metadata": {},
   "source": [
    "### **Ejercicios**\n",
    "\n",
    "Aquí tienes **enunciados de ejercicios** que puedes insertar directamente en el cuaderno, alineados con **cada tema** (Perceptrón/MLP, activaciones/pérdida, backprop/gradientes, regularización, optimizadores+scheduler, RNN/LSTM y motivación a atención).\n",
    "\n",
    "#### **1. Perceptrón, MLP, activaciones y función de pérdida**\n",
    "\n",
    "1. **Perceptrón -> Regresión logística (clasificación binaria).** Modifica el perceptrón \"a mano\" para que use salida sigmoide y entrene minimizando *binary cross-entropy*. Reporta: curva de pérdida, accuracy y frontera de decisión.\n",
    "2. **MLP en `make_moons`: capacidad vs sobreajuste.** Entrena el MLP variando el número de neuronas ocultas (por ejemplo, 8, 32, 128) y la profundidad (1 vs 3 capas). Entrega una tabla con *train acc*, *val acc* y brecha de generalización.\n",
    "3. **Función de pérdida y clases desbalanceadas.** Crea un desbalance artificial en `make_moons` (submuestreo de una clase) y compara: (a) CrossEntropy estándar, (b) pérdida ponderada por clase. Reporta métricas por clase (precision/recall/F1).\n",
    "\n",
    "#### **2. Entrenamiento y retropropagación: papel de los gradientes**\n",
    "\n",
    "4. **Ablación de gradientes: ¿qué capas aprenden primero?** Durante el entrenamiento del MLP, registra por época la norma del gradiente de cada capa (pesos) y grafica su evolución. Interpreta qué capas dominan el aprendizaje y por qué.\n",
    "5. **Diagnóstico de saturación.** Repite el entrenamiento sustituyendo ReLU por sigmoid/tanh y mide: (a) norma promedio del gradiente por época, (b) tiempo hasta alcanzar una accuracy objetivo. Explica la relación entre activación y vanishing gradients.\n",
    "6. **Escala de inicialización.** Cambia la inicialización de pesos (más pequeña vs más grande) y evalúa estabilidad del entrenamiento (exploding/vanishing). Reporta un gráfico de pérdida y norma de gradiente.\n",
    "\n",
    "#### **3. Verificación de retropropagación (diferencias finitas)**\n",
    "\n",
    "7. **Gradient check robusto.** Extiende la verificación por diferencias finitas para probar **N parámetros aleatorios** (por ejemplo, 50) en distintas capas y reporta el **máximo error relativo**. Compara resultados en `float32` vs `float64`.\n",
    "8. **Detectar un bug de autograd.** Introduce intencionalmente un error (por ejemplo,, `detach()` en una activación o en una rama del cómputo) y muestra que el gradient check falla. Explica qué cambió y por qué.\n",
    "\n",
    "#### **4. Regularización y generalización (L2, dropout, batch norm, early stopping)**\n",
    "\n",
    "9. **Comparación sistemática (tabla obligatoria).** Entrena 4 variantes del MLP: (a) baseline, (b) L2/weight decay, (c) dropout, (d) batch norm. Mantén el resto fijo. Entrega una tabla con: mejor *val acc*, época del mejor modelo, brecha train–val, y comentario breve.\n",
    "10. **Early stopping con sensibilidad.** Implementa early stopping con `patience ∈ {3, 10, 20}` y `min_delta` (por ejemplo, 0 y 1e-3). Reporta cómo cambia: (a) época de parada, (b) val acc final, (c) varianza entre corridas.\n",
    "11. **Regularización y calibración.** Evalúa calibración del clasificador (por ejemplo,, ECE o reliability diagram) para baseline vs L2 vs dropout. Concluye cuál regulariza mejor \"probabilidades\" y no solo accuracy.\n",
    "\n",
    "#### **5. Optimizadores modernos y programación de tasa de aprendizaje**\n",
    "\n",
    "12. **SGD+momentum vs Adam vs AdamW (fair comparison).** Para cada optimizador, realiza una pequeña búsqueda de LR (por ejemplo, 3 valores) y reporta: mejor configuración, curvas de pérdida y val acc. Concluye cuál converge más rápido y cuál generaliza mejor.\n",
    "13. **Scheduler: cosine con y sin warmup.** Añade warmup (por ejemplo, 5% de épocas) antes del cosine annealing. Compara: estabilidad inicial, pérdida temprana y rendimiento final.\n",
    "14. **Weight decay: Adam vs AdamW.** Manteniendo el mismo `weight_decay`, compara Adam vs AdamW y discute por qué no son equivalentes. Reporta diferencias en norma de pesos y brecha train-val.\n",
    "\n",
    "#### **6. Limitaciones de RNN/LSTM y motivación hacia atención**\n",
    "\n",
    "15. **Norma de gradiente vs longitud: RNN vs LSTM vs GRU.** Repite el experimento de secuencias largas con los tres modelos y grafica norma de gradiente vs longitud. Concluye cuál es más estable y en qué rango de longitudes.\n",
    "16. **Seq2Seq sin atención: \"overfit check\" + generalización.** (a) Fuerza overfit en un subconjunto mínimo y verifica que BLEU se acerque a 1.0 en train. (b) Luego sube el tamaño del dataset y mide BLEU en validación; analiza errores típicos (orden, palabras desconocidas, omisiones).\n",
    "17. **Ablación de atención: impacto real.** Entrena el modelo Seq2Seq **con** y **sin** atención bajo el mismo presupuesto (épocas/params). Reporta BLEU y 3 ejemplos cualitativos donde atención mejora (y 1 donde no).\n",
    "18. **Heatmaps de alineamiento: diagnóstico.** Genera heatmaps para al menos 5 frases y etiqueta patrones: alineamiento diagonal, saltos, repeticiones. Propón un cambio (por ejemplo, dropout en atención o ajuste de capacidad) y verifica si mejora los patrones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8684849a-582d-466c-a8f4-2559e94ba726",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tus respuestas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
