{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0f6b36d",
   "metadata": {},
   "source": [
    "### **Paradigmas de aprendizaje (Supervisado, Few‑shot, No Supervisado, Autosupervisado/Contrastivo, Refuerzo)**\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76a5687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración\n",
    "import os, math, random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, adjusted_rand_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "SEED = 7\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"device:\", device)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7141700",
   "metadata": {},
   "source": [
    "### **Dataset: Digits (8×8)**\n",
    "Usaremos `sklearn.datasets.load_digits()` para evitar descargas. Es pequeño, perfecto para demos rápidas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3248d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = load_digits()\n",
    "X = digits.data.astype(np.float32)          # (n, 64)\n",
    "y = digits.target.astype(np.int64)          # (n,)\n",
    "images = digits.images.astype(np.float32)   # (n, 8, 8)\n",
    "\n",
    "# Normalización a [0,1] (los dígitos vienen típicamente en [0,16])\n",
    "X01 = X / 16.0\n",
    "images01 = images / 16.0\n",
    "\n",
    "print(\"X01:\", X01.shape, \"y:\", y.shape)\n",
    "\n",
    "# Visualiza 12 ejemplos\n",
    "idx = np.random.choice(len(X01), size=12, replace=False)\n",
    "fig, axes = plt.subplots(3, 4, figsize=(6, 4))\n",
    "for ax, i in zip(axes.ravel(), idx):\n",
    "    ax.imshow(images01[i], cmap=\"gray\", vmin=0, vmax=1)\n",
    "    ax.set_title(str(y[i]))\n",
    "    ax.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0657de06",
   "metadata": {},
   "source": [
    "#### **2. Aprendizaje supervisado**\n",
    "##### **Idea**\n",
    "Aprende una función $f(x) \\rightarrow y$ usando pares etiquetados $(x, y)$. En el curso, esto sirve como baseline y para discutir:\n",
    "- particionado train/test,\n",
    "- métricas (accuracy, precision/recall),\n",
    "- sobreajuste y regularización.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defd04b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X01, y, test_size=0.25, random_state=SEED, stratify=y\n",
    ")\n",
    "\n",
    "# Pipeline típico: escalado -> modelo\n",
    "clf = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"lr\", LogisticRegression(max_iter=3000))\n",
    "])\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred, digits=3))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()\n",
    "plt.title(\"Matriz de confusión (LogReg)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378759dd",
   "metadata": {},
   "source": [
    "#### **Mini‑experimento: ¿qué pasa si tengo pocos datos etiquetados?**\n",
    "Esto conecta con el *few‑shot como régimen de pocos datos* (pero **no** es lo mismo que meta‑learning).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01f277b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_label_budget(label_budget_per_class: int):\n",
    "    # Mantén solo K ejemplos por clase en entrenamiento\n",
    "    keep = []\n",
    "    for c in np.unique(y_train):\n",
    "        idx_c = np.where(y_train == c)[0]\n",
    "        np.random.shuffle(idx_c)\n",
    "        keep.extend(idx_c[:label_budget_per_class])\n",
    "    keep = np.array(keep)\n",
    "\n",
    "    Xb, yb = X_train[keep], y_train[keep]\n",
    "\n",
    "    model = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"lr\", LogisticRegression(max_iter=3000))\n",
    "    ])\n",
    "    model.fit(Xb, yb)\n",
    "    acc = model.score(X_test, y_test)\n",
    "    return acc\n",
    "\n",
    "for k in [1, 2, 5, 10, 30, 100]:\n",
    "    acc = train_with_label_budget(k)\n",
    "    print(f\"K={k:3d} labels/clase  ->  accuracy={acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a9c3a6",
   "metadata": {},
   "source": [
    "#### **3. Few‑shot: N‑way, K‑shot (evaluación episódica)**\n",
    "##### **Definiciones operativas**\n",
    "- **N‑way**: número de clases en el episodio (por ejemplo, 5‑way).\n",
    "- **K‑shot**: número de ejemplos etiquetados por clase en el *support set* (por ejemplo, 1‑shot).\n",
    "- *Query set*: ejemplos a clasificar, sin etiquetas.\n",
    "\n",
    "Aquí implementamos un evaluador tipo **Prototypical Networks** (prototipos = media de embeddings del support).\n",
    "\n",
    "Para mostrar el valor del pretraining, compararemos embeddings:\n",
    "1) **Baseline**: píxeles\n",
    "2) **Autoencoder** (autosupervisado)\n",
    "3) **Contrastivo** (autosupervisado)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a7e969-d5ab-4ca8-ba24-0d38e6133a02",
   "metadata": {},
   "source": [
    "**Prototypical Networks**\n",
    "\n",
    "Un **evaluador tipo Prototypical Networks** es una forma de hacer *clasificación few-shot* donde, en vez de entrenar un clasificador completo desde cero para cada tarea, construyes un \"representante\" (**prototipo**) por clase usando los pocos ejemplos disponibles.\n",
    "\n",
    "**Support set (conjunto de soporte)**\n",
    "\n",
    "En un episodio few-shot divides los datos en dos partes:\n",
    "\n",
    "* **Support set**: los ejemplos **etiquetados** que te \"dan\" para aprender en ese episodio.\n",
    " - Ejemplo: en **5-way 1-shot**, el support set tiene **5 clases × 1 ejemplo por clase = 5 imágenes** con etiqueta.\n",
    "* **Query set**: ejemplos **a clasificar** (normalmente sin etiqueta para el modelo; tú sí la tienes para evaluar).\n",
    " - Ejemplo: 5-way 1-shot con 10 queries por clase -> 50 imágenes para probar.\n",
    "\n",
    "#### **¿Qué es un prototipo?**\n",
    "\n",
    "Primero pasas cada ejemplo por un **encoder** que lo convierte en un **embedding** (vector) en un espacio donde \"cosas parecidas quedan cerca\".\n",
    "\n",
    "Para cada clase (c), tomas los embeddings de sus ejemplos en el support set y calculas el **promedio**:\n",
    "\n",
    "$$\n",
    "\\mathbf{p}*c = \\frac{1}{K}\\sum*{i=1}^{K} \\mathbf{z}_i^{(c)}\n",
    "$$\n",
    "\n",
    "* $\\mathbf{z}_i^{(c)}$: embedding del i-ésimo ejemplo del support de la clase (c)\n",
    "* $K$: número de *shots* por clase (K-shot)\n",
    "* $\\mathbf{p}_c$: **prototipo** (centro) de la clase (c)\n",
    "\n",
    "Intuición: el prototipo es como el **centroide** de la clase en el espacio de embeddings.\n",
    "\n",
    "#### **¿Cómo clasifica el evaluador?**\n",
    "\n",
    "Para cada ejemplo del **query set**:\n",
    "\n",
    "1. Calculas su embedding $\\mathbf{z}_q$.\n",
    "2. Mides distancia a cada prototipo (típicamente **euclídea** o **coseno**):\n",
    "   $$\n",
    "   \\hat{y} = \\arg\\min_c ; d(\\mathbf{z}_q,\\mathbf{p}_c)\n",
    "   $$\n",
    "3. Predices la clase del prototipo más cercano.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bd6c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def prototypical_episode(embeddings, labels, n_way=5, k_shot=1, q_query=10):\n",
    "    \"\"\"\n",
    "    embeddings: (n, d) torch tensor\n",
    "    labels: (n,) torch tensor long\n",
    "    \"\"\"\n",
    "    classes = labels.unique().cpu().numpy().tolist()\n",
    "    chosen = random.sample(classes, n_way)\n",
    "\n",
    "    support_idx = []\n",
    "    query_idx = []\n",
    "\n",
    "    for c in chosen:\n",
    "        idx_c = (labels == c).nonzero(as_tuple=False).view(-1).cpu().numpy()\n",
    "        np.random.shuffle(idx_c)\n",
    "        support_idx.extend(idx_c[:k_shot])\n",
    "        query_idx.extend(idx_c[k_shot:k_shot + q_query])\n",
    "\n",
    "    support_idx = torch.tensor(support_idx, dtype=torch.long, device=embeddings.device)\n",
    "    query_idx   = torch.tensor(query_idx, dtype=torch.long, device=embeddings.device)\n",
    "\n",
    "    E_s = embeddings[support_idx]  # (n_way*k, d)\n",
    "    y_s = labels[support_idx]      # (n_way*k,)\n",
    "    E_q = embeddings[query_idx]\n",
    "    y_q = labels[query_idx]\n",
    "\n",
    "    # prototipos: media por clase\n",
    "    protos = []\n",
    "    proto_labels = []\n",
    "    for c in chosen:\n",
    "        mask = (y_s == c)\n",
    "        protos.append(E_s[mask].mean(dim=0))\n",
    "        proto_labels.append(int(c))\n",
    "    protos = torch.stack(protos, dim=0)  # (n_way, d)\n",
    "\n",
    "    # distancia euclídea al prototipo\n",
    "    dists = ((E_q.unsqueeze(1) - protos.unsqueeze(0))**2).sum(dim=-1)\n",
    "    pred = dists.argmin(dim=1)  # índice en [0, n_way)\n",
    "    pred_labels = torch.tensor([proto_labels[i] for i in pred.cpu().numpy()], device=embeddings.device)\n",
    "\n",
    "    acc = (pred_labels == y_q).float().mean().item()\n",
    "    return acc\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_fewshot(embeddings, labels, n_way=5, k_shot=1, q_query=10, episodes=200):\n",
    "    accs = []\n",
    "    for _ in range(episodes):\n",
    "        accs.append(prototypical_episode(embeddings, labels, n_way, k_shot, q_query))\n",
    "    return float(np.mean(accs)), float(np.std(accs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79c9187",
   "metadata": {},
   "source": [
    "##### **3.1 Baseline: embeddings = píxeles normalizados**\n",
    "No es un modelo, es una referencia mínima.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1a30d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "E_pixels = torch.tensor(X01, device=device)  # (n, 64)\n",
    "Y_t = torch.tensor(y, device=device)\n",
    "\n",
    "for (n_way, k_shot) in [(5,1), (5,5), (10,1), (10,5)]:\n",
    "    mean_acc, std_acc = evaluate_fewshot(E_pixels, Y_t, n_way=n_way, k_shot=k_shot, episodes=200)\n",
    "    print(f\"[pixels] {n_way}-way {k_shot}-shot  acc={mean_acc:.3f} ± {std_acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3d8104",
   "metadata": {},
   "source": [
    "#### **4. No supervisado: estructura, segmentos y anomalías**\n",
    "##### **4.1 Estructura: clustering (KMeans)**\n",
    "Buscamos agrupamientos sin usar etiquetas. Luego (solo para la demo) comparamos con etiquetas verdaderas usando **ARI**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbe58bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "kmeans = KMeans(n_clusters=k, n_init=20, random_state=SEED)\n",
    "clusters = kmeans.fit_predict(X01)\n",
    "\n",
    "ari = adjusted_rand_score(y, clusters)\n",
    "print(\"ARI (KMeans vs etiquetas reales):\", round(ari, 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74047e6",
   "metadata": {},
   "source": [
    "##### **4.2 Segmentos: segmentación simple de una imagen (KMeans en pixeles)**\n",
    "\n",
    "Ejemplo minimalista: separa *fondo vs trazo* en un dígito 8×8.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d650cbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = int(np.random.choice(len(images01)))\n",
    "img = images01[i].copy()\n",
    "pixels = img.reshape(-1, 1)  # (64,1)\n",
    "\n",
    "seg = KMeans(n_clusters=2, n_init=10, random_state=SEED).fit_predict(pixels)\n",
    "seg_img = seg.reshape(8, 8)\n",
    "\n",
    "# Asegura que el \"trazo\" sea 1 (por intensidad media)\n",
    "if img[seg_img==0].mean() > img[seg_img==1].mean():\n",
    "    seg_img = 1 - seg_img\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(7, 2.5))\n",
    "axes[0].imshow(img, cmap=\"gray\", vmin=0, vmax=1); axes[0].set_title(f\"Original (y={y[i]})\"); axes[0].axis(\"off\")\n",
    "axes[1].imshow(seg_img, cmap=\"gray\"); axes[1].set_title(\"Máscara (2 clusters)\"); axes[1].axis(\"off\")\n",
    "axes[2].imshow(img * seg_img, cmap=\"gray\", vmin=0, vmax=1); axes[2].set_title(\"Trazo extraído\"); axes[2].axis(\"off\")\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9238e1d",
   "metadata": {},
   "source": [
    "##### **4.3 Anomalías: detección con Isolation Forest**\n",
    "Creamos outliers sintéticos (ruido) y vemos si el detector los puntúa como anómalos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbc2a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_out = 150\n",
    "outliers = np.clip(np.random.rand(n_out, 64).astype(np.float32), 0, 1)  # ruido uniforme\n",
    "X_mix = np.vstack([X01, outliers])\n",
    "y_is_outlier = np.array([0]*len(X01) + [1]*n_out)  # 1 si es outlier\n",
    "\n",
    "iso = IsolationForest(n_estimators=300, contamination=n_out/len(X_mix), random_state=SEED)\n",
    "iso.fit(X_mix)\n",
    "\n",
    "scores = -iso.decision_function(X_mix)  # mayor = más anómalo\n",
    "thr = np.quantile(scores, 1 - n_out/len(X_mix))\n",
    "pred_out = (scores >= thr).astype(int)\n",
    "\n",
    "tp = int(((pred_out==1) & (y_is_outlier==1)).sum())\n",
    "fp = int(((pred_out==1) & (y_is_outlier==0)).sum())\n",
    "fn = int(((pred_out==0) & (y_is_outlier==1)).sum())\n",
    "\n",
    "precision = tp / max(tp+fp, 1)\n",
    "recall = tp / max(tp+fn, 1)\n",
    "print(f\"precision={precision:.3f}, recall={recall:.3f} (sobre outliers sintéticos)\")\n",
    "\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.hist(scores[y_is_outlier==0], bins=40, alpha=0.7, label=\"inliers (digits)\")\n",
    "plt.hist(scores[y_is_outlier==1], bins=40, alpha=0.7, label=\"outliers (ruido)\")\n",
    "plt.axvline(thr, linestyle=\"--\")\n",
    "plt.title(\"Scores de anomalía (IsolationForest)\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b80169",
   "metadata": {},
   "source": [
    "#### **5. Autosupervisado: motor del pretraining (autoencoder)**\n",
    "En autosupervisado, **la señal viene de los propios datos**. En un autoencoder, la tarea pretexto es **reconstruir x**.\n",
    "\n",
    "Luego usamos el **encoder** como extractor de embeddings para:\n",
    "- N‑way K‑shot,\n",
    "- clasificación con pocos labels (linear probe).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2371a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AE(nn.Module):\n",
    "    def __init__(self, d_in=64, d_lat=32):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(d_in, 128), nn.ReLU(),\n",
    "            nn.Linear(128, d_lat)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(d_lat, 128), nn.ReLU(),\n",
    "            nn.Linear(128, d_in), nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        xhat = self.decoder(z)\n",
    "        return xhat, z\n",
    "\n",
    "def train_ae(X, epochs=15, batch_size=128, lr=1e-3):\n",
    "    model = AE(d_in=X.shape[1], d_lat=32).to(device)\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    ds = TensorDataset(torch.tensor(X, dtype=torch.float32))\n",
    "    dl = DataLoader(ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for ep in range(1, epochs+1):\n",
    "        model.train()\n",
    "        losses = []\n",
    "        for (xb,) in dl:\n",
    "            xb = xb.to(device)\n",
    "            xhat, _ = model(xb)\n",
    "            loss = F.mse_loss(xhat, xb)\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            losses.append(loss.item())\n",
    "        if ep % 5 == 0 or ep == 1:\n",
    "            print(f\"Epoca {ep:02d}  recon_mse={np.mean(losses):.5f}\")\n",
    "    return model\n",
    "\n",
    "ae = train_ae(X01, epochs=15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bef4879",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def encode_with(model, X):\n",
    "    model.eval()\n",
    "    X_t = torch.tensor(X, dtype=torch.float32, device=device)\n",
    "    _, z = model(X_t)\n",
    "    return z\n",
    "\n",
    "Z_ae = encode_with(ae, X01)  # (n, d_lat)\n",
    "\n",
    "# Muestra reconstrucciones\n",
    "ae.eval()\n",
    "idx = np.random.choice(len(X01), size=8, replace=False)\n",
    "xb = torch.tensor(X01[idx], dtype=torch.float32, device=device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    xhat, _ = ae(xb)\n",
    "\n",
    "xhat = xhat.cpu().numpy().reshape(-1, 8, 8)\n",
    "\n",
    "fig, axes = plt.subplots(2, 8, figsize=(12, 3))\n",
    "for j in range(8):\n",
    "    axes[0, j].imshow(images01[idx[j]], cmap=\"gray\", vmin=0, vmax=1); axes[0, j].axis(\"off\")\n",
    "    axes[1, j].imshow(xhat[j], cmap=\"gray\", vmin=0, vmax=1); axes[1, j].axis(\"off\")\n",
    "axes[0,0].set_ylabel(\"orig\", rotation=0, labelpad=25)\n",
    "axes[1,0].set_ylabel(\"recon\", rotation=0, labelpad=25)\n",
    "plt.suptitle(\"Autoencoder: original vs reconstrucción\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f38c9e6",
   "metadata": {},
   "source": [
    "##### **5.1 Few‑shot con embeddings del autoencoder**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebc8874",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (n_way, k_shot) in [(5,1), (5,5), (10,1), (10,5)]:\n",
    "    mean_acc, std_acc = evaluate_fewshot(Z_ae, Y_t, n_way=n_way, k_shot=k_shot, episodes=200)\n",
    "    print(f\"[AE emb] {n_way}-way {k_shot}-shot  acc={mean_acc:.3f} ± {std_acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b057c1",
   "metadata": {},
   "source": [
    "##### **5.2 Clasificación supervisada con *pocos labels* + encoder congelado**\n",
    "Entrenamos un clasificador lineal sobre embeddings **Z** usando solo K etiquetas por clase.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fdca88",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_np = Z_ae.detach().cpu().numpy()\n",
    "Z_train, Z_test, y_train2, y_test2 = train_test_split(Z_np, y, test_size=0.25, random_state=SEED, stratify=y)\n",
    "\n",
    "def linear_probe_with_budget(Ztr, ytr, Zte, yte, label_budget_per_class: int):\n",
    "    keep = []\n",
    "    for c in np.unique(ytr):\n",
    "        idx_c = np.where(ytr == c)[0]\n",
    "        np.random.shuffle(idx_c)\n",
    "        keep.extend(idx_c[:label_budget_per_class])\n",
    "    keep = np.array(keep)\n",
    "\n",
    "    model = LogisticRegression(max_iter=3000)\n",
    "    model.fit(Ztr[keep], ytr[keep])\n",
    "    return model.score(Zte, yte)\n",
    "\n",
    "for k in [1, 2, 5, 10, 30, 100]:\n",
    "    acc = linear_probe_with_budget(Z_train, y_train2, Z_test, y_test2, k)\n",
    "    print(f\"[AE+linear] K={k:3d} labels/clase -> accuracy={acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfaaa98",
   "metadata": {},
   "source": [
    "#### **6. Autosupervisado: señal contrastiva (contrastive learning)**\n",
    "\n",
    "**Autosupervisado** es aprender representaciones **sin etiquetas humanas**, creando una \"señal\" desde los propios datos. En Digits, no usamos la clase (0-9) para entrenar, en vez de eso, definimos una tarea pretexto: *dos versiones del mismo dígito deben parecerse en el embedding*.\n",
    "\n",
    "Ahí entra **contrastive learning**: es un tipo de autosupervisado donde la señal es **comparar pares**. En estilo **SimCLR** con Digits:\n",
    "\n",
    "1. Tomamos una imagen (x) (por ejemplo. un \"3\").\n",
    "2. Generamos dos **vistas** $(v_1, v_2)$ con augmentations (ruido, masking de píxeles).\n",
    "3. Pasamos ambas por el mismo encoder (f$\\cdot$) y obtenemos embeddings $(z_1, z_2)$.\n",
    "4. La pérdida **NT-Xent** obliga a que $z_1$ y $z_2$ estén **cerca** (par positivo: mismo ejemplo) y que se alejen de embeddings de otras imágenes del batch (pares negativos: ejemplos distintos).\n",
    "\n",
    "Resultado: el encoder aprende un espacio donde \"dígitos similares\" tienden a agruparse, y luego sirve para **few-shot** o clasificación con pocos labels.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de593782",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, d_in=64, d_emb=32):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_in, 128), nn.ReLU(),\n",
    "            nn.Linear(128, d_emb)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "def augment_digits(x):\n",
    "    \"\"\"x: (batch, 64) en [0,1]\"\"\"\n",
    "    noise = 0.10 * torch.randn_like(x)\n",
    "    x2 = (x + noise).clamp(0, 1)\n",
    "    # masking aleatorio de 10% de pixeles\n",
    "    mask = (torch.rand_like(x2) < 0.10).float()\n",
    "    x2 = x2 * (1 - mask)\n",
    "    return x2\n",
    "\n",
    "def nt_xent(z1, z2, tau=0.2):\n",
    "    z1 = F.normalize(z1, dim=1)\n",
    "    z2 = F.normalize(z2, dim=1)\n",
    "    B = z1.size(0)\n",
    "\n",
    "    z = torch.cat([z1, z2], dim=0)  # (2B, d)\n",
    "    sim = torch.matmul(z, z.T) / tau  # (2B, 2B)\n",
    "\n",
    "    diag = torch.eye(2*B, device=z.device).bool()\n",
    "    sim = sim.masked_fill(diag, -1e9)\n",
    "\n",
    "    # positivos: (i, i+B) y (i+B, i)\n",
    "    pos = torch.cat([torch.arange(B, 2*B), torch.arange(0, B)]).to(z.device)\n",
    "    return F.cross_entropy(sim, pos)\n",
    "\n",
    "def train_contrastive(X, epochs=25, batch_size=256, lr=2e-3):\n",
    "    enc = Encoder(d_in=X.shape[1], d_emb=32).to(device)\n",
    "    opt = torch.optim.AdamW(enc.parameters(), lr=lr)\n",
    "\n",
    "    ds = TensorDataset(torch.tensor(X, dtype=torch.float32))\n",
    "    dl = DataLoader(ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "    for ep in range(1, epochs+1):\n",
    "        enc.train()\n",
    "        losses = []\n",
    "        for (xb,) in dl:\n",
    "            xb = xb.to(device)\n",
    "            v1 = augment_digits(xb)\n",
    "            v2 = augment_digits(xb)\n",
    "\n",
    "            z1 = enc(v1)\n",
    "            z2 = enc(v2)\n",
    "\n",
    "            loss = nt_xent(z1, z2, tau=0.2)\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            losses.append(loss.item())\n",
    "        if ep % 5 == 0 or ep == 1:\n",
    "            print(f\"Epoca {ep:02d}  nt_xent={np.mean(losses):.4f}\")\n",
    "    return enc\n",
    "\n",
    "enc = train_contrastive(X01, epochs=25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c9e773",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def encode_enc(enc, X):\n",
    "    enc.eval()\n",
    "    X_t = torch.tensor(X, dtype=torch.float32, device=device)\n",
    "    return enc(X_t)\n",
    "\n",
    "Z_con = encode_enc(enc, X01)\n",
    "\n",
    "for (n_way, k_shot) in [(5,1), (5,5), (10,1), (10,5)]:\n",
    "    mean_acc, std_acc = evaluate_fewshot(Z_con, Y_t, n_way=n_way, k_shot=k_shot, episodes=200)\n",
    "    print(f\"[Contrast emb] {n_way}-way {k_shot}-shot  acc={mean_acc:.3f} ± {std_acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0de813",
   "metadata": {},
   "source": [
    "#### **7. Aprendizaje por refuerzo: GridWorld + Q‑learning**\n",
    "Implementación desde cero para discutir MDP, política, valor y aprendizaje por interacción.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6155a2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "    def __init__(self, n=5):\n",
    "        self.n = n\n",
    "        self.start = (0, 0)\n",
    "        self.goal = (n-1, n-1)\n",
    "        self.traps = {(1, 3), (2, 1), (3, 3)}\n",
    "        self.walls = {(1, 1), (2, 3)}  # no se puede entrar\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.s = self.start\n",
    "        return self.state_id(self.s)\n",
    "\n",
    "    def state_id(self, s):\n",
    "        return s[0] * self.n + s[1]\n",
    "\n",
    "    def step(self, a):\n",
    "        r, c = self.s\n",
    "        drdc = {0:(-1,0), 1:(0,1), 2:(1,0), 3:(0,-1)}[a]\n",
    "        nr, nc = r + drdc[0], c + drdc[1]\n",
    "\n",
    "        nr = max(0, min(self.n-1, nr))\n",
    "        nc = max(0, min(self.n-1, nc))\n",
    "\n",
    "        if (nr, nc) in self.walls:\n",
    "            nr, nc = r, c\n",
    "\n",
    "        self.s = (nr, nc)\n",
    "\n",
    "        reward = -0.01\n",
    "        done = False\n",
    "        if self.s in self.traps:\n",
    "            reward = -1.0\n",
    "            done = True\n",
    "        elif self.s == self.goal:\n",
    "            reward = 1.0\n",
    "            done = True\n",
    "\n",
    "        return self.state_id(self.s), reward, done\n",
    "\n",
    "env = GridWorld(n=5)\n",
    "nS = env.n * env.n\n",
    "nA = 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2377a009",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(env, episodes=4000, alpha=0.15, gamma=0.98, eps0=1.0, eps_min=0.05, eps_decay=0.999):\n",
    "    Q = np.zeros((nS, nA), dtype=np.float32)\n",
    "    rewards = []\n",
    "\n",
    "    eps = eps0\n",
    "    for ep in range(episodes):\n",
    "        s = env.reset()\n",
    "        ep_ret = 0.0\n",
    "\n",
    "        for t in range(200):\n",
    "            if np.random.rand() < eps:\n",
    "                a = np.random.randint(nA)\n",
    "            else:\n",
    "                a = int(np.argmax(Q[s]))\n",
    "\n",
    "            s2, r, done = env.step(a)\n",
    "            ep_ret += r\n",
    "\n",
    "            Q[s, a] = Q[s, a] + alpha * (r + gamma * np.max(Q[s2]) - Q[s, a])\n",
    "            s = s2\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        rewards.append(ep_ret)\n",
    "        eps = max(eps_min, eps * eps_decay)\n",
    "\n",
    "    return Q, np.array(rewards)\n",
    "\n",
    "Q, rets = q_learning(env)\n",
    "\n",
    "window = 100\n",
    "mov = np.convolve(rets, np.ones(window)/window, mode=\"valid\")\n",
    "\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.plot(mov)\n",
    "plt.title(f\"Q-learning: retorno promedio móvil (ventana={window})\")\n",
    "plt.xlabel(\"episodio\")\n",
    "plt.ylabel(\"retorno\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0542d48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_policy(env, Q):\n",
    "    arrows = {0:\"↑\", 1:\"→\", 2:\"↓\", 3:\"←\"}\n",
    "    grid = []\n",
    "    for r in range(env.n):\n",
    "        row = []\n",
    "        for c in range(env.n):\n",
    "            if (r,c) == env.start:\n",
    "                row.append(\"S\")\n",
    "            elif (r,c) == env.goal:\n",
    "                row.append(\"G\")\n",
    "            elif (r,c) in env.walls:\n",
    "                row.append(\"█\")\n",
    "            elif (r,c) in env.traps:\n",
    "                row.append(\"X\")\n",
    "            else:\n",
    "                sid = env.state_id((r,c))\n",
    "                row.append(arrows[int(np.argmax(Q[sid]))])\n",
    "        grid.append(row)\n",
    "    return grid\n",
    "\n",
    "pol = render_policy(env, Q)\n",
    "for row in pol:\n",
    "    print(\" \".join(row))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390a54bb",
   "metadata": {},
   "source": [
    "#### **8. Ejercicios**\n",
    "\n",
    "##### **Aprendizaje Supervisado**\n",
    "\n",
    "1. **MLPClassifier vs LogReg (trade-off precisión/tiempo)**\n",
    "\n",
    "   * Reemplaza `LogisticRegression` por `sklearn.neural_network.MLPClassifier`.\n",
    "   * Prueba 3 configuraciones: `hidden_layer_sizes=(32,)`, `(64,)`, `(128,64)` y activa `early_stopping=True`.\n",
    "   * Reporta: `accuracy`, tiempo de entrenamiento (usa `time.perf_counter()`), y una matriz de confusión.\n",
    "   * Discute cuándo conviene un MLP \"pequeño\" frente a un modelo lineal en Digits.\n",
    "\n",
    "2. **Calibración de probabilidades (reliability)**\n",
    "\n",
    "   * Compara calibración de LogReg y MLP con `CalibratedClassifierCV` (métodos `sigmoid` y `isotonic`).\n",
    "   * Mide **Brier score** y dibuja **reliability diagram** (curva de calibración).\n",
    "   * Identifica si el modelo es *overconfident* (probabilidades altas incorrectas) o *underconfident*.\n",
    "\n",
    "##### **Few-shot (N-way/K-shot con prototipos)**\n",
    "\n",
    "3. **Curva K-shot (con barras de error)**\n",
    "\n",
    "   * Para `N ∈ {5, 10}` y `K ∈ {1,2,5,10}`, ejecuta `evaluate_fewshot(..., episodes=500)` y guarda `mean±std`.\n",
    "   * Grafica 4 curvas: (5-way y 10-way) × (embeddings píxeles vs embeddings AE vs embeddings contrastivos).\n",
    "   * Observa qué representación \"sube más rápido\" al aumentar K.\n",
    "\n",
    "4. **Cosine distance + normalización (ablation)**\n",
    "\n",
    "   * Modifica el episodio prototípico para usar **cosine distance**: normaliza `E_q` y `protos` con `F.normalize`.\n",
    "   * Compara contra euclídea para las 4 combinaciones (N,K).\n",
    "   * Discute por qué cosine suele ayudar cuando los embeddings están aprendidos y normalizados.\n",
    "\n",
    "##### **Aprendizaje no supervisado**\n",
    "\n",
    "5. **Clustering: sensibilidad a k + estabilidad**\n",
    "\n",
    "   * Evalúa `k ∈ {8,9,10,11,12}` con `n_init` alto.\n",
    "   * Reporta: inercia (`kmeans.inertia_`) y (solo para la demo) ARI.\n",
    "   * Repite con 5 semillas distintas y mide varianza: ¿qué tan estable es el agrupamiento?\n",
    "\n",
    "6. **Anomalías: outliers \"difíciles\" y degradación**\n",
    "\n",
    "   * Reemplaza outliers de \"ruido puro\" por perturbaciones *cercanas* a Digits:\n",
    "\n",
    "     (a) masking fuerte de píxeles, (b) ruido gaussiano suave, (c) inversión parcial (1−x) en regiones.\n",
    "   * Mantén la misma tasa de contaminación y reporta precisión/recall de detección.\n",
    "   * Explica qué tipo de outlier engaña más al detector y por qué.\n",
    "\n",
    "##### **Autosupervisado**\n",
    "\n",
    "7. **Autoencoder: capacidad latente y utilidad real**\n",
    "\n",
    "   * Entrena AE con `d_lat ∈ {8,16,32,64}`.\n",
    "   * Para cada uno: (a) MSE de reconstrucción, (b) few-shot (5-way 1-shot y 10-way 1-shot), (c) linear probe con K=5 labels/clase.\n",
    "\n",
    "8. **Contrastivo: temperatura $\\tau$ y diseño de augmentations (ablation serio)**\n",
    "\n",
    "   * Entrena 3 modelos con `tau ∈ {0.1,0.2,0.5}` manteniendo todo lo demás fijo.\n",
    "   * Luego fija τ y cambia augmentations:\n",
    "\n",
    "      * solo ruido, solo masking, ruido+masking.\n",
    "   * Evalúa few-shot y compara. Conclusión esperada: *augmentations definen qué invariancias aprende el embedding*.\n",
    "\n",
    "##### **Aprendizaje por Refuerzo**\n",
    "\n",
    "9. **Shaping de recompensas y política emergente**\n",
    "\n",
    "   * Cambia la penalidad por paso `{-0.00, -0.01, -0.05}` y añade (opcional) penalidad por chocar pared.\n",
    "   * Para cada setting: curva de retorno promedio móvil y política final impresa.\n",
    "   * Interpreta: ¿se vuelve más \"agresiva\" buscando la meta? ¿evita trampas antes?\n",
    "\n",
    "10. **Epsilon-greedy: decaimiento lineal vs exponencial**\n",
    "\n",
    "    * Implementa ε lineal: de 1.0 a 0.05 en, por ejemplo, 60% de episodios; luego constante.\n",
    "    * Compara contra el decaimiento exponencial actual (tu baseline).\n",
    "    * Reporta: velocidad de convergencia (episodio donde el retorno se estabiliza) y robustez (varianza entre 3 semillas).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff1c70c-b8f2-45a8-9ebc-4d836fa7d810",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tus respuestas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
