{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bb328b4",
   "metadata": {},
   "source": [
    "### **Autoencoders, VAE, GANs y Difusión + DiT**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f043b2",
   "metadata": {},
   "source": [
    "#### **0. Preparación del entorno**\n",
    "\n",
    "> Si faltan librerías, descomenta la instalación.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa98ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -q torch torchvision datasets diffusers transformers accelerate matplotlib numpy pillow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342f3d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd23d2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    if getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "device = get_device()\n",
    "print(\"Dispositivo:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5332d786",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images_grid(images, ncols=8, title=None, cmap=\"gray\"):\n",
    "    if isinstance(images, torch.Tensor):\n",
    "        imgs = images.detach().cpu()\n",
    "        if imgs.ndim == 2:\n",
    "            imgs = imgs.unsqueeze(0).unsqueeze(0)\n",
    "        elif imgs.ndim == 3:\n",
    "            imgs = imgs.unsqueeze(1)\n",
    "    else:\n",
    "        proc = []\n",
    "        for im in images:\n",
    "            arr = np.array(im) if isinstance(im, Image.Image) else np.array(im)\n",
    "            if arr.ndim == 2:\n",
    "                arr = arr[None, ...]\n",
    "            elif arr.ndim == 3 and arr.shape[-1] in (1, 3):\n",
    "                arr = np.transpose(arr, (2, 0, 1))\n",
    "            proc.append(torch.tensor(arr))\n",
    "        imgs = torch.stack(proc)\n",
    "\n",
    "    imgs = imgs.float()\n",
    "    if imgs.max() > 1.5:\n",
    "        imgs = imgs / 255.0\n",
    "\n",
    "    n = len(imgs)\n",
    "    nrows = math.ceil(n / ncols)\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(1.6*ncols, 1.6*nrows))\n",
    "    axes = np.array(axes).reshape(-1)\n",
    "    for ax in axes: ax.axis(\"off\")\n",
    "\n",
    "    for i in range(n):\n",
    "        img = imgs[i]\n",
    "        if img.shape[0] == 1:\n",
    "            axes[i].imshow(img[0], cmap=cmap)\n",
    "        else:\n",
    "            axes[i].imshow(np.transpose(img.numpy(), (1,2,0)))\n",
    "    if title:\n",
    "        fig.suptitle(title, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23146573",
   "metadata": {},
   "source": [
    "#### **1. Repaso breve: Autoencoders, VAE y GANs**\n",
    "\n",
    "##### **AE**\n",
    "- Encoder: $x\\to z$\n",
    "- Decoder: $z\\to \\hat{x}$\n",
    "- Optimiza reconstrucción (MSE/BCE)\n",
    "\n",
    "**Fortalezas:** compresión, representación latente, anomalías.  \n",
    "**Limitaciones:** reconstrucciones \"promedio\", no siempre gran generador.\n",
    "\n",
    "##### **VAE**\n",
    "- Encoder produce $\\mu$, $\\log\\sigma^2$\n",
    "- Reparametrización: $z = \\mu + \\sigma \\odot \\epsilon$\n",
    "\n",
    "Pérdida:\n",
    "$$\n",
    "\\mathcal{L}=\\mathcal{L}_{rec}+\\beta D_{KL}(q(z|x)||p(z))\n",
    "$$\n",
    "\n",
    "**Fortalezas:** latente probabilístico, muestreo.  \n",
    "**Limitaciones:** blur, trade-off con KL.\n",
    "\n",
    "##### **GAN**\n",
    "- G genera, D discrimina.\n",
    "\n",
    "**Fortalezas:** nitidez visual.  \n",
    "**Limitaciones:** inestabilidad, mode collapse.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7921bbad",
   "metadata": {},
   "source": [
    "##### **Preguntas rápidas (responder en markdown)**\n",
    "1. Diferencia principal entre el latente de AE y VAE.\n",
    "2. ¿Por qué GAN puede ser inestable?\n",
    "3. ¿Qué ventaja conceptual aporta difusión?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0394ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: escribe tus respuestas aquí si prefieres usar una celda de código.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5377c4f2",
   "metadata": {},
   "source": [
    "#### **2. Dataset base: MNIST**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa11beb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "mnist = load_dataset(\"ylecun/mnist\")\n",
    "to_tensor = transforms.ToTensor()\n",
    "\n",
    "def transform_batch(examples):\n",
    "    examples[\"image\"] = [to_tensor(img) for img in examples[\"image\"]]\n",
    "    return examples\n",
    "\n",
    "mnist = mnist.with_transform(transform_batch)\n",
    "\n",
    "train_loader = DataLoader(mnist[\"train\"], batch_size=128, shuffle=True)\n",
    "test_loader  = DataLoader(mnist[\"test\"], batch_size=128, shuffle=False)\n",
    "\n",
    "batch = next(iter(train_loader))\n",
    "show_images_grid(batch[\"image\"][:16], ncols=4, title=\"MNIST - muestra\")\n",
    "print(batch[\"image\"].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d9ea50",
   "metadata": {},
   "source": [
    "#### **3. Autoencoder guiado**\n",
    "\n",
    "Completa las partes con **TODO** y entrena el modelo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f906d803",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderAE(nn.Module):\n",
    "    def __init__(self, latent_dim=32):\n",
    "        super().__init__()\n",
    "        # TODO: puedes modificar esta arquitectura\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32*7*7, latent_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class DecoderAE(nn.Module):\n",
    "    def __init__(self, latent_dim=32):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(latent_dim, 32*7*7)\n",
    "        self.deconv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(32, 16, 4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 1, 4, stride=2, padding=1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        h = self.fc(z).view(-1, 32, 7, 7)\n",
    "        return self.deconv(h)\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, latent_dim=32):\n",
    "        super().__init__()\n",
    "        self.encoder = EncoderAE(latent_dim)\n",
    "        self.decoder = DecoderAE(latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO: x -> z -> x_hat\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        return x_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e6e1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ae = AutoEncoder(latent_dim=32).to(device)\n",
    "opt_ae = torch.optim.Adam(ae.parameters(), lr=1e-3)\n",
    "\n",
    "def train_ae(model, loader, optimizer, steps=200):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    step = 0\n",
    "    while step < steps:\n",
    "        for batch in loader:\n",
    "            x = batch[\"image\"].to(device)\n",
    "            x_hat = model(x)\n",
    "            loss = F.mse_loss(x_hat, x)  # prueba BCE también\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            step += 1\n",
    "            if step >= steps:\n",
    "                break\n",
    "    return losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b57c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_losses = train_ae(ae, train_loader, opt_ae, steps=200)\n",
    "plt.plot(ae_losses)\n",
    "plt.title(\"AE - pérdida\")\n",
    "plt.xlabel(\"Paso\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.show()\n",
    "print(\"Última pérdida:\", ae_losses[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfbe8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ae.eval()\n",
    "with torch.no_grad():\n",
    "    xb = next(iter(test_loader))[\"image\"][:8].to(device)\n",
    "    xb_hat = ae(xb)\n",
    "\n",
    "show_images_grid(xb.cpu(), ncols=8, title=\"AE - originales\")\n",
    "show_images_grid(xb_hat.cpu(), ncols=8, title=\"AE - reconstrucciones\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b47ab5",
   "metadata": {},
   "source": [
    "##### **Ejercicio conceptual (AE)**\n",
    "1. Prueba `latent_dim = 8` y `latent_dim = 2`.\n",
    "2. Compara MSE vs BCE.\n",
    "3. ¿Cómo usarías AE para detección de anomalías?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6255d2cd-13fc-43fc-bf41-9fd0ccddeae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tus respuestas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7d79e1",
   "metadata": {},
   "source": [
    "#### **4. VAE guiado**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc57177",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderVAE(nn.Module):\n",
    "    def __init__(self, latent_dim=16):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1,16,3,stride=2,padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16,32,3,stride=2,padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "        self.mu = nn.Linear(32*7*7, latent_dim)\n",
    "        self.logvar = nn.Linear(32*7*7, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.features(x)\n",
    "        # TODO\n",
    "        return self.mu(h), self.logvar(h)\n",
    "\n",
    "class DecoderVAE(nn.Module):\n",
    "    def __init__(self, latent_dim=16):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(latent_dim, 32*7*7)\n",
    "        self.deconv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(32,16,4,stride=2,padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16,1,4,stride=2,padding=1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "    def forward(self, z):\n",
    "        h = self.fc(z).view(-1,32,7,7)\n",
    "        return self.deconv(h)\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, latent_dim=16):\n",
    "        super().__init__()\n",
    "        self.encoder = EncoderVAE(latent_dim)\n",
    "        self.decoder = DecoderVAE(latent_dim)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        # TODO: reparametrización\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + std * eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encoder(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        x_hat = self.decoder(z)\n",
    "        return x_hat, mu, logvar\n",
    "\n",
    "def vae_loss(x_hat, x, mu, logvar, beta=1.0):\n",
    "    # TODO: reconstrucción + KL\n",
    "    rec = F.binary_cross_entropy(x_hat, x, reduction=\"mean\")\n",
    "    kl = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return rec + beta * kl, rec, kl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54625ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VAE(latent_dim=16).to(device)\n",
    "opt_vae = torch.optim.Adam(vae.parameters(), lr=1e-3)\n",
    "\n",
    "hist = []\n",
    "steps = 220\n",
    "step = 0\n",
    "vae.train()\n",
    "while step < steps:\n",
    "    for batch in train_loader:\n",
    "        x = batch[\"image\"].to(device)\n",
    "        x_hat, mu, logvar = vae(x)\n",
    "        loss, rec, kl = vae_loss(x_hat, x, mu, logvar, beta=1.0)\n",
    "\n",
    "        opt_vae.zero_grad()\n",
    "        loss.backward()\n",
    "        opt_vae.step()\n",
    "\n",
    "        hist.append((loss.item(), rec.item(), kl.item()))\n",
    "        step += 1\n",
    "        if step >= steps:\n",
    "            break\n",
    "\n",
    "hist = np.array(hist)\n",
    "plt.plot(hist[:,0], label=\"total\")\n",
    "plt.plot(hist[:,1], label=\"rec\")\n",
    "plt.plot(hist[:,2], label=\"kl\")\n",
    "plt.legend(); plt.title(\"VAE - pérdidas\"); plt.xlabel(\"Paso\"); plt.show()\n",
    "print(\"Última:\", hist[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1072ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.eval()\n",
    "with torch.no_grad():\n",
    "    xb = next(iter(test_loader))[\"image\"][:8].to(device)\n",
    "    xb_hat, mu, logvar = vae(xb)\n",
    "\n",
    "show_images_grid(xb.cpu(), ncols=8, title=\"VAE - originales\")\n",
    "show_images_grid(xb_hat.cpu(), ncols=8, title=\"VAE - reconstrucciones\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = torch.randn(16, 16, device=device)\n",
    "    samp = vae.decoder(z)\n",
    "show_images_grid(samp.cpu(), ncols=4, title=\"VAE - muestras\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a16d922",
   "metadata": {},
   "source": [
    "##### **Actividad guiada (VAE)**\n",
    "\n",
    "Cambia `beta` a: `[0.1, 1.0, 4.0]`\n",
    "\n",
    "Describe el efecto en:\n",
    "\n",
    "- reconstrucción,\n",
    "- regularización,\n",
    "- muestras generadas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1e2dd7-caf0-4ddb-b6f5-cc5a51a4c6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tus respuestas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274e20ce",
   "metadata": {},
   "source": [
    "#### **5. GAN (repaso + 1 paso de entrenamiento)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e9ac39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneratorGAN(nn.Module):\n",
    "    def __init__(self, z_dim=64):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(z_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 28*28),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    def forward(self, z):\n",
    "        return self.fc(z).view(-1,1,28,28)\n",
    "\n",
    "class DiscriminatorGAN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(28*28,256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256,128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(128,1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dca3fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = GeneratorGAN().to(device)\n",
    "D = DiscriminatorGAN().to(device)\n",
    "\n",
    "bce = nn.BCEWithLogitsLoss()\n",
    "opt_g = torch.optim.Adam(G.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
    "opt_d = torch.optim.Adam(D.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
    "\n",
    "batch = next(iter(train_loader))\n",
    "x_real = batch[\"image\"][:64].to(device)\n",
    "x_real_scaled = x_real * 2 - 1\n",
    "\n",
    "z = torch.randn(x_real.size(0), 64, device=device)\n",
    "x_fake = G(z)\n",
    "\n",
    "# D\n",
    "d_real = D(x_real_scaled)\n",
    "d_fake = D(x_fake.detach())  # detach para no actualizar G aquí\n",
    "loss_d = bce(d_real, torch.ones_like(d_real)) + bce(d_fake, torch.zeros_like(d_fake))\n",
    "opt_d.zero_grad(); loss_d.backward(); opt_d.step()\n",
    "\n",
    "# G\n",
    "d_fake2 = D(x_fake)\n",
    "loss_g = bce(d_fake2, torch.ones_like(d_fake2))\n",
    "opt_g.zero_grad(); loss_g.backward(); opt_g.step()\n",
    "\n",
    "print(\"loss_D:\", float(loss_d), \"loss_G:\", float(loss_g))\n",
    "show_images_grid(((x_fake[:16]+1)/2).cpu(), ncols=4, title=\"GAN - muestras (sin entrenamiento real)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469986d2",
   "metadata": {},
   "source": [
    "##### **Preguntas (GAN)**\n",
    "1. ¿Por qué escalamos las imágenes reales a `[-1, 1]`?\n",
    "2. ¿Para qué sirve `detach()`?\n",
    "3. Explica *mode collapse*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea809d04-5d63-40ea-9a62-5d0271001cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tus respuestas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49b57e4",
   "metadata": {},
   "source": [
    "#### **6. Difusión: idea central (añadir ruido y aprender un denoiser)**\n",
    "\n",
    "**Forward:**\n",
    "$$\n",
    "x_t = \\sqrt{\\bar{\\alpha}_t}x_0 + \\sqrt{1 - \\bar{\\alpha}_t}\\epsilon\n",
    "$$\n",
    "\n",
    "**Reverse:**\n",
    "\n",
    "- una red predice el ruido (u otro objetivo equivalente),\n",
    "- se aplica un scheduler para retroceder desde ruido puro hasta una muestra.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da3a282",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_beta_schedule(T=200, beta_start=1e-4, beta_end=0.02):\n",
    "    return torch.linspace(beta_start, beta_end, T)\n",
    "\n",
    "T = 200\n",
    "betas = linear_beta_schedule(T)\n",
    "alphas = 1.0 - betas\n",
    "alpha_bars = torch.cumprod(alphas, dim=0)\n",
    "\n",
    "plt.plot(betas.numpy(), label=\"beta_t\")\n",
    "plt.plot(alpha_bars.numpy(), label=\"alpha_bar_t\")\n",
    "plt.title(\"Schedule lineal\")\n",
    "plt.xlabel(\"t\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1684828",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_sample(x0, t, noise=None):\n",
    "    if noise is None:\n",
    "        noise = torch.randn_like(x0)\n",
    "\n",
    "    if isinstance(t, int):\n",
    "        t = torch.tensor([t], device=x0.device).repeat(x0.shape[0])\n",
    "\n",
    "    a_bar_t = alpha_bars.to(x0.device)[t].view(-1,1,1,1)\n",
    "    x_t = torch.sqrt(a_bar_t)*x0 + torch.sqrt(1-a_bar_t)*noise\n",
    "    return x_t, noise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca83b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "xb = next(iter(test_loader))[\"image\"][:4].to(device)\n",
    "times = [0, 20, 60, 120, 199]\n",
    "viz = []\n",
    "for t in times:\n",
    "    x_t, _ = q_sample(xb, t)\n",
    "    viz.extend([im for im in x_t.cpu()])\n",
    "show_images_grid(viz, ncols=len(times), title=\"Forward diffusion (más t = más ruido)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ca62d8",
   "metadata": {},
   "source": [
    "##### **Actividad de observación**\n",
    "- ¿En qué `t` deja de ser reconocible el dígito?\n",
    "- Relaciona eso con `alpha_bar_t`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b99474-133a-4b11-836c-360df0ba3341",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tus respuestas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6e61b1",
   "metadata": {},
   "source": [
    "#### **7. DDPM y DDIM (noción general)**\n",
    "\n",
    "##### **7.1 DDPM (entrenamiento)**\n",
    "\n",
    "El DDPM define primero un **proceso forward** (de degradación) que **sí conocemos**: tomamos una imagen real $x_0$ y la corrompemos paso a paso con ruido Gaussiano hasta llegar a algo casi indistinguible de $\\mathcal{N}(0,I)$.\n",
    "\n",
    "La forma más útil del forward no es simular $(x_1, x_2,\\cdots)$, sino usar la **forma cerrada**:\n",
    "\n",
    "$$\n",
    "x_t = \\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1-\\bar{\\alpha}_t}\\epsilon,\n",
    "\\quad \\epsilon\\sim\\mathcal{N}(0,I)\n",
    "$$\n",
    "\n",
    "donde:\n",
    "\n",
    "* $\\beta_t$ es el \"nivel de ruido\" del paso $t$,\n",
    "* $\\alpha_t = 1-\\beta_t$,\n",
    "* $\\bar{\\alpha}_t = \\prod_{s=1}^{t}\\alpha_s$.\n",
    "\n",
    "Con eso, tus 4 pasos quedan así (con el \"por qué\"):\n",
    "\n",
    "1. **Escoge $t$**\n",
    "   Se toma $t$ aleatorio (uniforme en $\\{1,\\dots,T\\})$ para que el modelo aprenda a denoiser **en todos los niveles de ruido**, no solo en uno.\n",
    "\n",
    "2. **Genera $x_t$**\n",
    "   Se construye $x_t$ en una sola operación usando la ecuación cerrada. Esto hace el entrenamiento eficiente (no necesitas simular toda la cadena).\n",
    "\n",
    "3. **Predice ruido**\n",
    "   Entrenas una red $\\epsilon_\\theta(x_t, t)$ para aproximar el ruido real $\\epsilon$ que metiste.\n",
    "   Intuición: si sabes \"qué ruido hay\", puedes **restarlo** y recuperar algo más cercano a $x_0$.\n",
    "\n",
    "4. **Minimiza MSE**\n",
    "   $$\n",
    "   \\mathcal{L}(\\theta)=\\mathbb{E}_{x_0,t,\\epsilon}\\left[||\\epsilon-\\epsilon_\\theta(x_t,t)||^2\\right]\n",
    "   $$\n",
    "\n",
    "¿Por qué MSE es tan natural? Porque el forward usa ruido Gaussiano y el proceso inverso se modela como gaussiano condicionado; con esa elección, el MSE se alinea con maximizar una cota variacional (ELBO) en la formulación original de DDPM.\n",
    "\n",
    "**Detalle práctico importante:** el modelo necesita saber $t$. Por eso se usa un **embedding temporal** (posicional/sinusoidal o MLP), para que la red sepa si está denoising \"poco ruido\" o \"mucho ruido\".\n",
    "\n",
    "##### **7.2 DDPM (muestreo)**\n",
    "\n",
    "Para generar, ya no tienes $x_0$. Arrancas con:\n",
    "\n",
    "$$\n",
    "x_T \\sim \\mathcal{N}(0,I)\n",
    "$$\n",
    "\n",
    "y aplicas un **proceso reverse** aprendido:\n",
    "\n",
    "$$\n",
    "x_{t-1} \\sim p_\\theta(x_{t-1}\\mid x_t)\n",
    "$$\n",
    "\n",
    "En DDPM, este reverse suele escribirse como una gaussiana con media $\\mu_\\theta(x_t,t)$ y varianza (a veces fija) $\\sigma_t^2$. La clave es que $\\mu_\\theta$ se calcula a partir de $\\epsilon_\\theta(x_t,t)$. Una forma común (conceptual) es:\n",
    "\n",
    "* Primero estimas $x_0$ desde $x_t$ y el ruido predicho:\n",
    "\n",
    "$$\n",
    "\\hat{x}_0 = \\frac{1}{\\sqrt{\\bar{\\alpha}_t}}\\left(x_t - \\sqrt{1-\\bar{\\alpha}_t}\\epsilon_\\theta(x_t,t)\\right)\n",
    "$$\n",
    "\n",
    "* Luego construyes $x_{t-1}$ como \"mezcla\" entre $x_t$ y $\\hat{x}_0$ más un poco de ruido:\n",
    "\n",
    "$$\n",
    "x_{t-1} = \\mu_\\theta(x_t,t) + \\sigma_t z,\\quad z\\sim\\mathcal{N}(0,I)\n",
    "$$\n",
    "\n",
    "**Intuición:**\n",
    "Cada paso quita una fracción de ruido *y vuelve a inyectar un poquito* (el término $\\sigma_t z)$ para mantener consistencia con la cadena probabilística. Eso hace el muestreo **estocástico**.\n",
    "\n",
    "##### **7.3 DDIM**\n",
    "\n",
    "DDIM (Denoising Diffusion Implicit Models) parte del mismo entrenamiento (misma $\\epsilon_\\theta)$, pero cambia el muestreo: en vez de seguir estrictamente la cadena estocástica de DDPM, define una familia de trayectorias donde puedes hacer el proceso **casi determinista**.\n",
    "\n",
    "La idea central:\n",
    "\n",
    "* DDPM: reverse **estocástico** (ruido en cada paso).\n",
    "* DDIM: reverse **determinista o con menos ruido**, usando una reparametrización que permite saltar pasos.\n",
    "\n",
    "Una forma de verlo:\n",
    "\n",
    "* Usas $\\hat{x}_0$ como antes.\n",
    "\n",
    "* Calculas directamente $x_{t-1}$ (o $x_{\\tau_{k-1}})$ combinando $\\hat{x}_0$ y $\\epsilon*\\theta$, con un parámetro $\\eta$ que controla cuán estocástico es:\n",
    "\n",
    "* $\\eta=0$: muestreo casi determinista (rápido, reproducible).\n",
    "\n",
    "* $\\eta>0$: reintroduces ruido (más diversidad, a veces mejor cobertura).\n",
    "\n",
    "DDIM permite usar un subconjunto de tiempos $\\tau_1 > \\tau_2 > \\dots > \\tau_K$ con $K \\ll T$. Por ejemplo, en vez de 1000 pasos, haces 50 o 20. Sigues usando la misma red, pero recorres una trayectoria \"más directa\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7518a8-ed74-4e88-b03e-1c937518e3cf",
   "metadata": {},
   "source": [
    "#### **Preguntas**\n",
    "\n",
    "1. **Pregunta conceptual:**\n",
    "   ¿Por qué muestrear $t$ al azar es mejor que entrenar siempre con un mismo $t$?.\n",
    "\n",
    "2. **Experimento de velocidad/calidad:**\n",
    "   Usa el mismo denoiser y compara muestreo con:\n",
    "\n",
    "* todos los pasos $T$,\n",
    "* solo 50 pasos (submuestreo de tiempos),\n",
    "  y describe qué se pierde primero: nitidez, coherencia global o diversidad.\n",
    "\n",
    "3. **Diversidad vs determinismo:**\n",
    "   Explica por qué $\\eta=04$ (DDIM determinista) reduce diversidad aunque mantenga calidad.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8bdf0e-d4da-40d8-b2b0-4b94706d5a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tus respuestas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6df7fbb",
   "metadata": {},
   "source": [
    "#### **8. Denoiser mínimo para difusión**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1fe2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeEmbedding(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(1, dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim, dim),\n",
    "        )\n",
    "    def forward(self, t):\n",
    "        return self.net((t.float()/T).unsqueeze(-1))\n",
    "\n",
    "class TinyDenoiser(nn.Module):\n",
    "    def __init__(self, time_dim=32):\n",
    "        super().__init__()\n",
    "        self.time_mlp = TimeEmbedding(time_dim)\n",
    "        self.conv1 = nn.Conv2d(1,32,3,padding=1)\n",
    "        self.conv2 = nn.Conv2d(32,32,3,padding=1)\n",
    "        self.conv3 = nn.Conv2d(32,1,3,padding=1)\n",
    "        self.to_scale = nn.Linear(time_dim, 32)\n",
    "        self.to_shift = nn.Linear(time_dim, 32)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        temb = self.time_mlp(t)\n",
    "        scale = self.to_scale(temb).unsqueeze(-1).unsqueeze(-1)\n",
    "        shift = self.to_shift(temb).unsqueeze(-1).unsqueeze(-1)\n",
    "\n",
    "        h = self.conv1(x)\n",
    "        h = F.relu(h * (1 + scale) + shift)\n",
    "        h = F.relu(self.conv2(h))\n",
    "        return self.conv3(h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a0baf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "denoiser = TinyDenoiser().to(device)\n",
    "opt_diff = torch.optim.Adam(denoiser.parameters(), lr=1e-3)\n",
    "\n",
    "def train_denoiser(model, loader, optimizer, steps=250):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    step = 0\n",
    "    while step < steps:\n",
    "        for batch in loader:\n",
    "            x0 = batch[\"image\"].to(device)\n",
    "            bsz = x0.size(0)\n",
    "\n",
    "            # TODO\n",
    "            t = torch.randint(0, T, (bsz,), device=device)\n",
    "            x_t, noise = q_sample(x0, t)\n",
    "            pred_noise = model(x_t, t)\n",
    "            loss = F.mse_loss(pred_noise, noise)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            step += 1\n",
    "            if step >= steps:\n",
    "                break\n",
    "    return losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee50e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_losses = train_denoiser(denoiser, train_loader, opt_diff, steps=260)\n",
    "plt.plot(diff_losses)\n",
    "plt.title(\"Denoiser - pérdida\")\n",
    "plt.xlabel(\"Paso\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.show()\n",
    "print(\"Última pérdida:\", diff_losses[-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d2fd47",
   "metadata": {},
   "source": [
    "#### **Preguntas de interpretación**\n",
    "\n",
    "1. ¿Por qué predecir ruido?.\n",
    "2. ¿Por qué muestrear `t` aleatorio?.\n",
    "3. ¿Qué pasa si `T` aumenta mucho?.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584174bb-8a10-4e53-998a-3bb89c977a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tus respuestas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c532f3",
   "metadata": {},
   "source": [
    "#### **9. Muestreo didáctico (DDPM)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256f46d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample_toy_ddpm(model, n_samples=16):\n",
    "    model.eval()\n",
    "    x = torch.randn(n_samples, 1, 28, 28, device=device)\n",
    "\n",
    "    for t_inv in range(T-1, -1, -1):\n",
    "        t = torch.full((n_samples,), t_inv, device=device, dtype=torch.long)\n",
    "        pred_noise = model(x, t)\n",
    "\n",
    "        a_t = alphas.to(device)[t].view(-1,1,1,1)\n",
    "        a_bar_t = alpha_bars.to(device)[t].view(-1,1,1,1)\n",
    "        beta_t = betas.to(device)[t].view(-1,1,1,1)\n",
    "\n",
    "        x = (1/torch.sqrt(a_t)) * (x - ((1-a_t)/torch.sqrt(1-a_bar_t)) * pred_noise)\n",
    "\n",
    "        if t_inv > 0:\n",
    "            x = x + torch.sqrt(beta_t) * torch.randn_like(x)\n",
    "\n",
    "    return x.clamp(0,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461d6114",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = sample_toy_ddpm(denoiser, n_samples=16)\n",
    "show_images_grid(samples.cpu(), ncols=4, title=\"Muestras (toy DDPM)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74eacbfb",
   "metadata": {},
   "source": [
    "#### **Ejercicio guiado (muestreo)**\n",
    "\n",
    "- Entrena más pasos y compara calidad.\n",
    "- Cambia `T` a 100.\n",
    "- Quita la parte estocástica y describe el efecto.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb5f8ee-50e2-4e88-82b4-108397ddb6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tus respuestas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2722016c",
   "metadata": {},
   "source": [
    "#### **10. Difusión más allá de imágenes**\n",
    "\n",
    "##### **Texto**\n",
    "La difusión en texto es más compleja porque los tokens son discretos. Se usan variantes sobre embeddings continuos o híbridas.\n",
    "\n",
    "##### **Audio**\n",
    "Aplicaciones: síntesis, denoising, restauración, TTS.\n",
    "\n",
    "##### **Datos científicos**\n",
    "Aplicaciones: imágenes médicas, simulación, estructuras, series.\n",
    "\n",
    "**Idea reusable:** representación + ruido + denoiser + métrica.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16c6d9d",
   "metadata": {},
   "source": [
    "##### **Actividad de discusión**\n",
    "Escoge  un dominio (texto, audio o datos científicos) y responde:\n",
    "\n",
    "1. ¿Qué sería `x0`?\n",
    "2. ¿Qué ruido usarían?\n",
    "3. ¿Qué métrica(s) usarían?\n",
    "4. ¿Qué riesgos técnicos/éticos ven?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41c9948-c498-4e84-9616-91bc6e73b47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tus respuestas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7ddcfb",
   "metadata": {},
   "source": [
    "#### **11. DiT (Diffusion + Transformers)**\n",
    "\n",
    "Un **DiT** usa un **Transformer tipo ViT** como *denoiser* en difusión: en vez de un UNet, procesa la imagen ruidosa $x_t$ como **secuencia de patches** (tokens), añade **embeddings de tiempo $t$** y **condición** (clase/texto), y predice $\\hat\\epsilon$ (o $x_0$)/(v)). El **scheduler** (DDPM/DDIM) sigue igual: solo cambia la red que predice.\n",
    "\n",
    "**Cómo entra $t$ y la condición**\n",
    "\n",
    "* $t$ y condición -> embeddings.\n",
    "* Se inyectan en los bloques Transformer (típicamente vía **modulación de LayerNorm/AdaLN-Film** y/o **cross-attention** para texto).\n",
    "\n",
    "\n",
    "**Limitaciones clave**\n",
    "\n",
    "* Atención completa cuesta **(O(N^2))** con (N) patches (sube con resolución).\n",
    "* UNet trae sesgos multi-escala \"gratis\"; DiT suele necesitar **más escala** para brillar.\n",
    "\n",
    "**En una frase:** DiT = difusión donde el denoiser es un Transformer sobre patches, con tiempo/condición embebidos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cd5403",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbed(nn.Module):\n",
    "    def __init__(self, in_ch=1, patch=4, emb_dim=128, img_size=28):\n",
    "        super().__init__()\n",
    "        self.patch = patch\n",
    "        self.grid = img_size // patch\n",
    "        self.n_patches = self.grid * self.grid\n",
    "        self.proj = nn.Conv2d(in_ch, emb_dim, kernel_size=patch, stride=patch)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.proj(x)\n",
    "        return h.flatten(2).transpose(1,2)  # [B,N,D]\n",
    "\n",
    "class TinyDiT(nn.Module):\n",
    "    def __init__(self, img_size=28, patch=4, emb_dim=128, depth=3, nhead=4):\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbed(1, patch, emb_dim, img_size)\n",
    "        n_patches = self.patch_embed.n_patches\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, n_patches, emb_dim) * 0.02)\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            nn.Linear(1, emb_dim), nn.ReLU(), nn.Linear(emb_dim, emb_dim)\n",
    "        )\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=emb_dim, nhead=nhead, dim_feedforward=emb_dim*4, batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(enc_layer, num_layers=depth)\n",
    "        self.patch = patch\n",
    "        self.to_patch_pixels = nn.Linear(emb_dim, patch*patch)\n",
    "\n",
    "    def forward(self, x_t, t):\n",
    "        B, C, H, W = x_t.shape\n",
    "        h = self.patch_embed(x_t) + self.pos_embed\n",
    "        t_emb = self.time_mlp((t.float()/T).unsqueeze(-1))\n",
    "        h = h + t_emb.unsqueeze(1)\n",
    "        h = self.transformer(h)\n",
    "\n",
    "        p = self.patch\n",
    "        G = self.patch_embed.grid\n",
    "        patch_vals = self.to_patch_pixels(h).view(B, G, G, p, p)\n",
    "        x = patch_vals.permute(0,1,3,2,4).contiguous().view(B,1,G*p,G*p)\n",
    "        x = F.interpolate(x, size=(H,W), mode=\"bilinear\", align_corners=False)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1018a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_dit = TinyDiT().to(device)\n",
    "xb = next(iter(train_loader))[\"image\"][:8].to(device)\n",
    "tb = torch.randint(0, T, (xb.size(0),), device=device)\n",
    "out = tiny_dit(xb, tb)\n",
    "print(\"Entrada:\", xb.shape, \"Salida:\", out.shape)\n",
    "print(\"Parámetros TinyDiT:\", sum(p.numel() for p in tiny_dit.parameters()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2734c763",
   "metadata": {},
   "source": [
    "#### **Ejercicio (DiT)**\n",
    "1. Cambia `patch=2`, `patch=7` y analiza implicancias.\n",
    "2. ¿Cómo agregarías condición por clase?\n",
    "3. ¿Qué papel tendría el encoder de texto en un DiT condicionado por texto?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f83503-b8e7-4e91-bde0-0857d1494c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tus respuestas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3cca30",
   "metadata": {},
   "source": [
    "#### **12. Generación condicionada por texto (visión general)**\n",
    "\n",
    "Un pipeline **texto -> imagen** suele funcionar así:\n",
    "\n",
    "1. **Encoder de texto (CLIP/T5)**\n",
    "   Convierte el prompt en embeddings (c) que capturan semántica (objetos, estilo, relaciones).\n",
    "\n",
    "* CLIP: fuerte alineamiento texto-imagen (muy usado en difusión).\n",
    "* T5: común en algunos modelos por su capacidad lingüística.\n",
    "\n",
    "2. **Modelo de difusión (UNet o DiT)**\n",
    "   Actúa como **denoiser condicionado**: recibe $z_t$ (imagen/latente ruidoso), el tiempo $t$ y la condición $c$.\n",
    "   La condición entra típicamente vía:\n",
    "\n",
    "* **cross-attention** (tokens visuales atienden a tokens de texto), y/o\n",
    "* modulación (AdaLN/FiLM).\n",
    "\n",
    "3. **Scheduler (DDPM/DDIM/Euler/…)**\n",
    "   Define *cómo* actualizas $z_t \\rightarrow z_{t-1}$: número de pasos, ruido, y fórmula de actualización.\n",
    "\n",
    "* Cambiar scheduler afecta velocidad/calidad sin reentrenar el denoiser.\n",
    "\n",
    "4. **VAE / Latent Diffusion (muy común)**\n",
    "   En vez de denoising en píxeles, se trabaja en un **latente comprimido**:\n",
    "\n",
    "* **VAE encoder:** imagen -> latente $z$ (menor resolución/dimensiones).\n",
    "* **Difusión:** denoise en $z_t$ (más barato).\n",
    "* **VAE decoder:** $z_0$ -> píxeles finales.\n",
    "\n",
    "**Por qué latentes es más eficiente**\n",
    "Denoising en latente reduce drásticamente el costo (menos \"pixeles\" efectivos), permitiendo alta resolución y modelos más grandes.\n",
    "\n",
    "**Pieza clave extra: Classifier-Free Guidance (CFG)**\n",
    "En muestreo se combinan dos predicciones:\n",
    "\n",
    "* una **condicionada** (con texto) y otra **no condicionada** (texto \"vacío\"), para aumentar \"obediencia al prompt\" a cambio de algo de diversidad.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae4121a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EJEMPLO OPCIONAL (no ejecutar si no tienes GPU/memoria suficiente)\n",
    "# from diffusers import AutoPipelineForText2Image\n",
    "# model_id = \"stabilityai/sdxl-turbo\"\n",
    "# pipe = AutoPipelineForText2Image.from_pretrained(\n",
    "#     model_id,\n",
    "#     torch_dtype=torch.float16 if device.type == \"cuda\" else torch.float32\n",
    "# ).to(device)\n",
    "# prompt = \"Un laboratorio de IA futurista, estilo infografía técnica\"\n",
    "# img = pipe(prompt=prompt, num_inference_steps=4, guidance_scale=0.0).images[0]\n",
    "# img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b354de",
   "metadata": {},
   "source": [
    "#### **Actividad escrita**\n",
    "1. ¿Qué hace el encoder de texto?\n",
    "2. ¿Qué significa `guidance` (intuición)?\n",
    "3. ¿Por qué usar un VAE dentro de un pipeline de difusión grande?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352f6e0b-b9fa-4d84-9cf2-787a147816f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tus respuestas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e33a7d",
   "metadata": {},
   "source": [
    "#### **13. Comparación integrada**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56a5f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: completar la comparación\n",
    "comparacion = {\n",
    "    \"Modelo\": [\"AE\", \"VAE\", \"GAN\", \"Difusión\"],\n",
    "    \"Estabilidad\": [\"\", \"\", \"\", \"\"],\n",
    "    \"Calidad visual\": [\"\", \"\", \"\", \"\"],\n",
    "    \"Velocidad de muestreo\": [\"\", \"\", \"\", \"\"],\n",
    "    \"Latente interpretable\": [\"\", \"\", \"\", \"\"],\n",
    "    \"Comentario\": [\"\", \"\", \"\", \"\"],\n",
    "}\n",
    "comparacion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cb92c9",
   "metadata": {},
   "source": [
    "#### **14. Mini-proyecto (elige uno)**\n",
    "\n",
    "**Opción A**: Agregar condición de clase (0-9) al denoiser de difusión.\n",
    "\n",
    "**Opción B**:  Comparar AE vs VAE con el mismo `latent_dim`.\n",
    "\n",
    "**Opción C**: Modificar `TinyDiT` para incluir condición de clase.\n",
    "\n",
    "**Opción D**: Cambiar MNIST por Fashion-MNIST y comparar resultados.\n",
    "\n",
    "##### **Entregable sugerido**\n",
    "- capturas/figuras,\n",
    "- tabla comparativa,\n",
    "- respuestas conceptuales,\n",
    "- código comentado.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5318c568",
   "metadata": {},
   "source": [
    "#### **15. Ejercicios de codificación**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d12843f",
   "metadata": {},
   "source": [
    "1. Implementa un scheduler coseno y comparar con lineal.  \n",
    "2. Construye un `TinyUNet` con skip connection.  \n",
    "3. Cambia el objetivo del denoiser para predecir `x0`.  \n",
    "4. Agrega embedding de clase al denoiser.  \n",
    "5. Implementa el muestreo rápido (menos pasos) y comparar calidad.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e16c17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plantilla - Ejercicio 1\n",
    "def cosine_beta_schedule(T=200, s=0.008):\n",
    "    \"\"\"TODO: Implementar schedule coseno y devolver betas.\"\"\"\n",
    "    raise NotImplementedError(\"Completar scheduler coseno\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72c3044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plantilla - Ejercicio 4\n",
    "class ClassConditionedDenoiser(nn.Module):\n",
    "    def __init__(self, num_classes=10, time_dim=32, class_dim=16):\n",
    "        super().__init__()\n",
    "        # TODO: embedding de clase + bloque de tiempo + convs\n",
    "        raise NotImplementedError(\"Completar arquitectura\")\n",
    "\n",
    "    def forward(self, x_t, t, y):\n",
    "        # TODO: usar etiqueta y (0-9) como condición\n",
    "        raise NotImplementedError(\"Completar forward\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
