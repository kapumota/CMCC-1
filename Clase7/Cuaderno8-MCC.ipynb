{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72bfeb0b",
   "metadata": {},
   "source": [
    "### **Aprendizaje por refuerzo y toma de decisiones**\n",
    "\n",
    "Este cuaderno integra y reorganiza código para cubrir los siguientes temas:\n",
    "\n",
    "- **Marco MDP:** estados, acciones, recompensas, políticas y valores.  \n",
    "- **Métodos básicos:** *Q-learning* y *Policy Gradient* (visión general).  \n",
    "- **RL en** juegos, robótica (control), y sistemas de recomendación (bandits).  \n",
    "- **Conexión RL-LLM:** RLHF como RL sobre recompensas humanas (pipeline simplificado).  \n",
    "- **Diseño de recompensas** y desafíos de **estabilidad/seguridad**.\n",
    "\n",
    "> Algunas secciones están marcadas como opcionales para mantener el tiempo de ejecución razonable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2e73c6",
   "metadata": {},
   "source": [
    "#### **0. Preparación del entorno**\n",
    "\n",
    "Este cuaderno usa **PyTorch** y, si está disponible, un entorno tipo **Gym/Gymnasium** para ejemplos de control.  \n",
    "Si tu entorno no tiene `gymnasium`/`gym`, puedes ejecutar sin problemas las partes de *GridWorld* y *Bandits*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2edb09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Compatibilidad Gym vs Gymnasium (si existe)\n",
    "try:\n",
    "    import gymnasium as gym\n",
    "    _GYMNASIUM = True\n",
    "except Exception:\n",
    "    try:\n",
    "        import gym\n",
    "        _GYMNASIUM = False\n",
    "    except Exception:\n",
    "        gym = None\n",
    "        _GYMNASIUM = False\n",
    "\n",
    "def reset_env(env, seed=None):\n",
    "    \"\"\"Devuelve obs de forma compatible con gymnasium/gym.\"\"\"\n",
    "    if seed is not None:\n",
    "        try:\n",
    "            out = env.reset(seed=seed)\n",
    "        except TypeError:\n",
    "            out = env.reset()\n",
    "    else:\n",
    "        out = env.reset()\n",
    "    if isinstance(out, tuple):\n",
    "        return out[0]\n",
    "    return out\n",
    "\n",
    "def step_env(env, action):\n",
    "    \"\"\"Devuelve (obs, reward, done) compatible con gymnasium/gym.\"\"\"\n",
    "    out = env.step(action)\n",
    "    if len(out) == 5:  # gymnasium: obs, reward, terminated, truncated, info\n",
    "        obs, reward, terminated, truncated, _ = out\n",
    "        done = terminated or truncated\n",
    "        return obs, reward, done\n",
    "    else:  # gym: obs, reward, done, info\n",
    "        obs, reward, done, _ = out\n",
    "        return obs, reward, done\n",
    "\n",
    "# Reproducibilidad mínima\n",
    "SEED = 7\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13deaf3b",
   "metadata": {},
   "source": [
    "#### **1. Marco MDP: definición operacional (con un GridWorld mínimo)**\n",
    "\n",
    "Un **MDP** se define por $(\\mathcal{S}, \\mathcal{A}, P, R, \\gamma)$:\n",
    "\n",
    "- **Estado** $s \\in \\mathcal{S}$: lo que el agente observa del mundo.\n",
    "- **Acción** $a \\in \\mathcal{A}$: decisión del agente.\n",
    "- **Transición** $P(s' \\mid s, a)$: dinámica del entorno.\n",
    "- **Recompensa** $R(s,a,s')$: señal de utilidad.\n",
    "- **Descuento** $\\gamma \\in [0,1)$: preferencia por recompensas futuras.\n",
    "\n",
    "Implementamos un GridWorld pequeño para tener control total (sin dependencias externas).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb10042",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class GridWorld:\n",
    "    '''\n",
    "    GridWorld 4x4:\n",
    "      - Inicio: (0,0)\n",
    "      - Meta:   (3,3) recompensa +1 y termina\n",
    "      - Pozo:   (1,1) recompensa -1 y termina (opcional)\n",
    "      - Penalización por paso: -0.01 (para evitar loops)\n",
    "    '''\n",
    "    n: int = 4\n",
    "    goal: tuple = (3, 3)\n",
    "    pit: tuple = (1, 1)\n",
    "    step_penalty: float = -0.01\n",
    "    gamma: float = 0.99\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.state = (0, 0)\n",
    "\n",
    "    @property\n",
    "    def state_dim(self):\n",
    "        return self.n * self.n\n",
    "\n",
    "    @property\n",
    "    def action_dim(self):\n",
    "        return 4  # 0=up,1=right,2=down,3=left\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = (0, 0)\n",
    "        return self._encode(self.state)\n",
    "\n",
    "    def _encode(self, s):\n",
    "        # one-hot\n",
    "        x = np.zeros(self.state_dim, dtype=np.float32)\n",
    "        idx = s[0] * self.n + s[1]\n",
    "        x[idx] = 1.0\n",
    "        return x\n",
    "\n",
    "    def step(self, a):\n",
    "        r, c = self.state\n",
    "        if a == 0:   r = max(0, r - 1)\n",
    "        elif a == 1: c = min(self.n - 1, c + 1)\n",
    "        elif a == 2: r = min(self.n - 1, r + 1)\n",
    "        elif a == 3: c = max(0, c - 1)\n",
    "\n",
    "        next_state = (r, c)\n",
    "\n",
    "        reward = self.step_penalty\n",
    "        done = False\n",
    "        if next_state == self.goal:\n",
    "            reward = 1.0\n",
    "            done = True\n",
    "        elif next_state == self.pit:\n",
    "            reward = -1.0\n",
    "            done = True\n",
    "\n",
    "        self.state = next_state\n",
    "        return self._encode(next_state), reward, done\n",
    "\n",
    "env_gw = GridWorld()\n",
    "obs = env_gw.reset()\n",
    "obs.shape, env_gw.action_dim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8372b76d",
   "metadata": {},
   "source": [
    "##### **1.1 Política y valor: evaluación Monte Carlo (MC)**\n",
    "\n",
    "Una **política** $\\pi(a\\mid s)$ define cómo se eligen acciones.  \n",
    "El **retorno descontado** es $G = \\sum_{t=0}^{T-1} \\gamma^t r_t$.  \n",
    "Aproximamos $V^\\pi(s)$ como promedio de retornos desde $s$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77252b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode_gw(env: GridWorld, policy_fn, max_steps=100):\n",
    "    s = env.reset()\n",
    "    total = 0.0\n",
    "    g = 1.0\n",
    "    for _ in range(max_steps):\n",
    "        a = policy_fn(s)\n",
    "        s, r, done = env.step(a)\n",
    "        total += g * r\n",
    "        g *= env.gamma\n",
    "        if done:\n",
    "            break\n",
    "    return total\n",
    "\n",
    "def random_policy(_s):\n",
    "    return random.randrange(env_gw.action_dim)\n",
    "\n",
    "vals = [run_episode_gw(env_gw, random_policy) for _ in range(2000)]\n",
    "float(np.mean(vals)), float(np.std(vals))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ebd2f6",
   "metadata": {},
   "source": [
    "#### **2. Método básico 1: Q-learning (tabular)**\n",
    "\n",
    "Aprendemos una función $Q(s,a)$ y la actualizamos con:\n",
    "\n",
    "$$\n",
    "Q(s,a)\\leftarrow Q(s,a) + \\alpha\\Big(r + \\gamma \\max_{a'}Q(s',a') - Q(s,a)\\Big)\n",
    "$$\n",
    "\n",
    "Usaremos **epsilon-greedy** para explorar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efb8f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot_to_index(onehot):\n",
    "    return int(np.argmax(onehot))\n",
    "\n",
    "def q_learning_gridworld(env: GridWorld, episodes=2000, alpha=0.1, gamma=0.99, eps_start=1.0, eps_end=0.05):\n",
    "    Q = np.zeros((env.state_dim, env.action_dim), dtype=np.float32)\n",
    "    rewards = []\n",
    "    for ep in range(episodes):\n",
    "        s = env.reset()\n",
    "        s_idx = onehot_to_index(s)\n",
    "        eps = eps_end + (eps_start - eps_end) * math.exp(-ep / (episodes/5))\n",
    "        total = 0.0\n",
    "\n",
    "        for _ in range(200):\n",
    "            if random.random() < eps:\n",
    "                a = random.randrange(env.action_dim)\n",
    "            else:\n",
    "                a = int(np.argmax(Q[s_idx]))\n",
    "\n",
    "            s2, r, done = env.step(a)\n",
    "            s2_idx = onehot_to_index(s2)\n",
    "\n",
    "            td_target = r + gamma * float(np.max(Q[s2_idx])) * (0.0 if done else 1.0)\n",
    "            Q[s_idx, a] += alpha * (td_target - float(Q[s_idx, a]))\n",
    "\n",
    "            total += r\n",
    "            s_idx = s2_idx\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        rewards.append(total)\n",
    "    return Q, rewards\n",
    "\n",
    "Q_tab, ep_rewards = q_learning_gridworld(env_gw, episodes=3000)\n",
    "np.mean(ep_rewards[-200:]), np.mean(ep_rewards[:200])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff678e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_policy_from_Q(Q):\n",
    "    def policy(s_onehot):\n",
    "        s_idx = onehot_to_index(s_onehot)\n",
    "        return int(np.argmax(Q[s_idx]))\n",
    "    return policy\n",
    "\n",
    "pi_greedy = greedy_policy_from_Q(Q_tab)\n",
    "\n",
    "def rollout_and_print(env, policy, max_steps=30):\n",
    "    s = env.reset()\n",
    "    def idx_to_rc(idx):\n",
    "        return (idx // env.n, idx % env.n)\n",
    "    traj = []\n",
    "    for _ in range(max_steps):\n",
    "        idx = onehot_to_index(s)\n",
    "        traj.append(idx_to_rc(idx))\n",
    "        a = policy(s)\n",
    "        s, _, done = env.step(a)\n",
    "        if done:\n",
    "            idx = onehot_to_index(s)\n",
    "            traj.append(idx_to_rc(idx))\n",
    "            break\n",
    "    return traj\n",
    "\n",
    "rollout_and_print(env_gw, pi_greedy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6244bb3",
   "metadata": {},
   "source": [
    "#### **3. Extensión: DQN (Deep Q-Network)**\n",
    "\n",
    "Cuando el estado es grande/continuo, se aproxima $Q(s,a)$ con una red (**DQN**).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373ab6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DQN (Deep Q-Network)\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "def update_dqn(policy_network, target_network, optimizer, replay_buffer, batch_size=64, gamma=0.99):\n",
    "    state, action, reward, next_state, done = replay_buffer.sample(batch_size)\n",
    "\n",
    "    q_values = policy_network(state).gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "    next_q_values = target_network(next_state).max(1)[0]\n",
    "    expected_q_values = reward + gamma * next_q_values * (1 - done)\n",
    "\n",
    "    loss = torch.nn.functional.mse_loss(q_values, expected_q_values)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d941d38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complemento para que el bloque DQN sea ejecutable (ReplayBuffer + loop corto) \n",
    "\n",
    "from collections import deque\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=50_000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def add(self, s, a, r, s2, done):\n",
    "        self.buffer.append((s, a, r, s2, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        s, a, r, s2, d = zip(*batch)\n",
    "        state      = torch.tensor(np.array(s),  dtype=torch.float32)\n",
    "        action     = torch.tensor(np.array(a),  dtype=torch.int64)\n",
    "        reward     = torch.tensor(np.array(r),  dtype=torch.float32)\n",
    "        next_state = torch.tensor(np.array(s2), dtype=torch.float32)\n",
    "        done       = torch.tensor(np.array(d),  dtype=torch.float32)\n",
    "        return state, action, reward, next_state, done\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "def run_dqn_cartpole_short(steps=5000, batch_size=64, target_sync=500):\n",
    "    if gym is None:\n",
    "        raise RuntimeError(\"No se encontró gym/gymnasium.\")\n",
    "\n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "    s = reset_env(env, seed=SEED)\n",
    "    state_dim = len(s)\n",
    "    action_dim = env.action_space.n\n",
    "\n",
    "    policy_net = DQN(state_dim, action_dim)\n",
    "    target_net = DQN(state_dim, action_dim)\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "    opt = optim.Adam(policy_net.parameters(), lr=1e-3)\n",
    "    rb = ReplayBuffer(capacity=50_000)\n",
    "\n",
    "    eps = 1.0\n",
    "    eps_min = 0.05\n",
    "    eps_decay = 0.995\n",
    "\n",
    "    s = reset_env(env)\n",
    "    ep_return = 0.0\n",
    "    returns = []\n",
    "\n",
    "    for t in range(1, steps + 1):\n",
    "        # epsilon-greedy\n",
    "        if random.random() < eps:\n",
    "            a = env.action_space.sample()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                q = policy_net(torch.tensor(s, dtype=torch.float32).unsqueeze(0))\n",
    "                a = int(torch.argmax(q, dim=1).item())\n",
    "\n",
    "        s2, r, done = step_env(env, a)\n",
    "        rb.add(s, a, r, s2, float(done))\n",
    "        s = s2\n",
    "        ep_return += float(r)\n",
    "\n",
    "        if done:\n",
    "            returns.append(ep_return)\n",
    "            s = reset_env(env)\n",
    "            ep_return = 0.0\n",
    "            eps = max(eps_min, eps * eps_decay)\n",
    "\n",
    "        if len(rb) >= batch_size:\n",
    "            update_dqn(policy_net, target_net, opt, rb, batch_size=batch_size)\n",
    "\n",
    "        if t % target_sync == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "        if t % 1000 == 0 and len(returns) > 0:\n",
    "            print(f\"step {t:5d} | eps={eps:.2f} | retorno promedio (últimos 10)={np.mean(returns[-10:]):.1f}\")\n",
    "\n",
    "    env.close()\n",
    "    return returns\n",
    "\n",
    "# Ejecuta si quieres una demostración rápida:\n",
    "# dqn_returns = run_dqn_cartpole_short(steps=6000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a0d013",
   "metadata": {},
   "source": [
    "> Ejercicio: ¿dónde añadirías **experience replay** y una **red objetivo** (*target network*) para estabilizar DQN?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a8fc29-94c8-42c7-8576-17a1a5e4973f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tus respuestas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4409fc",
   "metadata": {},
   "source": [
    "#### **4. Policy Gradient (REINFORCE)**\n",
    "\n",
    "En lugar de aprender $Q$, parametrizamos directamente $\\pi_\\theta(a\\mid s)$.  \n",
    "REINFORCE usa $\\nabla_\\theta \\log \\pi_\\theta(a_t\\mid s_t)\\,G_t$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b410af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy Gradient\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.softmax(self.fc2(x), dim=-1)\n",
    "        return x\n",
    "\n",
    "def update_policy(policy_network, optimizer, rewards, log_probs):\n",
    "    discounted_rewards = []\n",
    "    cumulative_reward = 0\n",
    "    for reward in rewards[::-1]:\n",
    "        cumulative_reward = reward + 0.99 * cumulative_reward\n",
    "        discounted_rewards.insert(0, cumulative_reward)\n",
    "\n",
    "    discounted_rewards = torch.tensor(discounted_rewards)\n",
    "    discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-9)\n",
    "\n",
    "    policy_loss = []\n",
    "    for log_prob, reward in zip(log_probs, discounted_rewards):\n",
    "        policy_loss.append(-log_prob * reward)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    policy_loss = torch.cat(policy_loss).sum()\n",
    "    policy_loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31981e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reinforce_cartpole(num_episodes=200, lr=1e-2, gamma=0.99, max_steps=500):\n",
    "    if gym is None:\n",
    "        raise RuntimeError(\"No se encontró gym/gymnasium. Ejecuta esta sección en un entorno con gym instalado.\")\n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "    s0 = reset_env(env, seed=SEED)\n",
    "    state_dim = len(s0)\n",
    "    action_dim = env.action_space.n\n",
    "\n",
    "    policy = PolicyNetwork(state_dim, action_dim)\n",
    "    optimizer = optim.Adam(policy.parameters(), lr=lr)\n",
    "\n",
    "    history = []\n",
    "\n",
    "    for ep in range(num_episodes):\n",
    "        s = reset_env(env)\n",
    "        rewards, log_probs = [], []\n",
    "        ep_return = 0.0\n",
    "\n",
    "        for _ in range(max_steps):\n",
    "            s_t = torch.tensor(s, dtype=torch.float32).unsqueeze(0)\n",
    "            probs = policy(s_t)\n",
    "            dist = torch.distributions.Categorical(probs)\n",
    "            a = dist.sample()\n",
    "            log_probs.append(dist.log_prob(a))\n",
    "\n",
    "            s, r, done = step_env(env, int(a.item()))\n",
    "            rewards.append(float(r))\n",
    "            ep_return += float(r)\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        update_policy(policy, optimizer, rewards, log_probs)\n",
    "        history.append(ep_return)\n",
    "\n",
    "        if (ep + 1) % 25 == 0:\n",
    "            print(f\"EP {ep+1:4d} | retorno promedio (últimos 25): {np.mean(history[-25:]):.1f}\")\n",
    "\n",
    "    env.close()\n",
    "    return history\n",
    "\n",
    "# Ejecuta si tienes gym instalado:\n",
    "# hist = reinforce_cartpole(num_episodes=200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef42404",
   "metadata": {},
   "source": [
    "##### **4.1 Estabilidad (concepto): varianza y baseline**\n",
    "\n",
    "REINFORCE suele tener alta varianza. Una mejora mínima es restar un **baseline** $b(s)$:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta \\log \\pi_\\theta(a_t\\mid s_t)\\,(G_t - b(s_t))\n",
    "$$\n",
    "\n",
    "> Ejercicio: modifica `update_policy` para incluir un baseline constante o aprende un crítico $V_\\phi(s)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b59ec3",
   "metadata": {},
   "source": [
    "#### **5. Aplicaciones: juegos, \"robótica\" (control), recomendación (bandits)**\n",
    "\n",
    "- **Juegos (discretos):** CartPole/FrozenLake -> estados relativamente pequeños.\n",
    "- **Control continuo (proxy de robótica):** Pendulum -> acciones continuas, suelen requerir actor-critic.\n",
    "- **Recomendación:** frecuentemente **bandit contextual** (una decisión por interacción) o RL secuencial (slates).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34788913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (A) Juego / control discreto: CartPole (si gym existe)\n",
    "if gym is not None:\n",
    "    env_cp = gym.make(\"CartPole-v1\")\n",
    "    s = reset_env(env_cp, seed=SEED)\n",
    "    print(\"CartPole: dim estado =\", len(s), \"| acciones =\", env_cp.action_space.n)\n",
    "    env_cp.close()\n",
    "else:\n",
    "    print(\"gym no disponible: salta ejemplos CartPole/Pendulum.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49036318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (B) Proxy de robótica / control continuo: Pendulum (si gym existe)\n",
    "if gym is not None:\n",
    "    try:\n",
    "        env_pd = gym.make(\"Pendulum-v1\")\n",
    "        s = reset_env(env_pd, seed=SEED)\n",
    "        print(\"Pendulum: dim estado =\", len(s), \"| acción shape =\", env_pd.action_space.shape)\n",
    "        env_pd.close()\n",
    "    except Exception as e:\n",
    "        print(\"No se pudo crear Pendulum-v1:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaed6a32",
   "metadata": {},
   "source": [
    "##### **5.1 Recomendación como Bandit Contextual (mini-entorno)**\n",
    "\n",
    "Contexto $x$, acción $a$ (ítem), recompensa $r$ (click/tiempo/etc.).  \n",
    "Comparamos **epsilon-greedy** con modelo lineal vs **LinUCB**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31254490",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextualBandit:\n",
    "    def __init__(self, d=5, k=5, noise=0.1):\n",
    "        self.d = d\n",
    "        self.k = k\n",
    "        self.noise = noise\n",
    "        self.theta = np.random.randn(k, d).astype(np.float32)  # parámetro real por acción\n",
    "\n",
    "    def sample_context(self):\n",
    "        x = np.random.randn(self.d).astype(np.float32)\n",
    "        x /= (np.linalg.norm(x) + 1e-8)\n",
    "        return x\n",
    "\n",
    "    def reward(self, x, a):\n",
    "        mu = float(np.dot(self.theta[a], x))\n",
    "        return mu + np.random.randn() * self.noise\n",
    "\n",
    "def linucb(bandit, T=2000, alpha=1.0):\n",
    "    d, k = bandit.d, bandit.k\n",
    "    A = [np.eye(d, dtype=np.float32) for _ in range(k)]\n",
    "    b = [np.zeros(d, dtype=np.float32) for _ in range(k)]\n",
    "    rewards = []\n",
    "\n",
    "    for _ in range(T):\n",
    "        x = bandit.sample_context()\n",
    "        p = []\n",
    "        for a in range(k):\n",
    "            A_inv = np.linalg.inv(A[a])\n",
    "            theta_hat = A_inv @ b[a]\n",
    "            mean = float(theta_hat @ x)\n",
    "            conf = alpha * math.sqrt(float(x @ A_inv @ x))\n",
    "            p.append(mean + conf)\n",
    "\n",
    "        a = int(np.argmax(p))\n",
    "        r = bandit.reward(x, a)\n",
    "        A[a] += np.outer(x, x)\n",
    "        b[a] += r * x\n",
    "        rewards.append(r)\n",
    "\n",
    "    return rewards\n",
    "\n",
    "def eps_greedy_linear(bandit, T=2000, eps=0.1, lr=0.05):\n",
    "    d, k = bandit.d, bandit.k\n",
    "    W = np.zeros((k, d), dtype=np.float32)\n",
    "    rewards = []\n",
    "\n",
    "    for _ in range(T):\n",
    "        x = bandit.sample_context()\n",
    "        if random.random() < eps:\n",
    "            a = random.randrange(k)\n",
    "        else:\n",
    "            a = int(np.argmax(W @ x))\n",
    "        r = bandit.reward(x, a)\n",
    "        err = r - float(W[a] @ x)\n",
    "        W[a] += lr * err * x\n",
    "        rewards.append(r)\n",
    "\n",
    "    return rewards\n",
    "\n",
    "bandit = ContextualBandit(d=6, k=6, noise=0.2)\n",
    "r_eps = eps_greedy_linear(bandit, T=3000, eps=0.1, lr=0.05)\n",
    "r_ucb = linucb(bandit, T=3000, alpha=1.0)\n",
    "\n",
    "print(\"Promedio últimas 500 (eps-greedy):\", np.mean(r_eps[-500:]))\n",
    "print(\"Promedio últimas 500 (LinUCB):    \", np.mean(r_ucb[-500:]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4205974e",
   "metadata": {},
   "source": [
    "#### **6. Conexión RL–LLM: RLHF como RL sobre recompensas humanas (control -> intuición)**\n",
    "\n",
    "RLHF típico:\n",
    "\n",
    "1) **SFT** (imitación)  \n",
    "2) **Reward Model** (preferencias)  \n",
    "3) **RL (PPO) + restricción KL**  \n",
    "\n",
    "Primero: RLHF \"tipo control\" (CartPole) con **preferencias simuladas**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabab138",
   "metadata": {},
   "outputs": [],
   "source": [
    "def human_score_cartpole_state(s):\n",
    "    # s = [x, x_dot, theta, theta_dot]\n",
    "    x, xdot, theta, thetadot = s\n",
    "    return - (abs(theta) + 0.1 * abs(x) + 0.01 * abs(thetadot))\n",
    "\n",
    "def generate_state_pairs_from_env(env_name=\"CartPole-v1\", num_pairs=600, rollout_len=50):\n",
    "    if gym is None:\n",
    "        raise RuntimeError(\"No se encontró gym/gymnasium.\")\n",
    "    env = gym.make(env_name)\n",
    "\n",
    "    pairs_i, pairs_j, prefs = [], [], []\n",
    "\n",
    "    for _ in range(num_pairs):\n",
    "        s = reset_env(env)\n",
    "        for _t in range(random.randrange(1, rollout_len)):\n",
    "            a = env.action_space.sample()\n",
    "            s, _, done = step_env(env, a)\n",
    "            if done:\n",
    "                s = reset_env(env)\n",
    "        s_i = np.array(s, dtype=np.float32)\n",
    "\n",
    "        s = reset_env(env)\n",
    "        for _t in range(random.randrange(1, rollout_len)):\n",
    "            a = env.action_space.sample()\n",
    "            s, _, done = step_env(env, a)\n",
    "            if done:\n",
    "                s = reset_env(env)\n",
    "        s_j = np.array(s, dtype=np.float32)\n",
    "\n",
    "        pref = 1 if human_score_cartpole_state(s_i) > human_score_cartpole_state(s_j) else 0\n",
    "        pairs_i.append(s_i); pairs_j.append(s_j); prefs.append(pref)\n",
    "\n",
    "    env.close()\n",
    "    state_i = torch.tensor(np.stack(pairs_i), dtype=torch.float32)\n",
    "    state_j = torch.tensor(np.stack(pairs_j), dtype=torch.float32)\n",
    "    preferences = torch.tensor(np.array(prefs), dtype=torch.float32)\n",
    "    return (state_i, state_j), preferences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb16a5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BradleyTerryRewardModel(nn.Module):\n",
    "    def __init__(self, state_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = torch.relu(self.fc1(state))\n",
    "        return self.fc2(x)\n",
    "\n",
    "def preference_probability(model, state_i, state_j):\n",
    "    score_i = model(state_i)\n",
    "    score_j = model(state_j)\n",
    "    return torch.sigmoid(score_i - score_j)\n",
    "\n",
    "def train_reward_model_bt(model, optimizer, states, preferences, num_epochs=200):\n",
    "    loss_fn = nn.BCELoss()\n",
    "    state_i, state_j = states\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        probs = preference_probability(model, state_i, state_j).squeeze()\n",
    "        loss = loss_fn(probs, preferences)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 50 == 0:\n",
    "            with torch.no_grad():\n",
    "                acc = ((probs > 0.5).float() == preferences).float().mean().item()\n",
    "            print(f\"Epoca {epoch:3d} | loss={loss.item():.4f} | acc={acc:.3f}\")\n",
    "\n",
    "if gym is not None:\n",
    "    (si, sj), prefs = generate_state_pairs_from_env(num_pairs=800)\n",
    "    rm = BradleyTerryRewardModel(state_dim=si.shape[1])\n",
    "    opt_rm = optim.Adam(rm.parameters(), lr=1e-3)\n",
    "    train_reward_model_bt(rm, opt_rm, (si, sj), prefs, num_epochs=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b42030",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallPolicy(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        return torch.softmax(self.fc2(x), dim=-1)\n",
    "\n",
    "def collect_trajectory_scores(env, policy, rm, max_steps=200):\n",
    "    s = reset_env(env)\n",
    "    log_probs = []\n",
    "    scores = []\n",
    "    for _ in range(max_steps):\n",
    "        st = torch.tensor(s, dtype=torch.float32).unsqueeze(0)\n",
    "        probs = policy(st)\n",
    "        dist = torch.distributions.Categorical(probs)\n",
    "        a = dist.sample()\n",
    "        log_probs.append(dist.log_prob(a))\n",
    "\n",
    "        score = rm(st).squeeze()\n",
    "        scores.append(score)\n",
    "\n",
    "        s, _, done = step_env(env, int(a.item()))\n",
    "        if done:\n",
    "            break\n",
    "    return log_probs, scores\n",
    "\n",
    "def policy_gradient_update(policy, optimizer, log_probs, rewards_like, gamma=0.99):\n",
    "    returns = []\n",
    "    G = torch.tensor(0.0)\n",
    "    for r in reversed(rewards_like):\n",
    "        G = r + gamma * G\n",
    "        returns.insert(0, G)\n",
    "    returns = torch.stack(returns)\n",
    "    returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "\n",
    "    loss = -(torch.stack(log_probs) * returns.detach()).sum()\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return float(loss.item())\n",
    "\n",
    "def train_policy_with_rm(num_episodes=80, lr=2e-3):\n",
    "    if gym is None:\n",
    "        raise RuntimeError(\"No se encontró gym/gymnasium.\")\n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "    s0 = reset_env(env, seed=SEED)\n",
    "    policy = SmallPolicy(len(s0), env.action_space.n)\n",
    "    opt = optim.Adam(policy.parameters(), lr=lr)\n",
    "\n",
    "    for ep in range(num_episodes):\n",
    "        logp, scores = collect_trajectory_scores(env, policy, rm, max_steps=300)\n",
    "        loss = policy_gradient_update(policy, opt, logp, scores)\n",
    "        if (ep + 1) % 20 == 0:\n",
    "            print(f\"EP {ep+1:3d} | loss={loss:.3f} | steps={len(scores)}\")\n",
    "\n",
    "    env.close()\n",
    "    return policy\n",
    "\n",
    "# Ejecuta si RM se entrenó:\n",
    "# policy_rm = train_policy_with_rm()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b648ca",
   "metadata": {},
   "source": [
    "#### **7. RL y LLM (pipeline simplificado)**\n",
    "\n",
    "Sin usar Transformers, reproducimos el **esqueleto RLHF**:\n",
    "\n",
    "- Tokenización simple (vocabulario pequeño).\n",
    "- Reward Model entrenado con preferencias.\n",
    "- Policy optimization con penalización KL a la política base.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c847102",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB = [\"A\",\"B\",\"C\",\"D\",\"E\",\"<eos>\"]\n",
    "stoi = {t:i for i,t in enumerate(VOCAB)}\n",
    "itos = {i:t for t,i in stoi.items()}\n",
    "\n",
    "def decode(seq):\n",
    "    return \"\".join(itos[i] for i in seq if itos[i] != \"<eos>\")\n",
    "\n",
    "def sample_sequence(step_logits_fn, max_len=8):\n",
    "    seq = []\n",
    "    for t in range(max_len):\n",
    "        logits = step_logits_fn(t)\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        dist = torch.distributions.Categorical(probs)\n",
    "        tok = int(dist.sample().item())\n",
    "        seq.append(tok)\n",
    "        if itos[tok] == \"<eos>\":\n",
    "            break\n",
    "    return seq\n",
    "\n",
    "def human_score_text(seq):\n",
    "    s = decode(seq)\n",
    "    return 2.0*s.count(\"A\") - 0.2*len(s)\n",
    "\n",
    "def generate_text_preferences(num_pairs=400, max_len=8):\n",
    "    ref_logits = torch.zeros(max_len, len(VOCAB))\n",
    "    pairs = []\n",
    "    labels = []\n",
    "    for _ in range(num_pairs):\n",
    "        si = sample_sequence(lambda t: ref_logits[t], max_len=max_len)\n",
    "        sj = sample_sequence(lambda t: ref_logits[t], max_len=max_len)\n",
    "        yi = 1 if human_score_text(si) > human_score_text(sj) else 0\n",
    "        pairs.append((si, sj))\n",
    "        labels.append(yi)\n",
    "    return pairs, torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "pairs, y = generate_text_preferences()\n",
    "pairs[0], y[0].item(), decode(pairs[0][0]), decode(pairs[0][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b563c4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurize(seq):\n",
    "    v = np.zeros(len(VOCAB), dtype=np.float32)\n",
    "    for tok in seq:\n",
    "        v[tok] += 1.0\n",
    "    v /= (v.sum() + 1e-8)\n",
    "    return v\n",
    "\n",
    "X_i = torch.tensor(np.stack([featurize(si) for si, _ in pairs]), dtype=torch.float32)\n",
    "X_j = torch.tensor(np.stack([featurize(sj) for _, sj in pairs]), dtype=torch.float32)\n",
    "\n",
    "class SeqRewardModel(nn.Module):\n",
    "    def __init__(self, V):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(V, 64)\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "def train_rm_seq(model, optimizer, X_i, X_j, y, epochs=300):\n",
    "    bce = nn.BCELoss()\n",
    "    for e in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        si = model(X_i)\n",
    "        sj = model(X_j)\n",
    "        p = torch.sigmoid(si - sj).squeeze()\n",
    "        loss = bce(p, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if e % 75 == 0:\n",
    "            acc = ((p > 0.5).float() == y).float().mean().item()\n",
    "            print(f\"epoca {e:3d} | loss={loss.item():.4f} | acc={acc:.3f}\")\n",
    "\n",
    "rm_seq = SeqRewardModel(len(VOCAB))\n",
    "opt_rm = optim.Adam(rm_seq.parameters(), lr=1e-3)\n",
    "train_rm_seq(rm_seq, opt_rm, X_i, X_j, y, epochs=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a575c856",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToySequencePolicy(nn.Module):\n",
    "    def __init__(self, max_len, V):\n",
    "        super().__init__()\n",
    "        self.logits = nn.Parameter(torch.zeros(max_len, V))\n",
    "\n",
    "    def step_logits(self, t):\n",
    "        return self.logits[t]\n",
    "\n",
    "def seq_logprob(policy: ToySequencePolicy, seq):\n",
    "    lp = torch.tensor(0.0)\n",
    "    for t, tok in enumerate(seq):\n",
    "        probs = torch.softmax(policy.step_logits(t), dim=-1)\n",
    "        lp = lp + torch.log(probs[tok] + 1e-12)\n",
    "        if itos[tok] == \"<eos>\":\n",
    "            break\n",
    "    return lp\n",
    "\n",
    "def kl_stepwise(policy, ref_logits):\n",
    "    kls = []\n",
    "    for t in range(ref_logits.shape[0]):\n",
    "        p = torch.softmax(policy.step_logits(t), dim=-1)\n",
    "        q = torch.softmax(ref_logits[t], dim=-1)\n",
    "        kls.append((p * (torch.log(p + 1e-12) - torch.log(q + 1e-12))).sum())\n",
    "    return torch.stack(kls).mean()\n",
    "\n",
    "def rm_reward_for_seq(seq):\n",
    "    x = torch.tensor(featurize(seq), dtype=torch.float32).unsqueeze(0)\n",
    "    return rm_seq(x).squeeze()\n",
    "\n",
    "def train_toy_rlhf(steps=400, batch=32, beta=0.1, lr=5e-2, max_len=8):\n",
    "    ref_logits = torch.zeros(max_len, len(VOCAB))\n",
    "    policy = ToySequencePolicy(max_len, len(VOCAB))\n",
    "    opt = optim.SGD(policy.parameters(), lr=lr)\n",
    "\n",
    "    for it in range(steps):\n",
    "        opt.zero_grad()\n",
    "        seqs = [sample_sequence(lambda t: policy.step_logits(t), max_len=max_len) for _ in range(batch)]\n",
    "        rewards = torch.stack([rm_reward_for_seq(s) for s in seqs])\n",
    "        logps = torch.stack([seq_logprob(policy, s) for s in seqs])\n",
    "\n",
    "        pg_loss = -(rewards.detach() * logps).mean()\n",
    "        kl = kl_stepwise(policy, ref_logits)\n",
    "        loss = pg_loss + beta * kl\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        if (it + 1) % 80 == 0:\n",
    "            ex = sample_sequence(lambda t: policy.step_logits(t), max_len=max_len)\n",
    "            print(f\"it {it+1:3d} | loss={loss.item():.3f} | rm_reward={rm_reward_for_seq(ex).item():.2f} | ejemplo='{decode(ex)}'\")\n",
    "\n",
    "    return policy\n",
    "\n",
    "toy_policy = train_toy_rlhf()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3b06af",
   "metadata": {},
   "source": [
    "#### **8. Diseño de recompensas y desafíos de estabilidad/seguridad**\n",
    "\n",
    "Problemas típicos:\n",
    "\n",
    "- **Reward hacking / Goodhart:** maximiza la señal, no el objetivo real.\n",
    "- **Shaping mal calibrado:** induce políticas no deseadas.\n",
    "- **Inestabilidad:** bootstrapping + aproximación + off-policy puede divergir.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47be327",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoopTrapEnv:\n",
    "    '''\n",
    "    Línea 0..4, acciones: 0=left, 1=right\n",
    "    Objetivo: llegar a 4 (+1 y termina)\n",
    "    Recompensa mal diseñada: +0.2 al visitar estado 2 (loop trap)\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.pos = 0\n",
    "        self.done = False\n",
    "\n",
    "    def reset(self):\n",
    "        self.pos = 0\n",
    "        self.done = False\n",
    "        return self.pos\n",
    "\n",
    "    def step(self, a):\n",
    "        if self.done:\n",
    "            return self.pos, 0.0, True\n",
    "        if a == 0:\n",
    "            self.pos = max(0, self.pos - 1)\n",
    "        else:\n",
    "            self.pos = min(4, self.pos + 1)\n",
    "\n",
    "        r = 0.0\n",
    "        if self.pos == 2:\n",
    "            r += 0.2\n",
    "        if self.pos == 4:\n",
    "            r += 1.0\n",
    "            self.done = True\n",
    "        return self.pos, r, self.done\n",
    "\n",
    "def qlearn_looptrap(episodes=2000, alpha=0.2, gamma=0.95, eps=0.2):\n",
    "    env = LoopTrapEnv()\n",
    "    Q = np.zeros((5, 2), dtype=np.float32)\n",
    "    for _ in range(episodes):\n",
    "        s = env.reset()\n",
    "        for _t in range(50):\n",
    "            a = random.randrange(2) if random.random() < eps else int(np.argmax(Q[s]))\n",
    "            s2, r, done = env.step(a)\n",
    "            target = r + gamma * float(np.max(Q[s2])) * (0.0 if done else 1.0)\n",
    "            Q[s, a] += alpha * (target - float(Q[s, a]))\n",
    "            s = s2\n",
    "            if done:\n",
    "                break\n",
    "    return Q\n",
    "\n",
    "Q_lt = qlearn_looptrap()\n",
    "pi = [int(np.argmax(Q_lt[s])) for s in range(5)]\n",
    "pi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd33029",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = LoopTrapEnv()\n",
    "s = env.reset()\n",
    "traj = [s]\n",
    "ret = 0.0\n",
    "for _ in range(25):\n",
    "    a = int(np.argmax(Q_lt[s]))\n",
    "    s, r, done = env.step(a)\n",
    "    traj.append(s)\n",
    "    ret += r\n",
    "    if done:\n",
    "        break\n",
    "traj, ret\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da292b7",
   "metadata": {},
   "source": [
    "> Ejercicio: añade **penalización por paso** o haz que la recompensa del estado 2 decrezca con visitas.\n",
    "> ¿La política sigue \"hackeando\" la recompensa?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f39bba-2360-4955-9f9c-cbee9a77244b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tus respuestas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4604a4b9-5b29-442d-94bd-0cae23299f83",
   "metadata": {},
   "source": [
    "#### **Ejercicios**\n",
    "\n",
    "##### **Ejercicio 1 - Modelar un problema como MDP (formalización)**\n",
    "\n",
    "Elige 1 de estos escenarios: (i) GridWorld con obstáculos, (ii) robot aspirador en una habitación, (iii) recomendador de noticias con \"fatiga\" del usuario.\n",
    "\n",
    "1. Define $S, A, P(s'|s,a), R(s,a), \\gamma$.\n",
    "2. Da **dos** versiones de recompensa: una \"simple\" y otra \"robusta\".\n",
    "\n",
    "**Entregable.** Una tabla con $S,A,R,\\gamma$ + explicación breve (máx. 12 líneas).\n",
    "\n",
    "##### **Ejercicio 2 - Bellman a mano (valor bajo una política)**\n",
    "\n",
    " Para un MDP pequeño (4-6 estados), fija una política $\\pi$ y calcula a mano:\n",
    "\n",
    "* $V^\\pi(s)$ para cada estado resolviendo ecuaciones de Bellman.\n",
    "* $Q^\\pi(s,a)$ para 2 acciones por estado.\n",
    "\n",
    "**Entregable.** Sistema de ecuaciones + solución numérica + interpretación (¿qué estados son \"buenos\" y por qué?).\n",
    "\n",
    "##### **Ejercicio 3 - Evaluación de política por simulación vs. por ecuaciones**\n",
    "En el mismo MDP del ejercicio 2:\n",
    "\n",
    "1. Estima $V^\\pi(s)$ por Monte Carlo (promedio de retornos).\n",
    "2. Compara con tu solución exacta.\n",
    "\n",
    "**Entregable.** Tabla \"exacto vs estimado\" + discusión de error y varianza.\n",
    "\n",
    "##### **Ejercicio 4 - Q-learning tabular: exploración y convergencia**\n",
    "\n",
    "**Enunciado.** Entrena Q-learning tabular en un entorno discreto (GridWorld/Taxi/FrozenLake).\n",
    "\n",
    "* Prueba **3** esquemas de exploración: $\\epsilon$ fijo, decaimiento lineal, decaimiento exponencial.\n",
    "* Mide: retorno promedio, tasa de éxito, pasos por episodio.\n",
    "\n",
    "**Entregable.** 3 curvas + tabla comparativa + conclusión.\n",
    "\n",
    "##### **Ejercicio 5 - Ablación mínimo**\n",
    "\n",
    "Con el mejor esquema del ejercicio 4, ejecuta 4 variantes:\n",
    "\n",
    "1. sin descuento ($\\gamma=1$)), 2) ($\\gamma$) bajo, 3) ($\\alpha$) alto, 4) ($\\alpha$) bajo.\n",
    "\n",
    "**Entregable.** Tabla de métricas final + explicación causal (por qué cambia el comportamiento).\n",
    "\n",
    "##### **Ejercicio 6 - DQN (o aproximación con función): por qué ayuda**\n",
    "\n",
    "Si el cuaderno incluye DQN (o aproximación con red):\n",
    "\n",
    "1. Compara tabular vs aproximación en un entorno con estado más grande.\n",
    "2. Reporta inestabilidad: oscilaciones, divergencia, sensibilidad a seeds.\n",
    "\n",
    "**Entregable.** Curvas + 5 observaciones sobre estabilidad/fragilidad.\n",
    "\n",
    "\n",
    "##### **Ejercicio 7 -REINFORCE: gradiente de política con baseline**\n",
    "\n",
    "Implementa/usa REINFORCE en un entorno tipo CartPole.\n",
    "\n",
    "* Variante A: sin baseline.\n",
    "* Variante B: con baseline (promedio móvil o value function).\n",
    "\n",
    "**Entregable.** Curvas comparadas + explicación de varianza (qué mejora y qué no).\n",
    "\n",
    "##### **Ejercicio 8 - Q-learning vs Policy Gradient (cuándo conviene cada uno)**\n",
    "\n",
    "En el mismo entorno, corre Q-learning/DQN y REINFORCE (o PPO si está). Analiza: estabilidad, sample-efficiency, sensibilidad a hiperparámetros.\n",
    "\n",
    "**Entregable.** Una tabla \"pros/cons\" + evidencia (curvas/estadísticas) + recomendación.\n",
    "\n",
    "##### **Ejercicio 9 - \"Robótica\" simplificada: control continuo y reward shaping**\n",
    "\n",
    "Usa un entorno de control continuo (por ejemplo, Pendulum o equivalente disponible).\n",
    "\n",
    "1. Diseña 2 recompensas: (A) original, (B) con shaping.\n",
    "2. Evalúa si el shaping acelera el aprendizaje pero introduce sesgos.\n",
    "\n",
    "**Entregable.** Curvas + comparación de políticas finales + reflexión: ¿qué sacrificas por aprender más rápido?\n",
    "\n",
    "##### **Ejercicio 10 - Recomendación como bandit contextual**\n",
    "\n",
    "Simula un recomendador:\n",
    "\n",
    "* Contexto ($x$) (features del usuario/sesión), acciones = items, recompensa = click/tiempo.\n",
    "  Implementa 2 métodos: ($\\epsilon$)-greedy vs UCB o Thompson (si lo manejan).\n",
    "\n",
    "**Entregable.** Regret acumulado + tasa de click + discusión de exploración/explotación.\n",
    "\n",
    "\n",
    "##### **Ejercicio 11 - Preferencias humanas: modelo Bradley–Terry (reward model)**\n",
    "\n",
    "Construye un dataset pequeño de preferencias (pares A/B con etiqueta \"A mejor que B\").\n",
    "\n",
    "* Entrena un \"reward model\" simple (logístico/Bradley-Terry) que prediga preferencias.\n",
    "* Evalúa accuracy y calibration (si puedes).\n",
    "\n",
    "**Entregable.** Métricas + 5 ejemplos donde el modelo falla + hipótesis del porqué.\n",
    "\n",
    "##### **Ejercicio 12 - RLHF simplificado: política optimizada con recompensa aprendida**\n",
    "\n",
    "Usa tu reward model del ejercicio 11 para mejorar una política (aunque sea en un entorno \"elemental\" de texto/acciones discretas).\n",
    "\n",
    "* Define una política $\\pi_\\theta$ y actualízala para maximizar la recompensa del reward model.\n",
    "\n",
    "**Entregable.** Antes/después: distribución de acciones, recompensa media, ejemplos cualitativos.\n",
    "\n",
    "##### **Ejercicio 13 - Penalización KL (estabilidad y \"no desviarse\" del modelo base)**\n",
    "\n",
    "Implementa el objetivo:\n",
    "\n",
    "$$\n",
    "\\max_\\pi \\ \\mathbb{E}[r] - \\beta , D_{KL}(\\pi ,|, \\pi_{ref})\n",
    "$$\n",
    "\n",
    "* Prueba 3 valores de $\\beta$.\n",
    "* Observa: \"calidad\" vs \"drift\" de la política.\n",
    "\n",
    "**Entregable.** Curvas + tabla: recompensa vs KL + conclusión (qué $\\beta$ elegirías y por qué).\n",
    "\n",
    "##### **Ejercicio 14 - Reward hacking (demostración controlada)**\n",
    "\n",
    "Diseña una recompensa que \"parezca correcta\" pero permita un atajo indeseable.\n",
    "Ejemplos: maximizar \"velocidad\" sin penalizar choques; maximizar \"clics\" sin penalizar desinformación.\n",
    "\n",
    "1. Muestra el comportamiento problemático.\n",
    "2. Propón una recompensa mejor (con constraints/regularización).\n",
    " \n",
    "**Entregable.** Evidencia del hack + nueva recompensa + evaluación comparativa.\n",
    "\n",
    "##### **Ejercicio 15 - Checklist de seguridad/estabilidad para tu agente**\n",
    "\n",
    "Crea un checklist con al menos 10 ítems para auditar un experimento de RL/RLHF:\n",
    "\n",
    "* datos (sesgo/ruido), recompensas (especificación), estabilidad (seeds), evaluación (OOD), monitorización (drift), límites (constraints).\n",
    "\n",
    "**Entregable.** Checklist + breve justificación por ítem.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71c1586-f0a0-4120-a44a-1261c398d1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tus respuestas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
