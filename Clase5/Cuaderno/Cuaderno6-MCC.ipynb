{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4a77a28",
   "metadata": {},
   "source": [
    "### **Vision computacional moderna y modelos multimodales**\n",
    "\n",
    "Este cuaderno está pensado para **explicar conceptos** de forma **simple** y **sin GPU** usando 3 figuras:\n",
    "\n",
    "- `car` (auto deportivo al atardecer)\n",
    "- `cat` (gato pixelado)\n",
    "- `puppy` (cachorro en la nieve)\n",
    "\n",
    "\n",
    "> **Nota:** Este cuaderno no entrena modelos grandes ni usa GPU. Usa demostraciones pequeñas y visuales para explicar las ideas de la clase.\n",
    "\n",
    "> **Modo recomendado (clase):** ejecutar la ruta **CPU**. Las demostraciones con modelos grandes quedan como **anexo opcional**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e002885e",
   "metadata": {},
   "source": [
    "#### **0. Preparación: cargar imágenes desde la carpeta `figuras`**\n",
    "\n",
    "Este bloque busca las imágenes primero en una carpeta llamada `figuras/`y  si no existe, usa la carpeta actual.\n",
    "\n",
    "Nombres esperados:\n",
    "- `car.png`\n",
    "- `cat.png`\n",
    "- `puppy.png`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55335dfa",
   "metadata": {},
   "source": [
    "#### **Cómo usar este cuaderno en clase**\n",
    "\n",
    "- Ejecuta las celdas **en orden**.\n",
    "- Lee los bloques de explicación y luego completa las celdas con `TODO`.\n",
    "- Todo está diseñado para correr en **CPU** (sin GPU).\n",
    "- Las imágenes se buscan en:\n",
    "  1. `figuras/`\n",
    "  2. carpeta actual\n",
    "  3. `/mnt/data`\n",
    "\n",
    "> Consejo: si trabajas en Google Colab, sube una carpeta `figuras` con `car.png`, `cat.png` y `puppy.png`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6741f357",
   "metadata": {},
   "source": [
    "Al final del cuaderno deberías poder explicar, con tus palabras:\n",
    "\n",
    "1. Por qué se pasó de CNN a **Vision Transformers (ViT)** en muchos escenarios.\n",
    "2. Cómo **CLIP** aprende un espacio conjunto texto-imagen.\n",
    "3. Cómo funciona (conceptualmente) **VQA**.\n",
    "4. Cómo un **MLLM** conecta visión (y otras modalidades) con un LLM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9d0ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import textwrap\n",
    "\n",
    "# Rutas candidatas (primero la carpeta 'figuras')\n",
    "candidate_dirs = [\n",
    "    Path(\"figuras\"),\n",
    "    Path(\".\"),          # útil si ejecutas el notebook junto a las imágenes\n",
    "    Path(\"/mnt/data\"),  # útil en este entorno\n",
    "]\n",
    "\n",
    "expected = {\n",
    "    \"car\": \"car.png\",\n",
    "    \"cat\": \"cat.png\",\n",
    "    \"puppy\": \"puppy.png\",\n",
    "}\n",
    "\n",
    "def find_image(name):\n",
    "    filename = expected[name]\n",
    "    for d in candidate_dirs:\n",
    "        p = d / filename\n",
    "        if p.exists():\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "image_paths = {name: find_image(name) for name in expected}\n",
    "\n",
    "missing = [k for k, v in image_paths.items() if v is None]\n",
    "if missing:\n",
    "    raise FileNotFoundError(\n",
    "        f\"No encontré estas imágenes: {missing}. \"\n",
    "        \"Colócalas en una carpeta 'figuras/' o junto al notebook.\"\n",
    "    )\n",
    "\n",
    "image_paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab0ddd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga las imágenes\n",
    "images = {name: Image.open(path).convert(\"RGB\") for name, path in image_paths.items()}\n",
    "\n",
    "# Muestra las tres imágenes\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "for ax, (name, img) in zip(axes, images.items()):\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(name)\n",
    "    ax.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b9a201",
   "metadata": {},
   "source": [
    "#### **1. De convoluciones a Vision Transformers (ViT)**\n",
    "\n",
    "Las **CNNs** procesan imágenes con filtros locales (convoluciones). Son muy buenas detectando bordes, texturas y patrones espaciales.\n",
    "\n",
    "Los **Vision Transformers (ViT)** toman una idea del NLP:\n",
    "- partir la entrada en **tokens**\n",
    "- pasar esos tokens por un **Transformer encoder**\n",
    "\n",
    "En ViT, los \"tokens\" son **parches de imagen** (patches).\n",
    "\n",
    "##### **Idea central**\n",
    "- **CNN:** aprende patrones locales con kernels.\n",
    "- **ViT:** divide la imagen en parches y usa **self-attention** para modelar relaciones globales.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4406eec",
   "metadata": {},
   "source": [
    "##### **1.1 Demostración visual: convertir una imagen en patches (como ViT)**\n",
    "\n",
    "Usaremos la imagen del gato para ver cómo una imagen se divide en parches y luego se aplana.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af45d0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def patchify(img, patch_size=64):\n",
    "    arr = np.array(img)\n",
    "    h, w, c = arr.shape\n",
    "    h2 = (h // patch_size) * patch_size\n",
    "    w2 = (w // patch_size) * patch_size\n",
    "    arr = arr[:h2, :w2]\n",
    "\n",
    "    patches = []\n",
    "    coords = []\n",
    "    for y in range(0, h2, patch_size):\n",
    "        for x in range(0, w2, patch_size):\n",
    "            patches.append(arr[y:y+patch_size, x:x+patch_size])\n",
    "            coords.append((x, y))\n",
    "    return arr, patches, coords\n",
    "\n",
    "cat_img = images[\"cat\"]\n",
    "cat_cropped, cat_patches, cat_coords = patchify(cat_img, patch_size=64)\n",
    "\n",
    "print(\"Tamaño original:\", np.array(cat_img).shape)\n",
    "print(\"Tamaño recortado para patching:\", cat_cropped.shape)\n",
    "print(\"Número de parches:\", len(cat_patches))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870f0722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Muestra imagen recortada + grilla de patches\n",
    "arr = cat_cropped.copy()\n",
    "h, w, _ = arr.shape\n",
    "patch_size = 64\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.imshow(arr)\n",
    "for x in range(0, w+1, patch_size):\n",
    "    ax.axvline(x-0.5, linewidth=1)\n",
    "for y in range(0, h+1, patch_size):\n",
    "    ax.axhline(y-0.5, linewidth=1)\n",
    "ax.set_title(\"Imagen del gato dividida en patches (ViT)\")\n",
    "ax.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c9d0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Muestra algunos patches en secuencia (tokens visuales)\n",
    "n_show = min(8, len(cat_patches))\n",
    "fig, axes = plt.subplots(1, n_show, figsize=(2*n_show, 2))\n",
    "if n_show == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for i in range(n_show):\n",
    "    axes[i].imshow(cat_patches[i])\n",
    "    axes[i].set_title(f\"P{i}\")\n",
    "    axes[i].axis(\"off\")\n",
    "\n",
    "plt.suptitle(\"Parches (tokens) de la imagen\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1eb301",
   "metadata": {},
   "source": [
    "##### **1.2 Actividad guiada (ViT)-completa**\n",
    "\n",
    "Responde brevemente:\n",
    "\n",
    "1. ¿Qué representa un *patch* en ViT?\n",
    "2. ¿Por qué se dice que los patches se vuelven \"tokens visuales\"?\n",
    "3. ¿Qué problema aparece si aumentamos mucho la resolución de la imagen?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a052b064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (estudiante):\n",
    "# Cambia el tamaño de patch a 32, 48 y 96.\n",
    "# Observa cuántos patches aparecen y cómo cambia el detalle.\n",
    "\n",
    "tamaños = [32, 48, 96]\n",
    "\n",
    "for p in tamaños:\n",
    "    print(f\"Probando patch_size={p}\")\n",
    "    # Reutiliza la imagen 'cat' si existe en el entorno del cuaderno original\n",
    "    # Sugerencia:\n",
    "    # patches, grid = patchify(images[\"cat\"], patch_size=p)\n",
    "    # print(\"Número de patches:\", len(patches))\n",
    "    # display(grid)\n",
    "    print(\"TODO\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a96a84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (estudiante):\n",
    "# Completa una función que estime cuántos tokens visuales genera ViT\n",
    "# para una imagen HxW con patch_size = p.\n",
    "\n",
    "def num_patches(h, w, p):\n",
    "    # return ...\n",
    "    pass\n",
    "\n",
    "ejemplos = [\n",
    "    (224, 224, 16),\n",
    "    (224, 224, 32),\n",
    "    (384, 384, 16),\n",
    "]\n",
    "\n",
    "for h, w, p in ejemplos:\n",
    "    print((h, w, p), \"->\", num_patches(h, w, p))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7a31e0",
   "metadata": {},
   "source": [
    "##### **1.3 ¿Qué hace ViT después de los patches?**\n",
    "\n",
    "A nivel conceptual (sin entrenar nada aquí):\n",
    "\n",
    "1. **Patchify** la imagen.\n",
    "2. Convertir cada patch en un vector (**linear projection**).\n",
    "3. Agregar información posicional (**positional embeddings**).\n",
    "4. Pasar la secuencia por un **Transformer Encoder**.\n",
    "5. Usar el token especial `[CLS]` (o pooling) para clasificar.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d69608-40f3-4cf3-8516-f6fb746f1754",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tus respuestas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a107f931",
   "metadata": {},
   "source": [
    "\n",
    "#### **2. Representaciones conjuntas texto-imagen (CLIP) y recuperación**\n",
    "\n",
    "##### **2.1 Idea de CLIP (elemental)**\n",
    "\n",
    "CLIP aprende dos encoders:\n",
    "- un **text encoder**\n",
    "- un **image encoder**\n",
    "\n",
    "Ambos producen embeddings en el **mismo espacio vectorial**, de modo que:\n",
    "- una imagen de un cachorro queda cerca de textos sobre un cachorro\n",
    "- una imagen de un auto queda cerca de textos sobre autos\n",
    "\n",
    "##### **2.2 Demostración sin un modelo grande**\n",
    "Aquí haremos una **versión explicativa**:\n",
    "- Definimos captions de texto.\n",
    "- Calculamos \"embeddings\" simples para mostrar la lógica de similitud y recuperación.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0cd205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Captions (puedes cambiarlos)\n",
    "captions = {\n",
    "    \"car\": \"A sports car driving on the road at sunset\",\n",
    "    \"cat\": \"A pixelated image of a cute cat\",\n",
    "    \"puppy\": \"A puppy playing in the snow\",\n",
    "}\n",
    "\n",
    "captions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9215cbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings \"simples\" no reales\n",
    "# Definimos un vocabulario pequeño para representar conceptos.\n",
    "vocab = [\n",
    "    \"car\", \"road\", \"sunset\",\n",
    "    \"cat\", \"pixelated\", \"cute\",\n",
    "    \"puppy\", \"dog\", \"snow\",\n",
    "]\n",
    "\n",
    "vocab_index = {w: i for i, w in enumerate(vocab)}\n",
    "\n",
    "def text_embedding(text):\n",
    "    vec = np.zeros(len(vocab), dtype=float)\n",
    "    t = text.lower()\n",
    "    for w in vocab:\n",
    "        if w in t:\n",
    "            vec[vocab_index[w]] += 1.0\n",
    "    # Normalización\n",
    "    norm = np.linalg.norm(vec)\n",
    "    return vec / norm if norm > 0 else vec\n",
    "\n",
    "# Para imágenes, hacemos un embedding simbólico asociado a su contenido (didáctico)\n",
    "image_semantics = {\n",
    "    \"car\": [\"car\", \"road\", \"sunset\"],\n",
    "    \"cat\": [\"cat\", \"pixelated\", \"cute\"],\n",
    "    \"puppy\": [\"puppy\", \"dog\", \"snow\"],\n",
    "}\n",
    "\n",
    "def image_embedding(image_name):\n",
    "    vec = np.zeros(len(vocab), dtype=float)\n",
    "    for w in image_semantics[image_name]:\n",
    "        vec[vocab_index[w]] += 1.0\n",
    "    norm = np.linalg.norm(vec)\n",
    "    return vec / norm if norm > 0 else vec\n",
    "\n",
    "# Construimos embeddings\n",
    "img_embs = {name: image_embedding(name) for name in images}\n",
    "txt_embs = {name: text_embedding(txt) for name, txt in captions.items()}\n",
    "\n",
    "img_embs[\"cat\"], txt_embs[\"cat\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca32630",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine(a, b):\n",
    "    denom = (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "    if denom == 0:\n",
    "        return 0.0\n",
    "    return float(np.dot(a, b) / denom)\n",
    "\n",
    "# Matriz de similitud texto-imagen (filas: textos, columnas: imágenes)\n",
    "img_names = list(images.keys())\n",
    "txt_names = list(captions.keys())\n",
    "\n",
    "sim_matrix = np.zeros((len(txt_names), len(img_names)))\n",
    "for i, tname in enumerate(txt_names):\n",
    "    for j, iname in enumerate(img_names):\n",
    "        sim_matrix[i, j] = cosine(txt_embs[tname], img_embs[iname])\n",
    "\n",
    "sim_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5776e7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualización de la matriz de similitud (estilo CLIP retrieval)\n",
    "fig, ax = plt.subplots(figsize=(7, 4))\n",
    "im = ax.imshow(sim_matrix)\n",
    "\n",
    "ax.set_xticks(range(len(img_names)))\n",
    "ax.set_xticklabels(img_names)\n",
    "ax.set_yticks(range(len(txt_names)))\n",
    "ax.set_yticklabels([captions[t] for t in txt_names])\n",
    "\n",
    "# Etiquetas envueltas para que no se corten\n",
    "ax.set_yticklabels([textwrap.fill(captions[t], 28) for t in txt_names])\n",
    "\n",
    "for i in range(sim_matrix.shape[0]):\n",
    "    for j in range(sim_matrix.shape[1]):\n",
    "        ax.text(j, i, f\"{sim_matrix[i,j]:.2f}\", ha=\"center\", va=\"center\")\n",
    "\n",
    "ax.set_title(\"Matriz de similitud texto–imagen (demo tipo CLIP)\")\n",
    "plt.colorbar(im, ax=ax, fraction=0.045)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7105e7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recuperación texto -> imagen (image retrieval)\n",
    "def retrieve_image(query_text, top_k=3):\n",
    "    q = text_embedding(query_text)\n",
    "    scores = []\n",
    "    for iname in img_names:\n",
    "        s = cosine(q, img_embs[iname])\n",
    "        scores.append((iname, s))\n",
    "    scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    return scores[:top_k]\n",
    "\n",
    "queries = [\n",
    "    \"a cute pixelated cat\",\n",
    "    \"a dog in the snow\",\n",
    "    \"a fast car on the road at sunset\",\n",
    "]\n",
    "\n",
    "for q in queries:\n",
    "    print(\"\\nConsulta:\", q)\n",
    "    for iname, score in retrieve_image(q):\n",
    "        print(f\"  {iname:>5s}  score={score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12c3908",
   "metadata": {},
   "source": [
    "#### **2.3 Demostración opcional con CLIP real**\n",
    "\n",
    "Esta sección agrega una **versión real** usando `transformers` y el modelo **`openai/clip-vit-base-patch32`**.\n",
    "\n",
    "Aquí se añade:\n",
    "\n",
    "- embeddings reales texto–imagen,\n",
    "- matriz de similitud real,\n",
    "- recuperación **texto->imagen** e **imagen->texto**,\n",
    "- métricas **Recall@K** y **mAP**.\n",
    "\n",
    "> **Nota:** requiere `transformers` (y normalmente internet para descargar el modelo la primera vez).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccf53ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [OPCIONAL] Si estás en Colab/entorno limpio, descomenta:\n",
    "# !pip install transformers torch pillow matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb49d90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLIP real (texto↔imagen) usando las imágenes y captions del cuaderno\n",
    "# Reutiliza:\n",
    "# - image_paths (definido en la sección 0)\n",
    "# - captions (definido en la sección 2)\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    from PIL import Image\n",
    "    from transformers import CLIPTokenizerFast, CLIPProcessor, CLIPModel\n",
    "except Exception as e:\n",
    "    raise ImportError(\n",
    "        \"Faltan dependencias para CLIP real. Instala transformers/torch/pillow/matplotlib.\"\n",
    "    ) from e\n",
    "\n",
    "model_id = \"openai/clip-vit-base-patch32\"\n",
    "clip_tokenizer = CLIPTokenizerFast.from_pretrained(model_id)\n",
    "clip_processor = CLIPProcessor.from_pretrained(model_id)\n",
    "clip_model = CLIPModel.from_pretrained(model_id)\n",
    "\n",
    "# Orden fijo para mapear imagen <-> caption (ground truth 1:1)\n",
    "keys = [\"car\", \"cat\", \"puppy\"]\n",
    "\n",
    "# Cargar imágenes reales desde rutas locales del cuaderno\n",
    "clip_images = [Image.open(image_paths[k]).convert(\"RGB\") for k in keys]\n",
    "clip_captions = [captions[k] for k in keys]\n",
    "\n",
    "# Embeddings de imágenes\n",
    "img_batch = clip_processor(images=clip_images, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    image_embeddings = clip_model.get_image_features(pixel_values=img_batch[\"pixel_values\"])\n",
    "\n",
    "# Embeddings de texto\n",
    "txt_batch = clip_tokenizer(\n",
    "    clip_captions,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    truncation=True\n",
    ")\n",
    "with torch.no_grad():\n",
    "    text_embeddings = clip_model.get_text_features(**txt_batch)\n",
    "\n",
    "# Normalización (para coseno via producto punto)\n",
    "image_embeddings = image_embeddings / image_embeddings.norm(dim=-1, keepdim=True)\n",
    "text_embeddings = text_embeddings / text_embeddings.norm(dim=-1, keepdim=True)\n",
    "\n",
    "# Matriz de similitud (filas=imágenes, columnas=textos)\n",
    "sim_matrix_real = (image_embeddings @ text_embeddings.T).cpu().numpy()\n",
    "\n",
    "print(\"Claves:\", keys)\n",
    "print(\"Matriz de similitud real CLIP (imagen x texto):\")\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "print(sim_matrix_real)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e088d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiza la matriz de similitud + retrieval + métricas (Recall@K, mAP)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Etiquetas legibles\n",
    "row_labels = [f\"img:{k}\" for k in keys]\n",
    "col_labels = [f\"txt:{k}\" for k in keys]\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.imshow(sim_matrix_real)\n",
    "plt.xticks(range(len(col_labels)), col_labels, rotation=20)\n",
    "plt.yticks(range(len(row_labels)), row_labels)\n",
    "\n",
    "for i in range(sim_matrix_real.shape[0]):\n",
    "    for j in range(sim_matrix_real.shape[1]):\n",
    "        plt.text(j, i, f\"{sim_matrix_real[i, j]:.2f}\", ha=\"center\", va=\"center\")\n",
    "\n",
    "plt.title(\"CLIP real: matriz de similitud\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "def ranks_text_to_image(sim_matrix):\n",
    "    # sim_matrix: imagen x texto\n",
    "    # para cada texto (columna), rankear imágenes (filas)\n",
    "    ranks = []\n",
    "    for j in range(sim_matrix.shape[1]):\n",
    "        order = np.argsort(-sim_matrix[:, j])  # mayor similitud primero\n",
    "        gt = j  # ground truth 1:1\n",
    "        rank_pos = int(np.where(order == gt)[0][0]) + 1  # rank inicia en 1\n",
    "        ranks.append(rank_pos)\n",
    "    return ranks\n",
    "\n",
    "\n",
    "def ranks_image_to_text(sim_matrix):\n",
    "    # para cada imagen (fila), rankear textos (columnas)\n",
    "    ranks = []\n",
    "    for i in range(sim_matrix.shape[0]):\n",
    "        order = np.argsort(-sim_matrix[i, :])\n",
    "        gt = i\n",
    "        rank_pos = int(np.where(order == gt)[0][0]) + 1\n",
    "        ranks.append(rank_pos)\n",
    "    return ranks\n",
    "\n",
    "\n",
    "def recall_at_k(ranks, k):\n",
    "    return float(np.mean([r <= k for r in ranks]))\n",
    "\n",
    "\n",
    "def mean_ap_single_relevant(ranks):\n",
    "    # Con 1 relevante por consulta: AP = 1/rank\n",
    "    ap = [1.0 / r for r in ranks]\n",
    "    return float(np.mean(ap))\n",
    "\n",
    "\n",
    "r_t2i = ranks_text_to_image(sim_matrix_real)\n",
    "r_i2t = ranks_image_to_text(sim_matrix_real)\n",
    "\n",
    "print(\"Ranks texto->imagen:\", r_t2i)\n",
    "print(\"Ranks imagen->texto:\", r_i2t)\n",
    "\n",
    "for k in [1, 2, 3]:\n",
    "    print(f\"Recall@{k} texto->imagen: {recall_at_k(r_t2i, k):.3f}\")\n",
    "    print(f\"Recall@{k} imagen->texto: {recall_at_k(r_i2t, k):.3f}\")\n",
    "\n",
    "print(f\"mAP (texto->imagen, 1 relevante/consulta): {mean_ap_single_relevant(r_t2i):.3f}\")\n",
    "print(f\"mAP (imagen->texto, 1 relevante/consulta): {mean_ap_single_relevant(r_i2t):.3f}\")\n",
    "\n",
    "\n",
    "# Muestra el top-k de retrieval en ambos sentidos\n",
    "def topk_text_to_image(text_idx, k=3):\n",
    "    order = np.argsort(-sim_matrix_real[:, text_idx])[:k]\n",
    "    return [keys[i] for i in order]\n",
    "\n",
    "def topk_image_to_text(image_idx, k=3):\n",
    "    order = np.argsort(-sim_matrix_real[image_idx, :])[:k]\n",
    "    return [keys[j] for j in order]\n",
    "\n",
    "print(\"\\nEjemplos de retrieval:\")\n",
    "for j, key in enumerate(keys):\n",
    "    print(f\"Texto '{key}' -> Top-3 imágenes:\", topk_text_to_image(j, k=3))\n",
    "\n",
    "for i, key in enumerate(keys):\n",
    "    print(f\"Imagen '{key}' -> Top-3 textos:\", topk_image_to_text(i, k=3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787b7cc0",
   "metadata": {},
   "source": [
    "##### **2.4 Actividad guiada (CLIP real)- completa aquí**\n",
    "\n",
    "Estas celdas permiten **replicar** la lógica de retrieval real usando `sim_matrix_real` y `keys`.\n",
    "\n",
    "> Si no ejecutaste la demostración real, puedes completar el código de forma conceptual y comentar qué faltaría para correrlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9b0bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (estudiante): completar retrieval texto -> imagen\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def topk_text_to_image_estudiante(sim_matrix, keys, text_idx, k=3):\n",
    "    # Pista:\n",
    "    # order = np.argsort(-sim_matrix[:, text_idx])[:k]\n",
    "    # return [keys[i] for i in order]\n",
    "    return []\n",
    "\n",
    "# Prueba sugerida (descomenta cuando completes):\n",
    "# for j, key in enumerate(keys):\n",
    "#     print(f\"Texto {key} ->\", topk_text_to_image_estudiante(sim_matrix_real, keys, j, k=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ec0892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (estudiante): completar retrieval imagen -> texto\n",
    "\n",
    "def topk_image_to_text_estudiante(sim_matrix, keys, image_idx, k=3):\n",
    "    # Pista:\n",
    "    # order = np.argsort(-sim_matrix[image_idx, :])[:k]\n",
    "    # return [keys[j] for j in order]\n",
    "    return []\n",
    "\n",
    "# Prueba sugerida:\n",
    "# for i, key in enumerate(keys):\n",
    "#     print(f\"Imagen {key} ->\", topk_image_to_text_estudiante(sim_matrix_real, keys, i, k=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8cc938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (estudiante): completar métricas de retrieval (Recall@K, mAP simplificado)\n",
    "\n",
    "def recall_at_k_estudiante(ranks, k):\n",
    "    # return float(np.mean([r <= k for r in ranks]))\n",
    "    return None\n",
    "\n",
    "def mean_ap_single_relevant_estudiante(ranks):\n",
    "    # AP = 1/rank cuando hay 1 relevante por consulta\n",
    "    # return float(np.mean([1.0/r for r in ranks]))\n",
    "    return None\n",
    "\n",
    "# Reto:\n",
    "# 1) Construye una lista `ranks` (posición del relevante correcto en el ranking)\n",
    "# 2) Reporta Recall@1, Recall@3, mAP\n",
    "# 3) Interpreta el resultado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0bc7b0",
   "metadata": {},
   "source": [
    "> Nota: En CLIP real, los embeddings se aprenden con **contrastive learning** usando millones de pares imagen-texto.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b5a4f2",
   "metadata": {},
   "source": [
    "##### **2.5 Actividad guiada (CLIP)-interpretación de retrieval**\n",
    "\n",
    "Observa la matriz de similitud y responde:\n",
    "\n",
    "1. ¿Qué significa una puntuación alta entre una frase y una imagen?\n",
    "2. ¿Por qué en CLIP suele ser útil normalizar embeddings?\n",
    "3. ¿Qué pasa si dos captions son muy parecidos entre sí?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316d1ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (estudiante):\n",
    "# Cambia las captions para introducir ambigüedad.\n",
    "# Ejemplo: pon dos captions muy similares para 'cat' y 'puppy'\n",
    "# y vuelve a ejecutar la matriz de similitud.\n",
    "\n",
    "captions_estudiante = {\n",
    "    \"cat\": \"TODO: escribe un caption\",\n",
    "    \"puppy\": \"TODO: escribe un caption\",\n",
    "    \"car\": \"TODO: escribe un caption\",\n",
    "}\n",
    "\n",
    "print(captions_estudiante)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6d7ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (estudiante):\n",
    "# Implementa una recuperación simple texto -> top-k imágenes usando cosine().\n",
    "\n",
    "def recuperar_topk(text_vec, image_vectors, k=2):\n",
    "    # image_vectors: dict nombre -> vector\n",
    "    # return lista ordenada [(nombre, score), ...]\n",
    "    pass\n",
    "\n",
    "# Ejemplo de uso (ajústalo con tus variables del cuaderno):\n",
    "# top = recuperar_topk(text_embeds[\"cat\"], image_embeds, k=2)\n",
    "# print(top)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c040f4",
   "metadata": {},
   "source": [
    "##### **Ejercicios textuales (sin código)**\n",
    "\n",
    "- Explica la diferencia entre:\n",
    "  - **Clasificación supervisada** (cat/dog)\n",
    "  - **Recuperación multimodal** (texto <-> imagen)\n",
    "- Da un ejemplo real de uso de CLIP en:\n",
    "  - e-commerce.\n",
    "  - moderación de contenido.\n",
    "  - búsqueda en repositorios multimedia.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f9ab03",
   "metadata": {},
   "source": [
    "#### **3. Modelos visión-lenguaje y VQA (Visual Question Answering)**\n",
    "\n",
    "Un sistema VQA recibe:\n",
    "- una **imagen**\n",
    "- una **pregunta en texto**\n",
    "\n",
    "y produce una **respuesta en texto**.\n",
    "\n",
    "Ejemplo:\n",
    "\n",
    "- Imagen: auto\n",
    "- Pregunta: *What do you see?*\n",
    "- Respuesta: *A sports car driving on the road at sunset.*\n",
    "\n",
    "##### **3.1 Demostración elemental (reglas + conocimiento visual mínimo)**\n",
    "Esta demostración **no es un modelo profundo**, pero sirve para explicar el flujo completo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa05018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base de conocimiento simple (hecha a mano) para explicar VQA\n",
    "visual_facts = {\n",
    "    \"car\": {\n",
    "        \"objects\": [\"car\", \"road\", \"sunset\"],\n",
    "        \"scene\": \"A sports car driving on the road at sunset.\",\n",
    "        \"weather\": \"clear sky with sunset light\",\n",
    "        \"dominant_topic\": \"transportation\"\n",
    "    },\n",
    "    \"cat\": {\n",
    "        \"objects\": [\"cat\", \"pixel-art\", \"cute\"],\n",
    "        \"scene\": \"A pixelated image of a cute cat.\",\n",
    "        \"weather\": \"not applicable\",\n",
    "        \"dominant_topic\": \"animal\"\n",
    "    },\n",
    "    \"puppy\": {\n",
    "        \"objects\": [\"puppy\", \"snow\", \"park\"],\n",
    "        \"scene\": \"A puppy sitting in the snow.\",\n",
    "        \"weather\": \"snowy\",\n",
    "        \"dominant_topic\": \"animal\"\n",
    "    },\n",
    "}\n",
    "\n",
    "def vqa_demo(image_name, question):\n",
    "    q = question.lower().strip()\n",
    "    facts = visual_facts[image_name]\n",
    "\n",
    "    if \"what\" in q and (\"see\" in q or \"image\" in q or \"picture\" in q):\n",
    "        return facts[\"scene\"]\n",
    "    if \"animal\" in q:\n",
    "        return \"Yes.\" if facts[\"dominant_topic\"] == \"animal\" else \"No.\"\n",
    "    if \"weather\" in q or \"snow\" in q:\n",
    "        return f\"Weather/scene condition: {facts['weather']}.\"\n",
    "    if \"objects\" in q or \"things\" in q:\n",
    "        return \"I can identify: \" + \", \".join(facts[\"objects\"]) + \".\"\n",
    "    if \"car\" in q:\n",
    "        return \"Yes, there is a car.\" if \"car\" in facts[\"objects\"] else \"No, I do not see a car.\"\n",
    "    return \"I need a more specific question (what, objects, weather, animal, car).\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cb9f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prueba VQA en las tres imágenes\n",
    "sample_questions = [\n",
    "    \"What do you see in this picture?\",\n",
    "    \"What objects are visible?\",\n",
    "    \"Is there an animal?\",\n",
    "    \"What is the weather?\",\n",
    "    \"Is there a car?\",\n",
    "]\n",
    "\n",
    "for name in [\"car\", \"cat\", \"puppy\"]:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Imagen:\", name)\n",
    "    for q in sample_questions:\n",
    "        print(\"Q:\", q)\n",
    "        print(\"A:\", vqa_demo(name, q))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adc3203",
   "metadata": {},
   "source": [
    "##### **3.2 ¿Cómo sería VQA moderno (real)?**\n",
    "Un modelo moderno hace algo parecido a esto:\n",
    "\n",
    "1. **Encoder visual** (por ejemplo ViT) produce representaciones de la imagen.\n",
    "2. **Encoder/LLM de texto** procesa la pregunta.\n",
    "3. Un módulo de fusión (cross-attention, Q-Former, etc.) combina ambos.\n",
    "4. El decodificador/LLM genera la respuesta.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a41864",
   "metadata": {},
   "source": [
    "##### **3.3 Actividad guiada (VQA)-diseño de preguntas**\n",
    "\n",
    "Formula 2 preguntas por imagen:\n",
    "\n",
    "- una **descriptiva** (qué hay en la imagen),\n",
    "- una **inferencial** (qué podría estar pasando).\n",
    "\n",
    "Luego compáralas con la respuesta de la demostración y comenta:\n",
    "\n",
    "- ¿Qué puede responder bien?\n",
    "- ¿Qué no puede responder bien una demo basada en reglas?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e82fd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (estudiante):\n",
    "# Crea una función para evaluar preguntas \"fuera de dominio\"\n",
    "# (por ejemplo, pedir color exacto, velocidad exacta, marca, etc.)\n",
    "\n",
    "preguntas_prueba = [\n",
    "    (\"car\", \"¿Qué objeto aparece?\"),\n",
    "    (\"car\", \"¿Qué velocidad exacta lleva?\"),\n",
    "    (\"cat\", \"¿Es un gato pixelado?\"),\n",
    "    (\"puppy\", \"¿Qué raza exacta es?\"),\n",
    "]\n",
    "\n",
    "# Sugerencia: recorre preguntas_prueba y llama a la función VQA del cuaderno\n",
    "for item in preguntas_prueba:\n",
    "    print(item, \"-> TODO\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e05800",
   "metadata": {},
   "source": [
    "##### **Ejercicio adicional**\n",
    "\n",
    "Describe, en 5-7 líneas, qué componentes adicionales necesitaría un sistema VQA moderno:\n",
    "\n",
    "- encoder visual,\n",
    "- encoder de texto / tokenizer,\n",
    "- módulo de fusión,\n",
    "- decoder o clasificador,\n",
    "- datos anotados.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5705d68b",
   "metadata": {},
   "source": [
    "#### **4. LLM multimodales (MLLM): texto, imagen, audio, vídeo**\n",
    "\n",
    "Un **MLLM** extiende un LLM para aceptar varias modalidades:\n",
    "\n",
    "- **Texto** (prompt)\n",
    "- **Imagen**\n",
    "- **Audio**\n",
    "- **Vídeo**\n",
    "\n",
    "El problema central es: **¿cómo convertir cada modalidad a un formato que el LLM pueda usar?**\n",
    "\n",
    "##### **4.1 Solución típica (alto nivel)**\n",
    "- Imagen -> **Vision encoder** (ViT)\n",
    "- Audio -> **Audio encoder** (por ejemplo, espectrogramas + transformer)\n",
    "- Vídeo -> encoder visual-temporal (frames + tiempo)\n",
    "- Luego un **módulo puente** (por ejemplo **Q-Former**) adapta esas representaciones para el LLM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669232af",
   "metadata": {},
   "source": [
    "##### **4.2 Q-Former (intuición elemental)**\n",
    "\n",
    "El **Q-Former** actúa como un **adaptador multimodal** y se sigue este proceso:\n",
    "\n",
    "- Un **Vision Transformer preentrenado** extrae características visuales.\n",
    "- Un **Q-Former entrenable** aprende consultas útiles (\"queries\") sobre esas características.\n",
    "- Esas salidas se **proyectan** al espacio del **LLM preentrenado**.\n",
    "- El LLM genera texto (descripción, respuesta VQA, resumen, etc.).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9423294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulación conceptual de un pipeline MLLM (texto, imagen, audio, vídeo)\n",
    "# No ejecuta modelos grandes: solo muestra el flujo.\n",
    "\n",
    "def mllm_router(modality, payload):\n",
    "    if modality == \"text\":\n",
    "        return {\"encoder\": \"Text encoder/tokenizer\", \"output\": \"text embeddings\"}\n",
    "    elif modality == \"image\":\n",
    "        return {\"encoder\": \"Vision encoder (ViT)\", \"output\": \"visual embeddings\"}\n",
    "    elif modality == \"audio\":\n",
    "        return {\"encoder\": \"Audio encoder\", \"output\": \"audio embeddings\"}\n",
    "    elif modality == \"video\":\n",
    "        return {\"encoder\": \"Video encoder (frames + temporal modeling)\", \"output\": \"video embeddings\"}\n",
    "    else:\n",
    "        return {\"encoder\": \"Unknown\", \"output\": \"N/A\"}\n",
    "\n",
    "modalities = [\"text\", \"image\", \"audio\", \"video\"]\n",
    "for m in modalities:\n",
    "    step = mllm_router(m, None)\n",
    "    print(f\"{m:>5s} -> {step['encoder']} -> {step['output']} -> adapter/Q-Former -> LLM -> texto\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c426ced",
   "metadata": {},
   "source": [
    "##### **4.3 Ruta **CPU-only** (sin BLIP-2 pesado)**\n",
    "\n",
    "Esta ruta refuerza el patrón multimodal (texto + imagen + audio + video) con **datos ligeros y reglas explicables**.  \n",
    "\n",
    "Sirve como práctica principal del curso cuando no se dispone de GPU.\n",
    "\n",
    "> **Sugerencia:** usa esta sección solo como demostración final o actividad opcional en Colab con GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f4d69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mini-lab CPU-only: respuesta multimodal con metadatos (didáctico)\n",
    "\n",
    "audio_db = {\n",
    "    \"audio_1\": {\"evento\": \"ladrido\", \"duracion_s\": 2.3, \"confianza\": 0.88},\n",
    "    \"audio_2\": {\"evento\": \"motor\", \"duracion_s\": 1.1, \"confianza\": 0.73},\n",
    "}\n",
    "\n",
    "video_db = {\n",
    "    \"video_1\": {\"evento\": \"auto_en_movimiento\", \"frames_clave\": [\"car\", \"sunset\"]},\n",
    "    \"video_2\": {\"evento\": \"gato_sentado\", \"frames_clave\": [\"cat\", \"pixel-art\"]},\n",
    "}\n",
    "\n",
    "def responder_multimodal_cpu(prompt, imagen_id=None, audio_id=None, video_id=None):\n",
    "    prompt_l = prompt.lower()\n",
    "    partes = []\n",
    "\n",
    "    if imagen_id is not None and \"captions\" in globals() and imagen_id in captions:\n",
    "        partes.append(f\"Imagen: {captions[imagen_id]}\")\n",
    "\n",
    "    if audio_id is not None and audio_id in audio_db:\n",
    "        a = audio_db[audio_id]\n",
    "        partes.append(f\"Audio: '{a['evento']}' ({a['duracion_s']} s, conf={a['confianza']:.2f})\")\n",
    "\n",
    "    if video_id is not None and video_id in video_db:\n",
    "        v = video_db[video_id]\n",
    "        partes.append(f\"Video: '{v['evento']}' con pistas {v['frames_clave']}\")\n",
    "\n",
    "    if \"duración\" in prompt_l or \"duracion\" in prompt_l:\n",
    "        if audio_id in audio_db:\n",
    "            return f\"La duración del audio es {audio_db[audio_id]['duracion_s']} s.\"\n",
    "        return \"No tengo audio seleccionado.\"\n",
    "\n",
    "    if \"evento\" in prompt_l:\n",
    "        if video_id in video_db:\n",
    "            return f\"El evento principal del video es: {video_db[video_id]['evento']}.\"\n",
    "        return \"No tengo video seleccionado.\"\n",
    "\n",
    "    if \"qué animal\" in prompt_l or \"que animal\" in prompt_l:\n",
    "        if imagen_id == \"puppy\":\n",
    "            return \"Parece un cachorro.\"\n",
    "        if imagen_id == \"cat\":\n",
    "            return \"Parece un gato.\"\n",
    "        return \"No identifico un animal claro.\"\n",
    "\n",
    "    return \" | \".join(partes) if partes else \"Sin modalidades activas.\"\n",
    "\n",
    "print(responder_multimodal_cpu(\"Describe la escena\", imagen_id=\"car\", video_id=\"video_1\"))\n",
    "print(responder_multimodal_cpu(\"¿Qué animal aparece?\", imagen_id=\"puppy\", audio_id=\"audio_1\"))\n",
    "print(responder_multimodal_cpu(\"¿Cuál es la duración del audio?\", audio_id=\"audio_1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3526d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (estudiante): completa aquí\n",
    "# 1) Agrega un nuevo audio y un nuevo video.\n",
    "# 2) Extiende `responder_multimodal_cpu` con una nueva intención (por ejemplo: \"confianza\" o \"frames\").\n",
    "# 3) Prueba con un prompt propio.\n",
    "\n",
    "# Tu prueba:\n",
    "# print(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d411999",
   "metadata": {},
   "source": [
    "##### **4.4 Anexo opcional (GPU) con **BLIP-2** (captioning + VQA) y puente a Q-Former**\n",
    "\n",
    "Esta sección agrega código real para conectar lo conceptual con un modelo visión–lenguaje moderno.\n",
    "\n",
    "Esta sección:\n",
    "\n",
    "- refuerza el patrón **encoder visual + adaptador/Q-Former + LLM**,\n",
    "- muestra **captioning** (generación anclada en imagen),\n",
    "- muestra **VQA** (pregunta-respuesta sobre imagen).\n",
    "\n",
    "##### **Recordatorio conceptual (BLIP-2 en dos etapas)**\n",
    "\n",
    "1. **Etapa 1:** aprender consultas (*queries*) que extraen información útil del encoder visual.  \n",
    "2. **Etapa 2:** proyectar esas consultas al espacio del LLM para generar texto condicionado.\n",
    "\n",
    "> **Importante:** `Salesforce/blip2-opt-2.7b` suele requerir **GPU** y bastante memoria. Deja esta sección como **opcional** en clase si el entorno es CPU.\n",
    "\n",
    "> **Sugerencia:** usa esta sección solo como demo final o actividad opcional en Colab con GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90a0853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [OPCIONAL] Si estás en Colab / entorno limpio, descomenta:\n",
    "# !pip install transformers accelerate sentencepiece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6e8c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga BLIP-2 (opcional, normalmente con GPU)\n",
    "\n",
    "#try:\n",
    "#    import torch\n",
    "#    from transformers import AutoProcessor, Blip2ForConditionalGeneration\n",
    "#except Exception as e:\n",
    "#    raise ImportError(\"Faltan dependencias para BLIP-2. Instala transformers/torch.\") from e\n",
    "\n",
    "#BLIP_MODEL_ID = \"Salesforce/blip2-opt-2.7b\"\n",
    "#BLIP_REV = \"51572668da0eb669e01a189dc22abe6088589a24\"  # revisión estable usada en el cuaderno fuente\n",
    "\n",
    "#device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#dtype = torch.float16 if device == \"cuda\" else torch.float32\n",
    "\n",
    "#print(\"Dispositivo:\", device)\n",
    "#print(\"dtype:\", dtype)\n",
    "\n",
    "#blip_processor = AutoProcessor.from_pretrained(BLIP_MODEL_ID, revision=BLIP_REV)\n",
    "\n",
    "# Nota: en CPU puede tardar mucho o no entrar en memoria. Mantener opcional.\n",
    "#blip_model = Blip2ForConditionalGeneration.from_pretrained(\n",
    "#    BLIP_MODEL_ID,\n",
    "#    revision=BLIP_REV,\n",
    "#    torch_dtype=dtype\n",
    "#)\n",
    "#blip_model.to(device)\n",
    "#print(\"BLIP-2 cargado.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab7c2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demostración 1: Captioning (imagen -> texto)\n",
    "#from PIL import Image\n",
    "\n",
    "# Reutilizamos la imagen local del auto\n",
    "#img_car = Image.open(image_paths[\"car\"]).convert(\"RGB\")\n",
    "\n",
    "#inputs = blip_processor(img_car, return_tensors=\"pt\")\n",
    "#if \"pixel_values\" in inputs:\n",
    "#    inputs[\"pixel_values\"] = inputs[\"pixel_values\"].to(device, dtype)\n",
    "#if \"input_ids\" in inputs:\n",
    "#    inputs[\"input_ids\"] = inputs[\"input_ids\"].to(device)\n",
    "#if \"attention_mask\" in inputs:\n",
    "#    inputs[\"attention_mask\"] = inputs[\"attention_mask\"].to(device)\n",
    "\n",
    "#with torch.no_grad():\n",
    "#    gen_ids = blip_model.generate(**inputs, max_new_tokens=30)\n",
    "\n",
    "#caption_real = blip_processor.batch_decode(gen_ids, skip_special_tokens=True)[0].strip()\n",
    "#print(\"Caption BLIP-2:\", caption_real)\n",
    "\n",
    "\n",
    "# Demostración 2: VQA (imagen + pregunta -> respuesta)\n",
    "#prompt = \"Question: What do you see in this image? Answer:\"\n",
    "#inputs_vqa = blip_processor(img_car, text=prompt, return_tensors=\"pt\")\n",
    "\n",
    "#if \"pixel_values\" in inputs_vqa:\n",
    "#    inputs_vqa[\"pixel_values\"] = inputs_vqa[\"pixel_values\"].to(device, dtype)\n",
    "#if \"input_ids\" in inputs_vqa:\n",
    "#    inputs_vqa[\"input_ids\"] = inputs_vqa[\"input_ids\"].to(device)\n",
    "#if \"attention_mask\" in inputs_vqa:\n",
    "#    inputs_vqa[\"attention_mask\"] = inputs_vqa[\"attention_mask\"].to(device)\n",
    "\n",
    "#with torch.no_grad():\n",
    "#    vqa_ids = blip_model.generate(**inputs_vqa, max_new_tokens=40)\n",
    "\n",
    "#answer_real = blip_processor.batch_decode(vqa_ids, skip_special_tokens=True)[0].strip()\n",
    "#print(\"Respuesta VQA BLIP-2:\", answer_real)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5109d067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Opcional) Prompt encadenado tipo chat\n",
    "# Útil para mostrar memoria textual simple sobre la misma imagen.\n",
    "\n",
    "#prompt_chat = (\n",
    "#    \"Question: What do you see in this image? \"\n",
    "#    \"Answer: A sports car driving on the road at sunset. \"\n",
    "#    \"Question: What colors are prominent? \"\n",
    "#    \"Answer:\"\n",
    "#)\n",
    "\n",
    "#inputs_chat = blip_processor(img_car, text=prompt_chat, return_tensors=\"pt\")\n",
    "#if \"pixel_values\" in inputs_chat:\n",
    "#    inputs_chat[\"pixel_values\"] = inputs_chat[\"pixel_values\"].to(device, dtype)\n",
    "#if \"input_ids\" in inputs_chat:\n",
    "#    inputs_chat[\"input_ids\"] = inputs_chat[\"input_ids\"].to(device)\n",
    "#if \"attention_mask\" in inputs_chat:\n",
    "#    inputs_chat[\"attention_mask\"] = inputs_chat[\"attention_mask\"].to(device)\n",
    "\n",
    "#with torch.no_grad():\n",
    "#    chat_ids = blip_model.generate(**inputs_chat, max_new_tokens=40)\n",
    "\n",
    "#chat_answer = blip_processor.batch_decode(chat_ids, skip_special_tokens=True)[0].strip()\n",
    "#print(\"Salida (prompt encadenado):\", chat_answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a188726d",
   "metadata": {},
   "source": [
    "##### **4.5 Actividad guiada (MLLM)-extender a audio y vídeo (opcional)**\n",
    "\n",
    "Completa la tabla conceptual:\n",
    "\n",
    "| Modalidad | Ejemplo de entrada | Encoder típico | Salida posible |\n",
    "|---|---|---|---|\n",
    "| Texto | ... | ... | ... |\n",
    "| Imagen | ... | ... | ... |\n",
    "| Audio | ... | ... | ... |\n",
    "| Vídeo | ... | ... | ... |\n",
    "\n",
    "Puedes responder en una celda Markdown nueva.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41953b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (estudiante):\n",
    "# Simula un \"router multimodal\" que detecte la modalidad\n",
    "# y devuelva qué encoder usarías (solo texto, sin ML real).\n",
    "\n",
    "def elegir_encoder(modalidad):\n",
    "    modalidad = modalidad.lower().strip()\n",
    "    # Completa:\n",
    "    # if modalidad == \"texto\": return ...\n",
    "    # ...\n",
    "    return \"TODO\"\n",
    "\n",
    "for m in [\"texto\", \"imagen\", \"audio\", \"video\"]:\n",
    "    print(m, \"->\", elegir_encoder(m))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc12bfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (estudiante, opcional):\n",
    "# Implementa una versión más completa del pipeline conceptual:\n",
    "# entrada -> encoder por modalidad -> proyección -> \"LLM\"\n",
    "# Usa strings y diccionarios, no hace falta deep learning real.\n",
    "\n",
    "class MiniMLLM:\n",
    "    def __init__(self):\n",
    "        self.encoders = {\n",
    "            \"texto\": \"TextEncoder\",\n",
    "            \"imagen\": \"VisionEncoder\",\n",
    "            \"audio\": \"AudioEncoder\",\n",
    "            \"video\": \"VideoEncoder\",\n",
    "        }\n",
    "\n",
    "    def procesar(self, modalidad, dato):\n",
    "        # TODO\n",
    "        return {\n",
    "            \"modalidad\": modalidad,\n",
    "            \"encoder\": \"TODO\",\n",
    "            \"embedding\": \"TODO\",\n",
    "            \"respuesta\": \"TODO\"\n",
    "        }\n",
    "\n",
    "demo = MiniMLLM()\n",
    "print(demo.procesar(\"imagen\", \"car.png\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b994c52d",
   "metadata": {},
   "source": [
    "#### **5. Ejercicios de evaluación rápida**\n",
    "\n",
    "##### **Conceptuales**\n",
    "\n",
    "1. ¿Por qué ViT usa patches en lugar de convoluciones?\n",
    "2. ¿Qué optimiza CLIP durante el entrenamiento?\n",
    "3. ¿Qué diferencia hay entre VQA y captioning?\n",
    "4. ¿Qué papel cumple un módulo tipo **Q-Former**?\n",
    "\n",
    "##### **De aplicación**\n",
    "\n",
    "1. Propón un caso de uso educativo con imagen + texto.\n",
    "2. Propón un caso de uso industrial con imagen + audio.\n",
    "3. Explica cuándo **no** usarías un MLLM (coste, datos, latencia, privacidad).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7ddd9d-46d4-4830-bb75-288d6a6f5eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tus respuestas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
